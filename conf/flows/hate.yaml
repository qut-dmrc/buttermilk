defaults:
  - _self_

hate:
  _target_: buttermilk.runner.moa.MoA
  source: hate speech guidelines
  save: ${save}

  steps:
    # - name: describer
    #   num_runs: 1
    #   variants:
    #     template: describe
    #     download_if_necessary: true
    #     model:
    #       - gemini2pro

    - name: context_owl
      agent: LLMAgent
      num_runs: 1
      save: ${save}
      variants:
        template: owl
        criteria: 
          - ambiguity
          - power
        model: ${llms.general}
      inputs:
        record: record

    - name: feedback
      description: Applies criteria to content.
      agent: LLMAgent
      num_runs: 1
      variants:
        template: judge
        model: ${llms.general}
        formatting: json_rules
        criteria:
          - criteria_ordinary
          - criteria_gelber
          - criteria_hatefb_factorised
      inputs:
        record: record
        feedback: context_owl

    - name: judge
      description: Applies criteria to content.
      agent: LLMAgent
      num_runs: 1
      variants:
        template: judge
        model: ${llms.general}
        formatting: json_rules
        criteria:
          - criteria_ordinary
          - criteria_gelber
          - criteria_hatefb_factorised
      inputs:
        record: record
        feedback: context_owl

    - name: feedback
      description: QA for mistakes.
      agent: LLMAgent
      num_runs: 1
      save: ${save}
      variants:
        template: feedback
        model: ${llms.general}
      inputs:
        context: context

    - name: synth
      agent: LLMAgent
      description: Synthesizes draft answers.
      num_runs: 1
      save: ${save}
      variants:
        model: ${llms.synthesisers}
        template: synthesise_judger
        instructions: >
          Carefully apply EACH of the CRITERIA in order and provide a COMPLETE and SPECIFIC explanation about whether the particular rule has been violated and how. Use quotes from the content where necessary to support your analysis.
        criteria: criteria_ordinary
        formatting: json_rules
      inputs:
        record: record
        answers: judge
        feedback: 
          - context_owl
          - feedback

    - name: eval
      agent: LLMAgent
      num_runs: 1
      variants:
        template: differences
        criteria: criteria_ordinary
        model: ${llms.synthesisers}
      inputs:
        content: record.fulltext
        answers:
          - judger
          - synth