name: evaluator
num_runs: 1
concurrent: 8
max_batch: 128
init:
  template: evaluate
# model: ["haiku"]
model:
  #   #   # - o1-preview
  # - llama31_70b
  #   #   - llama31_8b
  #   #   # - llama31_405b
  - gpt4o
  #   #   - opus
  # - sonnet
  #   #   - haiku
  # - gemini15pro
save:
  destination: bq
  dataset: 'dmrc-analysis.toxicity.step'
  schema: buttermilk/schemas/step.json
data:
  - type: job
    name: answer
    dataset: dmrc-analysis.toxicity.step_extracted
    last_n_days: 3
    aggregate: false
    filter:
      step: trans_reporting
      agent:
    group:
      step: step
      record_id: record_id
      criteria: step_info.criteria
    columns:
      expected: groundtruth
      id: job_id
      predicted: predicted
      answer: outputs.reasons

