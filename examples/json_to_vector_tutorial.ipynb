{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JSON to Vector Database Tutorial\n",
    "\n",
    "This tutorial demonstrates how to convert any JSON dataset into a searchable vector database using Buttermilk's infrastructure.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "- üìÑ Loading JSON data from various sources (local files, cloud storage, URLs)\n",
    "- üîß Configuring vector stores for different data types\n",
    "- ‚úÇÔ∏è Chunking strategies for optimal search performance\n",
    "- üß† Embedding generation with different models\n",
    "- üîç Semantic search techniques and optimization\n",
    "- üìä Best practices for production deployments\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Buttermilk environment configured\n",
    "- Basic familiarity with JSON data structures\n",
    "- Access to embedding models (Vertex AI recommended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-06-16 18:50:52\u001b[0m [] \u001b[1;30mINFO\u001b[0m bm_init.py:745 Logging set up for run: platform='local' name='bm_api' job='json_tutorial' run_id='20250616T0850Z-cHd3-docker-desktop-debian' ip=None node_name='docker-desktop' save_dir='/tmp/tmpsv6awzna/bm_api/json_tutorial/20250616T0850Z-cHd3-docker-desktop-debian' flow_api=None. Save directory: /tmp/tmpsv6awzna/bm_api/json_tutorial/20250616T0850Z-cHd3-docker-desktop-debian\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Initialized Buttermilk <span style=\"font-weight: bold\">(</span>bm<span style=\"font-weight: bold\">)</span> with configuration:\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Initialized Buttermilk \u001b[1m(\u001b[0mbm\u001b[1m)\u001b[0m with configuration:\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'platform'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'local'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'name'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'bm_api'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'job'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'json_tutorial'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'run_id'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'20250616T0850Z-cHd3-docker-desktop-debian'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'node_name'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'docker-desktop'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'save_dir'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'/tmp/tmpsv6awzna/bm_api/json_tutorial/20250616T0850Z-cHd3-docker-desktop-debian'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'connections'</span>: <span style=\"font-weight: bold\">[]</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'secret_provider'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'type'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'gcp'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'project'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'prosocial-443205'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'models_secret'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'dev__llm__connections'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'credentials_secret'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'dev__shared_credentials'</span>\n",
       "    <span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'logger_cfg'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'type'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'gcp'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'project'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'prosocial-443205'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'location'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'us-central1'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'verbose'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span><span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'pubsub'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'type'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'gcp'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'project'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'prosocial-443205'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'jobs_subscription'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'jobs-sub'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'status_subscription'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'flow-sub'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'status_topic'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'flow'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'jobs_topic'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'jobs'</span>\n",
       "    <span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'clouds'</span>: <span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'type'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'gcp'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'project'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'prosocial-443205'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'quota_project_id'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'prosocial-443205'</span><span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'type'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'vertex'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'project'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'prosocial-443205'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'region'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'us-central1'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'location'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'us-central1'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'bucket'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'prosocial-de'</span>\n",
       "        <span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">]</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'tracing'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'enabled'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'api_key'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">''</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'provider'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'weave'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'otlp_headers'</span>: <span style=\"font-weight: bold\">{}}</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'datasets'</span>: <span style=\"font-weight: bold\">{}</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'save_dir_base'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'/tmp/tmpsv6awzna'</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m{\u001b[0m\n",
       "    \u001b[32m'platform'\u001b[0m: \u001b[32m'local'\u001b[0m,\n",
       "    \u001b[32m'name'\u001b[0m: \u001b[32m'bm_api'\u001b[0m,\n",
       "    \u001b[32m'job'\u001b[0m: \u001b[32m'json_tutorial'\u001b[0m,\n",
       "    \u001b[32m'run_id'\u001b[0m: \u001b[32m'20250616T0850Z-cHd3-docker-desktop-debian'\u001b[0m,\n",
       "    \u001b[32m'node_name'\u001b[0m: \u001b[32m'docker-desktop'\u001b[0m,\n",
       "    \u001b[32m'save_dir'\u001b[0m: \u001b[32m'/tmp/tmpsv6awzna/bm_api/json_tutorial/20250616T0850Z-cHd3-docker-desktop-debian'\u001b[0m,\n",
       "    \u001b[32m'connections'\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m,\n",
       "    \u001b[32m'secret_provider'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "        \u001b[32m'type'\u001b[0m: \u001b[32m'gcp'\u001b[0m,\n",
       "        \u001b[32m'project'\u001b[0m: \u001b[32m'prosocial-443205'\u001b[0m,\n",
       "        \u001b[32m'models_secret'\u001b[0m: \u001b[32m'dev__llm__connections'\u001b[0m,\n",
       "        \u001b[32m'credentials_secret'\u001b[0m: \u001b[32m'dev__shared_credentials'\u001b[0m\n",
       "    \u001b[1m}\u001b[0m,\n",
       "    \u001b[32m'logger_cfg'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'type'\u001b[0m: \u001b[32m'gcp'\u001b[0m, \u001b[32m'project'\u001b[0m: \u001b[32m'prosocial-443205'\u001b[0m, \u001b[32m'location'\u001b[0m: \u001b[32m'us-central1'\u001b[0m, \u001b[32m'verbose'\u001b[0m: \u001b[3;91mFalse\u001b[0m\u001b[1m}\u001b[0m,\n",
       "    \u001b[32m'pubsub'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "        \u001b[32m'type'\u001b[0m: \u001b[32m'gcp'\u001b[0m,\n",
       "        \u001b[32m'project'\u001b[0m: \u001b[32m'prosocial-443205'\u001b[0m,\n",
       "        \u001b[32m'jobs_subscription'\u001b[0m: \u001b[32m'jobs-sub'\u001b[0m,\n",
       "        \u001b[32m'status_subscription'\u001b[0m: \u001b[32m'flow-sub'\u001b[0m,\n",
       "        \u001b[32m'status_topic'\u001b[0m: \u001b[32m'flow'\u001b[0m,\n",
       "        \u001b[32m'jobs_topic'\u001b[0m: \u001b[32m'jobs'\u001b[0m\n",
       "    \u001b[1m}\u001b[0m,\n",
       "    \u001b[32m'clouds'\u001b[0m: \u001b[1m[\u001b[0m\n",
       "        \u001b[1m{\u001b[0m\u001b[32m'type'\u001b[0m: \u001b[32m'gcp'\u001b[0m, \u001b[32m'project'\u001b[0m: \u001b[32m'prosocial-443205'\u001b[0m, \u001b[32m'quota_project_id'\u001b[0m: \u001b[32m'prosocial-443205'\u001b[0m\u001b[1m}\u001b[0m,\n",
       "        \u001b[1m{\u001b[0m\n",
       "            \u001b[32m'type'\u001b[0m: \u001b[32m'vertex'\u001b[0m,\n",
       "            \u001b[32m'project'\u001b[0m: \u001b[32m'prosocial-443205'\u001b[0m,\n",
       "            \u001b[32m'region'\u001b[0m: \u001b[32m'us-central1'\u001b[0m,\n",
       "            \u001b[32m'location'\u001b[0m: \u001b[32m'us-central1'\u001b[0m,\n",
       "            \u001b[32m'bucket'\u001b[0m: \u001b[32m'prosocial-de'\u001b[0m\n",
       "        \u001b[1m}\u001b[0m\n",
       "    \u001b[1m]\u001b[0m,\n",
       "    \u001b[32m'tracing'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'enabled'\u001b[0m: \u001b[3;92mTrue\u001b[0m, \u001b[32m'api_key'\u001b[0m: \u001b[32m''\u001b[0m, \u001b[32m'provider'\u001b[0m: \u001b[32m'weave'\u001b[0m, \u001b[32m'otlp_headers'\u001b[0m: \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\u001b[1m}\u001b[0m,\n",
       "    \u001b[32m'datasets'\u001b[0m: \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "    \u001b[32m'save_dir_base'\u001b[0m: \u001b[32m'/tmp/tmpsv6awzna'\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-06-16 18:50:52\u001b[0m [] \u001b[1;30mINFO\u001b[0m nb.py:59 Starting interactive run for bm_api job json_tutorial in notebook\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">üöÄ Buttermilk initialized for JSON-to-Vector tutorial\n",
       "</pre>\n"
      ],
      "text/plain": [
       "üöÄ Buttermilk initialized for JSON-to-Vector tutorial\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-06-16 18:50:52\u001b[0m [] \u001b[1;30mINFO\u001b[0m save.py:641 Successfully dumped data to local disk (JSON): /tmp/tmpsv6awzna/bm_api/json_tutorial/20250616T0850Z-cHd3-docker-desktop-debian/tmpsrg7shbs.json.\n",
      "\u001b[32m2025-06-16 18:50:52\u001b[0m [] \u001b[1;30mINFO\u001b[0m save.py:215 Successfully saved data using dump_to_disk to: /tmp/tmpsv6awzna/bm_api/json_tutorial/20250616T0850Z-cHd3-docker-desktop-debian/tmpsrg7shbs.json.\n",
      "\u001b[32m2025-06-16 18:50:52\u001b[0m [] \u001b[1;30mINFO\u001b[0m bm_init.py:831 {'message': 'Successfully saved data to: /tmp/tmpsv6awzna/bm_api/json_tutorial/20250616T0850Z-cHd3-docker-desktop-debian/tmpsrg7shbs.json', 'uri': '/tmp/tmpsv6awzna/bm_api/json_tutorial/20250616T0850Z-cHd3-docker-desktop-debian/tmpsrg7shbs.json', 'run_id': '20250616T0850Z-cHd3-docker-desktop-debian'}\n"
     ]
    }
   ],
   "source": [
    "# Setup and imports\n",
    "import json\n",
    "import tempfile\n",
    "import asyncio\n",
    "from pathlib import Path\n",
    "from rich import print\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# Buttermilk imports\n",
    "from buttermilk.utils.nb import init\n",
    "from buttermilk._core.dmrc import get_bm, set_bm\n",
    "from buttermilk._core.config import DataSourceConfig\n",
    "from buttermilk._core.types import Record\n",
    "from buttermilk.data.loaders import create_data_loader\n",
    "from buttermilk.data.vector import ChromaDBEmbeddings, InputDocument, DefaultTextSplitter, list_to_async_iterator\n",
    "\n",
    "# Initialize Buttermilk\n",
    "cfg = init(job=\"json_tutorial\")\n",
    "bm = get_bm()\n",
    "\n",
    "print(\"üöÄ Buttermilk initialized for JSON-to-Vector tutorial\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Preparing Sample JSON Data\n",
    "\n",
    "Let's start by creating some sample JSON data to work with. We'll create different types of documents to demonstrate various scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample JSON data representing different document types\n",
    "sample_documents = [\n",
    "    {\n",
    "        \"id\": \"doc_001\",\n",
    "        \"title\": \"Introduction to Machine Learning\",\n",
    "        \"content\": \"Machine learning is a subset of artificial intelligence that enables computers to learn and make decisions from data without being explicitly programmed. It involves algorithms that can identify patterns in data and make predictions or classifications based on those patterns. The field has applications in image recognition, natural language processing, recommendation systems, and autonomous vehicles.\",\n",
    "        \"category\": \"technology\",\n",
    "        \"author\": \"Dr. Alice Smith\",\n",
    "        \"date\": \"2024-01-15\",\n",
    "        \"tags\": [\"AI\", \"machine learning\", \"algorithms\"]\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"doc_002\", \n",
    "        \"title\": \"Sustainable Energy Solutions\",\n",
    "        \"content\": \"Renewable energy sources such as solar, wind, and hydroelectric power are becoming increasingly important in combating climate change. Solar panels convert sunlight directly into electricity through photovoltaic cells, while wind turbines harness wind energy to generate power. These technologies are becoming more efficient and cost-effective, making them viable alternatives to fossil fuels.\",\n",
    "        \"category\": \"environment\",\n",
    "        \"author\": \"Prof. Bob Johnson\",\n",
    "        \"date\": \"2024-02-03\",\n",
    "        \"tags\": [\"renewable energy\", \"solar\", \"wind\", \"climate\"]\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"doc_003\",\n",
    "        \"title\": \"Modern Web Development Frameworks\",\n",
    "        \"content\": \"Web development has evolved significantly with the introduction of modern frameworks like React, Vue.js, and Angular. These frameworks provide component-based architectures that make it easier to build complex, interactive user interfaces. React uses a virtual DOM for efficient updates, Vue.js offers a gentle learning curve with powerful features, and Angular provides a full-featured framework with TypeScript integration.\",\n",
    "        \"category\": \"technology\",\n",
    "        \"author\": \"Carol Developer\",\n",
    "        \"date\": \"2024-01-28\",\n",
    "        \"tags\": [\"web development\", \"React\", \"Vue\", \"Angular\"]\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"doc_004\",\n",
    "        \"title\": \"Urban Gardening Techniques\",\n",
    "        \"content\": \"Urban gardening allows city dwellers to grow their own food and plants in limited spaces. Techniques include container gardening, vertical gardens, and rooftop farming. These methods can help improve food security, reduce environmental impact, and provide mental health benefits. Hydroponic and aquaponic systems are particularly effective for small spaces, allowing for year-round growing without soil.\",\n",
    "        \"category\": \"lifestyle\",\n",
    "        \"author\": \"David Green\",\n",
    "        \"date\": \"2024-02-10\",\n",
    "        \"tags\": [\"gardening\", \"urban\", \"hydroponics\", \"sustainability\"]\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"doc_005\",\n",
    "        \"title\": \"Quantum Computing Fundamentals\",\n",
    "        \"content\": \"Quantum computing represents a paradigm shift from classical computing by leveraging quantum mechanical phenomena like superposition and entanglement. Unlike classical bits that exist in either 0 or 1 states, quantum bits (qubits) can exist in multiple states simultaneously. This allows quantum computers to potentially solve certain problems exponentially faster than classical computers, particularly in cryptography, optimization, and scientific simulation.\",\n",
    "        \"category\": \"technology\",\n",
    "        \"author\": \"Dr. Eva Quantum\",\n",
    "        \"date\": \"2024-02-15\",\n",
    "        \"tags\": [\"quantum computing\", \"qubits\", \"superposition\", \"cryptography\"]\n",
    "    }\n",
    "]\n",
    "\n",
    "# Save sample data to a temporary JSON file\n",
    "temp_dir = tempfile.mkdtemp()\n",
    "json_file_path = Path(temp_dir) / \"sample_documents.json\"\n",
    "\n",
    "with open(json_file_path, 'w') as f:\n",
    "    for doc in sample_documents:\n",
    "        f.write(json.dumps(doc) + '\\n')  # JSONL format\n",
    "\n",
    "print(f\"üìÑ Created sample JSON data with {len(sample_documents)} documents\")\n",
    "print(f\"üìÅ Saved to: {json_file_path}\")\n",
    "print(f\"\\nSample document structure:\")\n",
    "print(json.dumps(sample_documents[0], indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: JSON Data Loading\n",
    "\n",
    "Now let's explore different ways to load JSON data using Buttermilk's data loading infrastructure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: Loading JSONL (JSON Lines) files\n",
    "print(\"üìä Method 1: Loading JSONL file\")\n",
    "\n",
    "# Create data source configuration\n",
    "jsonl_config = DataSourceConfig(\n",
    "    type=\"file\",\n",
    "    path=str(json_file_path),\n",
    "    # Map JSON fields to Record fields\n",
    "    columns={\n",
    "        \"record_id\": \"id\",     # Map 'id' to 'record_id'\n",
    "        \"content\": \"content\"   # Map 'content' to 'content'\n",
    "    }\n",
    ")\n",
    "\n",
    "# Create and use data loader\n",
    "loader = create_data_loader(jsonl_config)\n",
    "records = list(loader)\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(records)} records\")\n",
    "print(f\"\\nFirst record:\")\n",
    "first_record = records[0]\n",
    "print(f\"  Record ID: {first_record.record_id}\")\n",
    "print(f\"  Content preview: {first_record.content[:100]}...\")\n",
    "print(f\"  Metadata keys: {list(first_record.metadata.keys())}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2: Custom JSON loading with different field mappings\n",
    "print(\"\\nüìä Method 2: Custom field mapping\")\n",
    "\n",
    "# Alternative mapping strategy\n",
    "custom_config = DataSourceConfig(\n",
    "    type=\"file\",\n",
    "    path=str(json_file_path),\n",
    "    columns={\n",
    "        \"record_id\": \"id\",\n",
    "        \"content\": \"title\",    # Use title as primary content\n",
    "        \"uri\": \"content\"       # Store full content in URI field\n",
    "    }\n",
    ")\n",
    "\n",
    "loader_custom = create_data_loader(custom_config)\n",
    "records_custom = list(loader_custom)\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(records_custom)} records with custom mapping\")\n",
    "print(f\"  Content (title): {records_custom[0].content}\")\n",
    "print(f\"  URI (full content): {records_custom[0].uri[:100]}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 3: Handling complex JSON structures\n",
    "print(\"\\nüìä Method 3: Complex JSON structures\")\n",
    "\n",
    "# Create a more complex JSON structure\n",
    "complex_document = {\n",
    "    \"document\": {\n",
    "        \"id\": \"complex_001\",\n",
    "        \"metadata\": {\n",
    "            \"title\": \"Advanced Data Science\",\n",
    "            \"author\": {\"name\": \"Dr. Jane Doe\", \"affiliation\": \"University XYZ\"},\n",
    "            \"publication_date\": \"2024-03-01\"\n",
    "        },\n",
    "        \"sections\": [\n",
    "            {\"heading\": \"Introduction\", \"content\": \"Data science combines statistics, programming, and domain expertise...\"},\n",
    "            {\"heading\": \"Methods\", \"content\": \"We employed machine learning algorithms including random forests...\"}\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "# For complex structures, we often need custom processing\n",
    "def process_complex_json(json_data: dict) -> Record:\n",
    "    \"\"\"\n",
    "    Custom function to process complex JSON structures\n",
    "    \"\"\"\n",
    "    doc = json_data[\"document\"]\n",
    "    \n",
    "    # Extract and combine content from sections\n",
    "    content_parts = []\n",
    "    content_parts.append(doc[\"metadata\"][\"title\"])\n",
    "    \n",
    "    for section in doc[\"sections\"]:\n",
    "        content_parts.append(f\"{section['heading']}: {section['content']}\")\n",
    "    \n",
    "    combined_content = \"\\n\\n\".join(content_parts)\n",
    "    \n",
    "    # Create metadata\n",
    "    metadata = {\n",
    "        \"title\": doc[\"metadata\"][\"title\"],\n",
    "        \"author_name\": doc[\"metadata\"][\"author\"][\"name\"],\n",
    "        \"author_affiliation\": doc[\"metadata\"][\"author\"][\"affiliation\"],\n",
    "        \"publication_date\": doc[\"metadata\"][\"publication_date\"],\n",
    "        \"sections_count\": len(doc[\"sections\"])\n",
    "    }\n",
    "    \n",
    "    return Record(\n",
    "        record_id=doc[\"id\"],\n",
    "        content=combined_content,\n",
    "        metadata=metadata\n",
    "    )\n",
    "\n",
    "# Process the complex document\n",
    "complex_record = process_complex_json(complex_document)\n",
    "print(f\"‚úÖ Processed complex JSON structure\")\n",
    "print(f\"  Record ID: {complex_record.record_id}\")\n",
    "print(f\"  Content: {complex_record.content[:150]}...\")\n",
    "print(f\"  Metadata: {complex_record.metadata}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Vector Store Configuration\n",
    "\n",
    "Let's explore different vector store configurations for various use cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration 1: Standard setup for general documents\n",
    "print(\"‚öôÔ∏è Configuration 1: Standard Document Processing\")\n",
    "\n",
    "standard_vector_store = ChromaDBEmbeddings(\n",
    "    collection_name=\"tutorial_standard\",\n",
    "    persist_directory=f\"{temp_dir}/standard_vectorstore\",\n",
    "    embedding_model=\"text-embedding-005\",\n",
    "    dimensionality=3072,\n",
    "    concurrency=5,  # Lower for tutorial\n",
    "    arrow_save_dir=f\"{temp_dir}/standard_chunks\"\n",
    ")\n",
    "\n",
    "# Initialize cache\n",
    "await standard_vector_store.ensure_cache_initialized()\n",
    "print(f\"‚úÖ Standard vector store initialized\")\n",
    "print(f\"   Collection: {standard_vector_store.collection.name}\")\n",
    "print(f\"   Embedding model: {standard_vector_store.embedding_model}\")\n",
    "print(f\"   Dimensionality: {standard_vector_store.dimensionality}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration 2: Optimized for short texts\n",
    "print(\"\\n‚öôÔ∏è Configuration 2: Short Text Optimization\")\n",
    "\n",
    "short_text_vector_store = ChromaDBEmbeddings(\n",
    "    collection_name=\"tutorial_short_text\",\n",
    "    persist_directory=f\"{temp_dir}/short_text_vectorstore\",\n",
    "    embedding_model=\"text-embedding-005\",\n",
    "    dimensionality=3072,\n",
    "    concurrency=3,\n",
    "    upsert_batch_size=20,  # Smaller batches for short texts\n",
    "    arrow_save_dir=f\"{temp_dir}/short_text_chunks\"\n",
    ")\n",
    "\n",
    "await short_text_vector_store.ensure_cache_initialized()\n",
    "print(f\"‚úÖ Short text vector store initialized\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration 3: High-performance setup\n",
    "print(\"\\n‚öôÔ∏è Configuration 3: High-Performance Setup\")\n",
    "\n",
    "# Note: In production, you'd use higher concurrency and batch sizes\n",
    "performance_config = {\n",
    "    \"collection_name\": \"tutorial_performance\",\n",
    "    \"persist_directory\": f\"{temp_dir}/performance_vectorstore\",\n",
    "    \"embedding_model\": \"text-embedding-005\",\n",
    "    \"dimensionality\": 3072,\n",
    "    \"concurrency\": 20,           # Higher concurrency\n",
    "    \"upsert_batch_size\": 100,    # Larger batches\n",
    "    \"embedding_batch_size\": 10,  # Batch embeddings\n",
    "    \"arrow_save_dir\": f\"{temp_dir}/performance_chunks\"\n",
    "}\n",
    "\n",
    "print(f\"üìã High-performance configuration:\")\n",
    "for key, value in performance_config.items():\n",
    "    print(f\"   {key}: {value}\")\n",
    "    \n",
    "print(\"\\nüí° For production, consider:\")\n",
    "print(\"   - Persistent storage volumes\")\n",
    "print(\"   - SSD storage for better performance\")\n",
    "print(\"   - Monitoring embedding API costs\")\n",
    "print(\"   - Load balancing for high traffic\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Text Chunking Strategies\n",
    "\n",
    "Different types of content require different chunking strategies for optimal search performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strategy 1: Standard chunking\n",
    "print(\"‚úÇÔ∏è Chunking Strategy 1: Standard Documents\")\n",
    "\n",
    "standard_splitter = DefaultTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200\n",
    ")\n",
    "\n",
    "# Test with our sample document\n",
    "sample_text = sample_documents[0][\"content\"]\n",
    "standard_chunks = standard_splitter.split_text(sample_text)\n",
    "\n",
    "print(f\"Original text length: {len(sample_text)} characters\")\n",
    "print(f\"Number of chunks: {len(standard_chunks)}\")\n",
    "print(f\"Average chunk size: {sum(len(chunk) for chunk in standard_chunks) / len(standard_chunks):.0f} characters\")\n",
    "print(f\"\\nFirst chunk: {standard_chunks[0][:100]}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strategy 2: Small chunks for precise search\n",
    "print(\"\\n‚úÇÔ∏è Chunking Strategy 2: Small Chunks (Precise Search)\")\n",
    "\n",
    "precise_splitter = DefaultTextSplitter(\n",
    "    chunk_size=500,   # Smaller chunks\n",
    "    chunk_overlap=100  # Less overlap\n",
    ")\n",
    "\n",
    "precise_chunks = precise_splitter.split_text(sample_text)\n",
    "print(f\"Number of chunks: {len(precise_chunks)}\")\n",
    "print(f\"Average chunk size: {sum(len(chunk) for chunk in precise_chunks) / len(precise_chunks):.0f} characters\")\n",
    "print(\"\\nüìù Use case: When you need to find very specific information\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strategy 3: Large chunks for context preservation\n",
    "print(\"\\n‚úÇÔ∏è Chunking Strategy 3: Large Chunks (Context Preservation)\")\n",
    "\n",
    "context_splitter = DefaultTextSplitter(\n",
    "    chunk_size=2000,  # Larger chunks\n",
    "    chunk_overlap=400  # More overlap\n",
    ")\n",
    "\n",
    "# Use a longer text for this example\n",
    "long_text = \" \".join([doc[\"content\"] for doc in sample_documents[:3]])\n",
    "context_chunks = context_splitter.split_text(long_text)\n",
    "\n",
    "print(f\"Long text length: {len(long_text)} characters\")\n",
    "print(f\"Number of chunks: {len(context_chunks)}\")\n",
    "print(f\"Average chunk size: {sum(len(chunk) for chunk in context_chunks) / len(context_chunks):.0f} characters\")\n",
    "print(\"\\nüìù Use case: When context and relationships between ideas are important\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strategy 4: Custom chunking for structured content\n",
    "print(\"\\n‚úÇÔ∏è Chunking Strategy 4: Custom Structured Chunking\")\n",
    "\n",
    "def custom_structured_chunker(text: str, metadata: dict) -> list[str]:\n",
    "    \"\"\"\n",
    "    Custom chunking that respects document structure\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    \n",
    "    # Include title and metadata in first chunk\n",
    "    title = metadata.get(\"title\", \"\")\n",
    "    author = metadata.get(\"author\", \"\")\n",
    "    category = metadata.get(\"category\", \"\")\n",
    "    \n",
    "    header_chunk = f\"Title: {title}\\nAuthor: {author}\\nCategory: {category}\\n\\n{text[:500]}\"\n",
    "    chunks.append(header_chunk)\n",
    "    \n",
    "    # Split remaining text into smaller chunks\n",
    "    remaining_text = text[500:]\n",
    "    if remaining_text:\n",
    "        splitter = DefaultTextSplitter(chunk_size=800, chunk_overlap=150)\n",
    "        remaining_chunks = splitter.split_text(remaining_text)\n",
    "        chunks.extend(remaining_chunks)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# Test custom chunking\n",
    "sample_doc = sample_documents[0]\n",
    "custom_chunks = custom_structured_chunker(sample_doc[\"content\"], sample_doc)\n",
    "\n",
    "print(f\"Number of chunks: {len(custom_chunks)}\")\n",
    "print(f\"\\nFirst chunk (with metadata):\")\n",
    "print(custom_chunks[0][:200] + \"...\")\n",
    "print(\"\\nüìù Use case: When you want to preserve document metadata and structure\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Document Processing Pipeline\n",
    "\n",
    "Now let's put it all together and process our documents through the complete pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Records to InputDocuments\n",
    "print(\"üîÑ Converting Records to InputDocuments\")\n",
    "\n",
    "def records_to_input_docs(records: List[Record]) -> List[InputDocument]:\n",
    "    \"\"\"\n",
    "    Convert Buttermilk Records to InputDocuments for vector processing\n",
    "    \"\"\"\n",
    "    input_docs = []\n",
    "    \n",
    "    for record in records:\n",
    "        # Use title from metadata if available, otherwise create one\n",
    "        title = \"Unknown Document\"\n",
    "        if record.metadata:\n",
    "            title = record.metadata.get(\"title\", f\"Document {record.record_id}\")\n",
    "        \n",
    "        input_doc = InputDocument(\n",
    "            record_id=record.record_id,\n",
    "            title=title,\n",
    "            full_text=record.content,\n",
    "            file_path=\"\",  # Not from a file\n",
    "            metadata=record.metadata or {}\n",
    "        )\n",
    "        input_docs.append(input_doc)\n",
    "    \n",
    "    return input_docs\n",
    "\n",
    "# Convert our sample records\n",
    "input_documents = records_to_input_docs(records)\n",
    "print(f\"‚úÖ Created {len(input_documents)} InputDocuments\")\n",
    "\n",
    "# Show example\n",
    "example_doc = input_documents[0]\n",
    "print(f\"\\nExample InputDocument:\")\n",
    "print(f\"  ID: {example_doc.record_id}\")\n",
    "print(f\"  Title: {example_doc.title}\")\n",
    "print(f\"  Text length: {len(example_doc.full_text)} characters\")\n",
    "print(f\"  Metadata keys: {list(example_doc.metadata.keys())}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process documents through the pipeline\n",
    "print(\"üè≠ Processing Documents Through Pipeline\")\n",
    "print(\"This includes: chunking ‚Üí embedding ‚Üí storing in ChromaDB\")\n",
    "\n",
    "# Use standard configuration and chunking\n",
    "text_splitter = DefaultTextSplitter(chunk_size=800, chunk_overlap=150)\n",
    "\n",
    "# Process each document\n",
    "processed_count = 0\n",
    "total_chunks = 0\n",
    "\n",
    "for i, doc in enumerate(input_documents):\n",
    "    print(f\"\\nüìÑ Processing document {i+1}/{len(input_documents)}: {doc.title}\")\n",
    "    \n",
    "    try:\n",
    "        # Step 1: Chunk the document\n",
    "        chunked_doc = await text_splitter.process(doc)\n",
    "        if not chunked_doc or not chunked_doc.chunks:\n",
    "            print(f\"  ‚ö†Ô∏è No chunks created for {doc.record_id}\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"  ‚úÇÔ∏è Created {len(chunked_doc.chunks)} chunks\")\n",
    "        \n",
    "        # Step 2: Embed and store\n",
    "        processed_doc = await standard_vector_store.process(chunked_doc)\n",
    "        if processed_doc:\n",
    "            processed_count += 1\n",
    "            total_chunks += len(processed_doc.chunks)\n",
    "            print(f\"  ‚úÖ Embedded and stored {len(processed_doc.chunks)} chunks\")\n",
    "        else:\n",
    "            print(f\"  ‚ùå Failed to process {doc.record_id}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ùå Error processing {doc.record_id}: {e}\")\n",
    "\n",
    "print(f\"\\nüéâ Pipeline Complete!\")\n",
    "print(f\"Successfully processed: {processed_count}/{len(input_documents)} documents\")\n",
    "print(f\"Total chunks stored: {total_chunks}\")\n",
    "print(f\"Vector store collection count: {standard_vector_store.collection.count()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Semantic Search Techniques\n",
    "\n",
    "Now let's explore different search techniques and optimization strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic semantic search function\n",
    "def semantic_search(query: str, n_results: int = 5, collection=None):\n",
    "    \"\"\"\n",
    "    Perform semantic search on the vector store\n",
    "    \"\"\"\n",
    "    if collection is None:\n",
    "        collection = standard_vector_store.collection\n",
    "    \n",
    "    results = collection.query(\n",
    "        query_texts=[query],\n",
    "        n_results=n_results,\n",
    "        include=[\"documents\", \"metadatas\", \"distances\"]\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nüîç Search: '{query}'\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    if not results['documents'] or not results['documents'][0]:\n",
    "        print(\"No results found\")\n",
    "        return []\n",
    "    \n",
    "    search_results = []\n",
    "    for i, (doc, metadata, distance) in enumerate(zip(\n",
    "        results['documents'][0],\n",
    "        results['metadatas'][0],\n",
    "        results['distances'][0]\n",
    "    )):\n",
    "        similarity = 1 - distance\n",
    "        result = {\n",
    "            'rank': i + 1,\n",
    "            'similarity': similarity,\n",
    "            'document': doc,\n",
    "            'metadata': metadata\n",
    "        }\n",
    "        search_results.append(result)\n",
    "        \n",
    "        print(f\"\\nüìÑ Result {i+1} (similarity: {similarity:.3f})\")\n",
    "        print(f\"Document: {metadata.get('document_title', 'Unknown')}\")\n",
    "        print(f\"Text: {doc[:150]}...\")\n",
    "        if 'category' in metadata:\n",
    "            print(f\"Category: {metadata.get('category', 'Unknown')}\")\n",
    "    \n",
    "    return search_results\n",
    "\n",
    "# Test different types of queries\n",
    "test_queries = [\n",
    "    \"machine learning algorithms\",\n",
    "    \"renewable energy solar power\", \n",
    "    \"web development frameworks\",\n",
    "    \"gardening in small spaces\"\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    results = semantic_search(query, n_results=2)\n",
    "    print(\"\\n\" + \"-\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced search with filtering\n",
    "print(\"\\nüîç Advanced Search: Filtering by Metadata\")\n",
    "\n",
    "def filtered_search(query: str, filters: dict = None, n_results: int = 5):\n",
    "    \"\"\"\n",
    "    Semantic search with metadata filtering\n",
    "    \"\"\"\n",
    "    search_params = {\n",
    "        \"query_texts\": [query],\n",
    "        \"n_results\": n_results,\n",
    "        \"include\": [\"documents\", \"metadatas\", \"distances\"]\n",
    "    }\n",
    "    \n",
    "    if filters:\n",
    "        search_params[\"where\"] = filters\n",
    "    \n",
    "    results = standard_vector_store.collection.query(**search_params)\n",
    "    \n",
    "    print(f\"\\nüîç Filtered Search: '{query}'\")\n",
    "    if filters:\n",
    "        print(f\"Filters: {filters}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    if results['documents'] and results['documents'][0]:\n",
    "        for i, (doc, metadata, distance) in enumerate(zip(\n",
    "            results['documents'][0],\n",
    "            results['metadatas'][0],\n",
    "            results['distances'][0]\n",
    "        )):\n",
    "            similarity = 1 - distance\n",
    "            print(f\"\\nüìÑ Result {i+1} (similarity: {similarity:.3f})\")\n",
    "            print(f\"Document: {metadata.get('document_title', 'Unknown')}\")\n",
    "            print(f\"Category: {metadata.get('category', 'Unknown')}\")\n",
    "            print(f\"Text: {doc[:100]}...\")\n",
    "    else:\n",
    "        print(\"No results found with current filters\")\n",
    "\n",
    "# Test filtered searches\n",
    "print(\"üß™ Testing different filter combinations:\")\n",
    "\n",
    "# Filter by category\n",
    "filtered_search(\n",
    "    \"programming\", \n",
    "    filters={\"category\": \"technology\"},\n",
    "    n_results=3\n",
    ")\n",
    "\n",
    "# Filter by author (if available in metadata)\n",
    "filtered_search(\n",
    "    \"sustainable practices\",\n",
    "    filters={\"author\": \"Prof. Bob Johnson\"},\n",
    "    n_results=2\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search optimization techniques\n",
    "print(\"\\n‚ö° Search Optimization Techniques\")\n",
    "\n",
    "def optimized_search(query: str, similarity_threshold: float = 0.5):\n",
    "    \"\"\"\n",
    "    Search with similarity threshold filtering\n",
    "    \"\"\"\n",
    "    # Get more results initially\n",
    "    results = standard_vector_store.collection.query(\n",
    "        query_texts=[query],\n",
    "        n_results=20,  # Get more to filter\n",
    "        include=[\"documents\", \"metadatas\", \"distances\"]\n",
    "    )\n",
    "    \n",
    "    if not results['documents'] or not results['documents'][0]:\n",
    "        return []\n",
    "    \n",
    "    # Filter by similarity threshold\n",
    "    filtered_results = []\n",
    "    for doc, metadata, distance in zip(\n",
    "        results['documents'][0],\n",
    "        results['metadatas'][0],\n",
    "        results['distances'][0]\n",
    "    ):\n",
    "        similarity = 1 - distance\n",
    "        if similarity >= similarity_threshold:\n",
    "            filtered_results.append({\n",
    "                'similarity': similarity,\n",
    "                'document': doc,\n",
    "                'metadata': metadata\n",
    "            })\n",
    "    \n",
    "    print(f\"\\nüéØ Optimized Search: '{query}' (threshold: {similarity_threshold})\")\n",
    "    print(f\"Found {len(filtered_results)} results above threshold\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for i, result in enumerate(filtered_results[:5]):  # Show top 5\n",
    "        print(f\"\\nüìÑ Result {i+1} (similarity: {result['similarity']:.3f})\")\n",
    "        print(f\"Document: {result['metadata'].get('document_title', 'Unknown')}\")\n",
    "        print(f\"Text: {result['document'][:100]}...\")\n",
    "    \n",
    "    return filtered_results\n",
    "\n",
    "# Test optimized search\n",
    "high_quality_results = optimized_search(\"artificial intelligence\", similarity_threshold=0.6)\n",
    "all_results = optimized_search(\"artificial intelligence\", similarity_threshold=0.3)\n",
    "\n",
    "print(f\"\\nüìä Search Quality Comparison:\")\n",
    "print(f\"High threshold (0.6): {len(high_quality_results)} results\")\n",
    "print(f\"Low threshold (0.3): {len(all_results)} results\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-query search for better recall\n",
    "print(\"\\nüéØ Multi-Query Search for Better Recall\")\n",
    "\n",
    "def multi_query_search(queries: List[str], n_results_per_query: int = 3):\n",
    "    \"\"\"\n",
    "    Search with multiple related queries and combine results\n",
    "    \"\"\"\n",
    "    all_results = {}\n",
    "    \n",
    "    for query in queries:\n",
    "        results = standard_vector_store.collection.query(\n",
    "            query_texts=[query],\n",
    "            n_results=n_results_per_query,\n",
    "            include=[\"documents\", \"metadatas\", \"distances\"]\n",
    "        )\n",
    "        \n",
    "        if results['documents'] and results['documents'][0]:\n",
    "            for doc, metadata, distance in zip(\n",
    "                results['documents'][0],\n",
    "                results['metadatas'][0],\n",
    "                results['distances'][0]\n",
    "            ):\n",
    "                doc_id = metadata.get('document_id', doc[:50])\n",
    "                similarity = 1 - distance\n",
    "                \n",
    "                # Keep the best similarity score for each document\n",
    "                if doc_id not in all_results or similarity > all_results[doc_id]['similarity']:\n",
    "                    all_results[doc_id] = {\n",
    "                        'similarity': similarity,\n",
    "                        'document': doc,\n",
    "                        'metadata': metadata,\n",
    "                        'matching_query': query\n",
    "                    }\n",
    "    \n",
    "    # Sort by similarity\n",
    "    sorted_results = sorted(all_results.values(), key=lambda x: x['similarity'], reverse=True)\n",
    "    \n",
    "    print(f\"\\nüîç Multi-Query Search Results\")\n",
    "    print(f\"Queries: {queries}\")\n",
    "    print(f\"Unique documents found: {len(sorted_results)}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for i, result in enumerate(sorted_results[:5]):\n",
    "        print(f\"\\nüìÑ Result {i+1} (similarity: {result['similarity']:.3f})\")\n",
    "        print(f\"Matched query: '{result['matching_query']}'\")\n",
    "        print(f\"Document: {result['metadata'].get('document_title', 'Unknown')}\")\n",
    "        print(f\"Text: {result['document'][:100]}...\")\n",
    "    \n",
    "    return sorted_results\n",
    "\n",
    "# Test multi-query search\n",
    "related_queries = [\n",
    "    \"machine learning\",\n",
    "    \"artificial intelligence\", \n",
    "    \"AI algorithms\",\n",
    "    \"neural networks\"\n",
    "]\n",
    "\n",
    "multi_results = multi_query_search(related_queries)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Performance Analysis and Best Practices\n",
    "\n",
    "Let's analyze the performance of our vector store and discuss best practices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector store analysis\n",
    "print(\"üìä Vector Store Analysis\")\n",
    "\n",
    "collection = standard_vector_store.collection\n",
    "total_documents = collection.count()\n",
    "\n",
    "# Get sample data for analysis\n",
    "sample_data = collection.get(\n",
    "    limit=min(10, total_documents),\n",
    "    include=[\"documents\", \"metadatas\", \"embeddings\"]\n",
    ")\n",
    "\n",
    "print(f\"üìà Collection Statistics:\")\n",
    "print(f\"  Total chunks: {total_documents}\")\n",
    "print(f\"  Collection name: {collection.name}\")\n",
    "print(f\"  Storage location: {standard_vector_store.persist_directory}\")\n",
    "\n",
    "if sample_data['documents']:\n",
    "    # Analyze chunk sizes\n",
    "    chunk_sizes = [len(doc) for doc in sample_data['documents']]\n",
    "    avg_chunk_size = sum(chunk_sizes) / len(chunk_sizes)\n",
    "    min_chunk_size = min(chunk_sizes)\n",
    "    max_chunk_size = max(chunk_sizes)\n",
    "    \n",
    "    print(f\"\\nüìè Chunk Size Analysis:\")\n",
    "    print(f\"  Average chunk size: {avg_chunk_size:.0f} characters\")\n",
    "    print(f\"  Min chunk size: {min_chunk_size} characters\")\n",
    "    print(f\"  Max chunk size: {max_chunk_size} characters\")\n",
    "    \n",
    "    # Analyze metadata\n",
    "    if sample_data['metadatas']:\n",
    "        metadata_keys = set()\n",
    "        for metadata in sample_data['metadatas']:\n",
    "            metadata_keys.update(metadata.keys())\n",
    "        \n",
    "        print(f\"\\nüè∑Ô∏è Metadata Analysis:\")\n",
    "        print(f\"  Unique metadata keys: {sorted(metadata_keys)}\")\n",
    "    \n",
    "    # Analyze embeddings\n",
    "    if sample_data['embeddings'] and sample_data['embeddings'][0]:\n",
    "        embedding_dim = len(sample_data['embeddings'][0])\n",
    "        print(f\"\\nüß† Embedding Analysis:\")\n",
    "        print(f\"  Embedding dimensionality: {embedding_dim}\")\n",
    "        print(f\"  Expected dimensionality: {standard_vector_store.dimensionality}\")\n",
    "        print(f\"  ‚úÖ Dimensions match\" if embedding_dim == standard_vector_store.dimensionality else \"‚ùå Dimension mismatch!\")\n",
    "\n",
    "# Storage analysis\n",
    "storage_path = Path(standard_vector_store.persist_directory)\n",
    "if storage_path.exists():\n",
    "    print(f\"\\nüíæ Storage Analysis:\")\n",
    "    total_size = 0\n",
    "    file_count = 0\n",
    "    \n",
    "    for item in storage_path.rglob('*'):\n",
    "        if item.is_file():\n",
    "            size = item.stat().st_size\n",
    "            total_size += size\n",
    "            file_count += 1\n",
    "            if size > 1024:  # Show files larger than 1KB\n",
    "                size_mb = size / (1024 * 1024)\n",
    "                print(f\"  {item.name}: {size_mb:.2f} MB\")\n",
    "    \n",
    "    print(f\"  Total files: {file_count}\")\n",
    "    print(f\"  Total storage: {total_size / (1024 * 1024):.2f} MB\")\n",
    "    print(f\"  Storage per chunk: {total_size / total_documents / 1024:.1f} KB\" if total_documents > 0 else \"  No chunks\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance benchmarking\n",
    "print(\"\\n‚ö° Performance Benchmarking\")\n",
    "\n",
    "import time\n",
    "\n",
    "def benchmark_search(queries: List[str], n_runs: int = 3):\n",
    "    \"\"\"\n",
    "    Benchmark search performance\n",
    "    \"\"\"\n",
    "    times = []\n",
    "    \n",
    "    for run in range(n_runs):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        for query in queries:\n",
    "            collection.query(\n",
    "                query_texts=[query],\n",
    "                n_results=5,\n",
    "                include=[\"documents\", \"metadatas\"]\n",
    "            )\n",
    "        \n",
    "        total_time = time.time() - start_time\n",
    "        times.append(total_time)\n",
    "    \n",
    "    avg_time = sum(times) / len(times)\n",
    "    avg_time_per_query = avg_time / len(queries)\n",
    "    \n",
    "    print(f\"üèÉ Benchmark Results ({n_runs} runs, {len(queries)} queries each):\")\n",
    "    print(f\"  Average total time: {avg_time:.3f} seconds\")\n",
    "    print(f\"  Average time per query: {avg_time_per_query:.3f} seconds\")\n",
    "    print(f\"  Queries per second: {1/avg_time_per_query:.1f}\")\n",
    "    \n",
    "    return avg_time_per_query\n",
    "\n",
    "# Run benchmark\n",
    "benchmark_queries = [\n",
    "    \"technology\",\n",
    "    \"environment\", \n",
    "    \"programming\",\n",
    "    \"sustainable energy\",\n",
    "    \"machine learning algorithms\"\n",
    "]\n",
    "\n",
    "avg_query_time = benchmark_search(benchmark_queries)\n",
    "\n",
    "# Performance recommendations\n",
    "print(f\"\\nüí° Performance Recommendations:\")\n",
    "if avg_query_time > 0.5:\n",
    "    print(f\"  ‚ö†Ô∏è Query time ({avg_query_time:.3f}s) is high. Consider:\")\n",
    "    print(f\"     - Reducing embedding dimensionality\")\n",
    "    print(f\"     - Using fewer chunks (larger chunk size)\")\n",
    "    print(f\"     - Adding more RAM/faster storage\")\n",
    "elif avg_query_time > 0.1:\n",
    "    print(f\"  ‚úÖ Query time ({avg_query_time:.3f}s) is acceptable\")\n",
    "    print(f\"     - Consider optimization for production workloads\")\n",
    "else:\n",
    "    print(f\"  üöÄ Query time ({avg_query_time:.3f}s) is excellent!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best practices summary\n",
    "print(\"\\nüìã Best Practices Summary\")\n",
    "\n",
    "best_practices = {\n",
    "    \"Data Preparation\": [\n",
    "        \"Clean and normalize text before processing\",\n",
    "        \"Preserve important metadata for filtering\", \n",
    "        \"Consider document structure when chunking\",\n",
    "        \"Remove or handle special characters appropriately\"\n",
    "    ],\n",
    "    \"Chunking Strategy\": [\n",
    "        \"Balance chunk size vs. context preservation\",\n",
    "        \"Use overlap to maintain context across chunks\",\n",
    "        \"Consider domain-specific chunking (e.g., by paragraphs, sections)\",\n",
    "        \"Test different chunk sizes for your use case\"\n",
    "    ],\n",
    "    \"Vector Store Configuration\": [\n",
    "        \"Choose embedding model based on your domain\",\n",
    "        \"Use appropriate dimensionality for your needs\",\n",
    "        \"Configure batch sizes based on available resources\",\n",
    "        \"Monitor embedding API costs and usage\"\n",
    "    ],\n",
    "    \"Search Optimization\": [\n",
    "        \"Use similarity thresholds to filter low-quality results\",\n",
    "        \"Implement metadata filtering for precise searches\",\n",
    "        \"Consider multi-query approaches for better recall\",\n",
    "        \"Cache frequently used search results\"\n",
    "    ],\n",
    "    \"Production Deployment\": [\n",
    "        \"Use persistent storage volumes\",\n",
    "        \"Implement backup and recovery strategies\", \n",
    "        \"Monitor performance and resource usage\",\n",
    "        \"Plan for scaling as data grows\",\n",
    "        \"Implement proper error handling and retries\"\n",
    "    ],\n",
    "    \"Cost Management\": [\n",
    "        \"Use pre-computed embeddings when possible\",\n",
    "        \"Batch embedding operations efficiently\",\n",
    "        \"Monitor embedding API usage and costs\",\n",
    "        \"Consider alternative embedding models for different use cases\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "for category, practices in best_practices.items():\n",
    "    print(f\"\\nüîß {category}:\")\n",
    "    for practice in practices:\n",
    "        print(f\"   ‚Ä¢ {practice}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Common Use Cases and Patterns\n",
    "\n",
    "Let's explore some common patterns for different types of applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Case 1: Document Q&A System\n",
    "print(\"üìö Use Case 1: Document Q&A System\")\n",
    "\n",
    "def document_qa(question: str, n_context_chunks: int = 3):\n",
    "    \"\"\"\n",
    "    Simple document Q&A using vector search + context\n",
    "    \"\"\"\n",
    "    # Search for relevant chunks\n",
    "    results = standard_vector_store.collection.query(\n",
    "        query_texts=[question],\n",
    "        n_results=n_context_chunks,\n",
    "        include=[\"documents\", \"metadatas\", \"distances\"]\n",
    "    )\n",
    "    \n",
    "    if not results['documents'] or not results['documents'][0]:\n",
    "        return \"No relevant information found.\"\n",
    "    \n",
    "    # Combine context from top chunks\n",
    "    context_parts = []\n",
    "    for doc, metadata, distance in zip(\n",
    "        results['documents'][0],\n",
    "        results['metadatas'][0],\n",
    "        results['distances'][0]\n",
    "    ):\n",
    "        similarity = 1 - distance\n",
    "        if similarity > 0.5:  # Only use high-quality matches\n",
    "            title = metadata.get('document_title', 'Unknown Document')\n",
    "            context_parts.append(f\"From '{title}': {doc}\")\n",
    "    \n",
    "    if not context_parts:\n",
    "        return \"No sufficiently relevant information found.\"\n",
    "    \n",
    "    # In a real system, you'd send this to an LLM\n",
    "    context = \"\\n\\n\".join(context_parts)\n",
    "    \n",
    "    return {\n",
    "        'question': question,\n",
    "        'context': context,\n",
    "        'sources': len(context_parts)\n",
    "    }\n",
    "\n",
    "# Test Q&A\n",
    "qa_questions = [\n",
    "    \"What are the benefits of machine learning?\",\n",
    "    \"How do solar panels work?\",\n",
    "    \"What is quantum computing?\"\n",
    "]\n",
    "\n",
    "for question in qa_questions:\n",
    "    answer = document_qa(question)\n",
    "    print(f\"\\n‚ùì Question: {question}\")\n",
    "    if isinstance(answer, dict):\n",
    "        print(f\"üìñ Found context from {answer['sources']} sources\")\n",
    "        print(f\"Context preview: {answer['context'][:200]}...\")\n",
    "    else:\n",
    "        print(f\"Answer: {answer}\")\n",
    "    print(\"-\" * 40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Case 2: Content Recommendation System\n",
    "print(\"\\nüéØ Use Case 2: Content Recommendation System\")\n",
    "\n",
    "def recommend_similar_content(document_id: str, n_recommendations: int = 3):\n",
    "    \"\"\"\n",
    "    Recommend similar content based on a document\n",
    "    \"\"\"\n",
    "    # Find the original document\n",
    "    original_doc = standard_vector_store.collection.get(\n",
    "        where={\"document_id\": document_id},\n",
    "        include=[\"documents\", \"metadatas\"]\n",
    "    )\n",
    "    \n",
    "    if not original_doc['documents']:\n",
    "        return f\"Document {document_id} not found\"\n",
    "    \n",
    "    # Use the document content as query\n",
    "    query_text = original_doc['documents'][0]\n",
    "    \n",
    "    # Search for similar documents (excluding the original)\n",
    "    results = standard_vector_store.collection.query(\n",
    "        query_texts=[query_text],\n",
    "        n_results=n_recommendations + 5,  # Get extra to filter out original\n",
    "        include=[\"documents\", \"metadatas\", \"distances\"]\n",
    "    )\n",
    "    \n",
    "    recommendations = []\n",
    "    for doc, metadata, distance in zip(\n",
    "        results['documents'][0],\n",
    "        results['metadatas'][0],\n",
    "        results['distances'][0]\n",
    "    ):\n",
    "        # Skip the original document\n",
    "        if metadata.get('document_id') == document_id:\n",
    "            continue\n",
    "            \n",
    "        similarity = 1 - distance\n",
    "        recommendations.append({\n",
    "            'document_id': metadata.get('document_id'),\n",
    "            'title': metadata.get('document_title', 'Unknown'),\n",
    "            'similarity': similarity,\n",
    "            'category': metadata.get('category', 'Unknown'),\n",
    "            'preview': doc[:100] + \"...\"\n",
    "        })\n",
    "        \n",
    "        if len(recommendations) >= n_recommendations:\n",
    "            break\n",
    "    \n",
    "    return recommendations\n",
    "\n",
    "# Test recommendations\n",
    "print(f\"üîç Testing content recommendations:\")\n",
    "\n",
    "# Get a document ID from our collection\n",
    "sample_docs = standard_vector_store.collection.get(\n",
    "    limit=1,\n",
    "    include=[\"metadatas\"]\n",
    ")\n",
    "\n",
    "if sample_docs['metadatas']:\n",
    "    test_doc_id = sample_docs['metadatas'][0].get('document_id')\n",
    "    recommendations = recommend_similar_content(test_doc_id)\n",
    "    \n",
    "    print(f\"\\nüìÑ Recommendations for document: {test_doc_id}\")\n",
    "    \n",
    "    if isinstance(recommendations, list):\n",
    "        for i, rec in enumerate(recommendations):\n",
    "            print(f\"\\n{i+1}. {rec['title']} (similarity: {rec['similarity']:.3f})\")\n",
    "            print(f\"   Category: {rec['category']}\")\n",
    "            print(f\"   Preview: {rec['preview']}\")\n",
    "    else:\n",
    "        print(recommendations)\n",
    "else:\n",
    "    print(\"No documents available for testing\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Case 3: Content Classification and Tagging\n",
    "print(\"\\nüè∑Ô∏è Use Case 3: Content Classification and Tagging\")\n",
    "\n",
    "def auto_tag_content(text: str, existing_tags: List[str] = None):\n",
    "    \"\"\"\n",
    "    Automatically suggest tags for content based on similar documents\n",
    "    \"\"\"\n",
    "    if existing_tags is None:\n",
    "        existing_tags = [\"technology\", \"environment\", \"lifestyle\", \"education\", \"business\"]\n",
    "    \n",
    "    # Search for similar content\n",
    "    results = standard_vector_store.collection.query(\n",
    "        query_texts=[text],\n",
    "        n_results=10,\n",
    "        include=[\"documents\", \"metadatas\", \"distances\"]\n",
    "    )\n",
    "    \n",
    "    if not results['documents'] or not results['documents'][0]:\n",
    "        return []\n",
    "    \n",
    "    # Analyze tags from similar documents\n",
    "    tag_scores = {}\n",
    "    \n",
    "    for doc, metadata, distance in zip(\n",
    "        results['documents'][0],\n",
    "        results['metadatas'][0],\n",
    "        results['distances'][0]\n",
    "    ):\n",
    "        similarity = 1 - distance\n",
    "        if similarity < 0.3:  # Skip low-similarity results\n",
    "            continue\n",
    "            \n",
    "        # Extract tags from metadata\n",
    "        doc_category = metadata.get('category', '')\n",
    "        doc_tags = metadata.get('tags', [])\n",
    "        \n",
    "        # Score category\n",
    "        if doc_category:\n",
    "            tag_scores[doc_category] = tag_scores.get(doc_category, 0) + similarity\n",
    "        \n",
    "        # Score individual tags\n",
    "        if isinstance(doc_tags, list):\n",
    "            for tag in doc_tags:\n",
    "                tag_scores[tag] = tag_scores.get(tag, 0) + similarity * 0.5\n",
    "    \n",
    "    # Sort tags by score\n",
    "    suggested_tags = sorted(tag_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return suggested_tags[:5]  # Return top 5 suggestions\n",
    "\n",
    "# Test auto-tagging\n",
    "test_texts = [\n",
    "    \"Deep learning neural networks are revolutionizing computer vision and natural language processing applications.\",\n",
    "    \"Solar energy and wind power are becoming more cost-effective alternatives to fossil fuels for electricity generation.\",\n",
    "    \"Container gardening allows urban dwellers to grow vegetables and herbs in small apartment spaces.\"\n",
    "]\n",
    "\n",
    "for i, text in enumerate(test_texts):\n",
    "    print(f\"\\nüìù Text {i+1}: {text[:80]}...\")\n",
    "    \n",
    "    suggested_tags = auto_tag_content(text)\n",
    "    \n",
    "    print(f\"üè∑Ô∏è Suggested tags:\")\n",
    "    for tag, score in suggested_tags:\n",
    "        print(f\"   {tag} (confidence: {score:.2f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Cleanup and Next Steps\n",
    "\n",
    "Let's clean up our tutorial resources and discuss production considerations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup and summary\n",
    "print(\"üßπ Cleanup and Summary\")\n",
    "\n",
    "# Show what was created\n",
    "print(f\"\\nüìÅ Files created during tutorial:\")\n",
    "print(f\"  JSON data: {json_file_path}\")\n",
    "print(f\"  Vector store: {standard_vector_store.persist_directory}\")\n",
    "print(f\"  Chunks storage: {standard_vector_store.arrow_save_dir}\")\n",
    "\n",
    "# Storage usage\n",
    "storage_path = Path(temp_dir)\n",
    "total_size = sum(f.stat().st_size for f in storage_path.rglob('*') if f.is_file())\n",
    "print(f\"  Total storage used: {total_size / (1024 * 1024):.2f} MB\")\n",
    "\n",
    "# Final statistics\n",
    "final_count = standard_vector_store.collection.count()\n",
    "print(f\"\\nüìä Final Statistics:\")\n",
    "print(f\"  Documents processed: {len(input_documents)}\")\n",
    "print(f\"  Chunks created: {final_count}\")\n",
    "print(f\"  Vector store collection: {standard_vector_store.collection.name}\")\n",
    "\n",
    "print(f\"\\nüóëÔ∏è Cleanup:\")\n",
    "print(f\"To remove tutorial files: rm -rf {temp_dir}\")\n",
    "print(f\"(Uncomment the line below to auto-cleanup)\")\n",
    "\n",
    "# Uncomment to actually clean up\n",
    "# import shutil\n",
    "# shutil.rmtree(temp_dir)\n",
    "# print(\"‚úÖ Tutorial files cleaned up\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Production readiness checklist\n",
    "print(\"\\nüöÄ Production Readiness Checklist\")\n",
    "\n",
    "production_checklist = {\n",
    "    \"‚úÖ Essential\": [\n",
    "        \"Set up persistent storage volumes\",\n",
    "        \"Configure backup and recovery procedures\",\n",
    "        \"Implement proper error handling and logging\",\n",
    "        \"Set up monitoring and alerting\",\n",
    "        \"Secure API keys and credentials\",\n",
    "        \"Test with production data volumes\"\n",
    "    ],\n",
    "    \"‚ö° Performance\": [\n",
    "        \"Optimize chunk sizes for your domain\",\n",
    "        \"Configure batch sizes based on resources\",\n",
    "        \"Implement caching for frequent queries\",\n",
    "        \"Consider using pre-computed embeddings\",\n",
    "        \"Set up load balancing if needed\",\n",
    "        \"Monitor embedding API usage and costs\"\n",
    "    ],\n",
    "    \"üîí Security\": [\n",
    "        \"Implement authentication and authorization\",\n",
    "        \"Encrypt data at rest and in transit\", \n",
    "        \"Set up network security (VPC, firewalls)\",\n",
    "        \"Regular security audits and updates\",\n",
    "        \"Data privacy compliance (GDPR, etc.)\",\n",
    "        \"Secure API endpoints\"\n",
    "    ],\n",
    "    \"üìà Scalability\": [\n",
    "        \"Plan for horizontal scaling\",\n",
    "        \"Implement data partitioning strategies\",\n",
    "        \"Set up auto-scaling policies\",\n",
    "        \"Consider distributed vector stores\",\n",
    "        \"Plan for disaster recovery\",\n",
    "        \"Monitor resource utilization\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "for category, items in production_checklist.items():\n",
    "    print(f\"\\n{category}:\")\n",
    "    for item in items:\n",
    "        print(f\"   ‚ñ° {item}\")\n",
    "\n",
    "print(f\"\\nüéì What You've Learned:\")\n",
    "learning_outcomes = [\n",
    "    \"How to load JSON data using Buttermilk's data loaders\",\n",
    "    \"Different vector store configuration strategies\",\n",
    "    \"Text chunking techniques for optimal search\",\n",
    "    \"Embedding generation and storage workflows\",\n",
    "    \"Semantic search implementation and optimization\",\n",
    "    \"Common use cases and design patterns\",\n",
    "    \"Performance analysis and best practices\",\n",
    "    \"Production deployment considerations\"\n",
    "]\n",
    "\n",
    "for outcome in learning_outcomes:\n",
    "    print(f\"   ‚úì {outcome}\")\n",
    "\n",
    "print(f\"\\nüìö Additional Resources:\")\n",
    "print(f\"   ‚Ä¢ OSB Vector Database Guide: /docs/osb_vector_database_guide.md\")\n",
    "print(f\"   ‚Ä¢ OSB Example Notebook: /examples/osb_vector_example.ipynb\")\n",
    "print(f\"   ‚Ä¢ Buttermilk Documentation: Check the /docs directory\")\n",
    "print(f\"   ‚Ä¢ Vector Store Tests: /tests/flows/test_embeddings.py\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "üéâ **Congratulations!** You've successfully completed the JSON-to-Vector Database tutorial.\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Flexible Data Loading**: Buttermilk's data loading infrastructure supports various JSON formats and sources\n",
    "2. **Configurable Vector Stores**: ChromaDB integration provides powerful, scalable vector search capabilities\n",
    "3. **Chunking Strategies**: Different approaches optimize for different use cases (precision vs. context)\n",
    "4. **Search Optimization**: Multiple techniques improve search quality and performance\n",
    "5. **Production Ready**: Built-in support for async processing, error handling, and monitoring\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- **Experiment** with your own JSON datasets\n",
    "- **Optimize** chunking and embedding strategies for your domain\n",
    "- **Integrate** with Buttermilk flows for automated processing\n",
    "- **Scale** to production with proper infrastructure\n",
    "- **Monitor** performance and costs in production\n",
    "\n",
    "### Architecture Benefits\n",
    "\n",
    "Buttermilk's vector database infrastructure provides:\n",
    "\n",
    "- üîß **Modular Design**: Mix and match components as needed\n",
    "- üöÄ **Async Processing**: Scalable for large datasets\n",
    "- ‚òÅÔ∏è **Cloud Integration**: Native support for GCS, BigQuery, etc.\n",
    "- üîç **Rich Metadata**: Powerful filtering and search capabilities\n",
    "- üìä **Observable**: Built-in logging and tracing\n",
    "- üõ°Ô∏è **Robust**: Error handling and retry mechanisms\n",
    "\n",
    "Ready to build amazing search and AI applications with your JSON data! üöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
