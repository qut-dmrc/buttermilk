{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OSB Vector Database Example\n",
    "\n",
    "This notebook demonstrates how to work with the Online Safety Bureau (OSB) dataset using Buttermilk's vector database infrastructure.\n",
    "\n",
    "## Overview\n",
    "\n",
    "The OSB dataset contains legal case summaries and decisions from the Online Safety Bureau. This example shows how to:\n",
    "\n",
    "1. Load OSB JSON data from Google Cloud Storage\n",
    "2. Create and populate a ChromaDB vector store\n",
    "3. Perform semantic search queries\n",
    "4. Use the OSB expert prompt template for legal analysis\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Buttermilk environment properly configured\n",
    "- Access to Google Cloud Storage (gs://prosocial-public/osb/)\n",
    "- Vertex AI embedding model access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Buttermilk with OSB configuration\n",
    "from buttermilk.utils.nb import init\n",
    "from buttermilk._core.dmrc import get_bm, set_bm\n",
    "from rich import print\n",
    "import asyncio\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Initialize with OSB flow configuration\n",
    "cfg = init(job=\"osb_example\", overrides=[\"flows=[osb]\"])\n",
    "set_bm(cfg.bm)\n",
    "bm = get_bm()\n",
    "\n",
    "print(\"‚úÖ Buttermilk initialized for OSB example\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Exploring the OSB Configuration\n",
    "\n",
    "Let's examine the existing OSB configuration to understand how it's structured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the OSB flow configuration\n",
    "print(\"OSB Flow Configuration:\")\n",
    "print(json.dumps(cfg.flows.osb.model_dump(), indent=2))\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"OSB Data Source Configuration:\")\n",
    "if hasattr(cfg.flows.osb, 'storage') and 'cases' in cfg.flows.osb.storage:\n",
    "    print(json.dumps(cfg.flows.osb.storage.cases.model_dump(), indent=2))\n",
    "else:\n",
    "    print(\"Data source configuration not found in expected location\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loading OSB JSON Data\n",
    "\n",
    "The OSB dataset is stored as a JSON file in Google Cloud Storage. Let's load it using Buttermilk's data loading infrastructure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from buttermilk._core.config import DataSourceConfig\n",
    "from buttermilk.data.loaders import create_data_loader\n",
    "\n",
    "# Create data source configuration for OSB JSON file\n",
    "osb_config = DataSourceConfig(\n",
    "    type=\"file\",\n",
    "    path=\"gs://prosocial-public/osb/03_osb_fulltext_summaries.json\",\n",
    "    # Map JSON fields to Record fields if needed\n",
    "    columns={\n",
    "        \"content\": \"text\",  # Map 'text' field to 'content'\n",
    "        \"record_id\": \"id\"   # Map 'id' field to 'record_id'\n",
    "    }\n",
    ")\n",
    "\n",
    "# Create data loader\n",
    "loader = create_data_loader(osb_config)\n",
    "\n",
    "print(f\"Created data loader: {type(loader).__name__}\")\n",
    "print(f\"Loading OSB data from: {osb_config.path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and examine the first few records\n",
    "osb_records = list(loader)\n",
    "print(f\"‚úÖ Loaded {len(osb_records)} OSB records\")\n",
    "\n",
    "# Examine the first record\n",
    "if osb_records:\n",
    "    first_record = osb_records[0]\n",
    "    print(\"\\nFirst record structure:\")\n",
    "    print(f\"Record ID: {first_record.record_id}\")\n",
    "    print(f\"Content preview: {first_record.content[:200]}...\")\n",
    "    print(f\"Metadata keys: {list(first_record.metadata.keys()) if first_record.metadata else 'None'}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No records loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Setting Up ChromaDB Vector Store\n",
    "\n",
    "Now let's create a ChromaDB vector store to index the OSB documents for semantic search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from buttermilk.data.vector import ChromaDBEmbeddings, InputDocument, DefaultTextSplitter\n",
    "import tempfile\n",
    "\n",
    "# Create a temporary directory for our vector store\n",
    "temp_dir = tempfile.mkdtemp()\n",
    "print(f\"Vector store will be created in: {temp_dir}\")\n",
    "\n",
    "# Initialize ChromaDB vector store\n",
    "vector_store = ChromaDBEmbeddings(\n",
    "    collection_name=\"osb_cases\",\n",
    "    persist_directory=temp_dir,\n",
    "    embedding_model=\"text-embedding-005\",  # Using the same model as OSB config\n",
    "    dimensionality=3072,  # Vertex AI text-embedding-005 dimensionality\n",
    "    concurrency=5,  # Reduce for example\n",
    "    arrow_save_dir=f\"{temp_dir}/chunks\"\n",
    ")\n",
    "\n",
    "print(\"‚úÖ ChromaDB vector store initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the cache for remote ChromaDB (if needed)\n",
    "await vector_store.ensure_cache_initialized()\n",
    "print(\"‚úÖ Vector store cache initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Converting Records to InputDocuments\n",
    "\n",
    "We need to convert Buttermilk Records to InputDocuments for the vector store processing pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Records to InputDocuments\n",
    "input_docs = []\n",
    "\n",
    "for record in osb_records[:10]:  # Process first 10 for example\n",
    "    input_doc = InputDocument(\n",
    "        record_id=record.record_id,\n",
    "        title=record.metadata.get('title', f\"OSB Case {record.record_id}\") if record.metadata else f\"OSB Case {record.record_id}\",\n",
    "        full_text=record.content,\n",
    "        file_path=\"\",  # Not from file\n",
    "        metadata=record.metadata or {}\n",
    "    )\n",
    "    input_docs.append(input_doc)\n",
    "\n",
    "print(f\"‚úÖ Created {len(input_docs)} InputDocuments for processing\")\n",
    "print(f\"Example document title: {input_docs[0].title}\")\n",
    "print(f\"Example document text length: {len(input_docs[0].full_text)} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Processing Documents: Chunking and Embedding\n",
    "\n",
    "Now we'll process the documents through the complete pipeline: chunking, embedding, and storing in ChromaDB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from buttermilk.data.vector import list_to_async_iterator\n",
    "\n",
    "# Create text splitter for chunking\n",
    "text_splitter = DefaultTextSplitter(\n",
    "    chunk_size=1000,  # Smaller chunks for legal text\n",
    "    chunk_overlap=200\n",
    ")\n",
    "\n",
    "print(\"Processing documents through the vector store pipeline...\")\n",
    "print(\"This includes: chunking ‚Üí embedding ‚Üí storing in ChromaDB\")\n",
    "\n",
    "# Process documents one by one to show progress\n",
    "successful_docs = 0\n",
    "total_chunks = 0\n",
    "\n",
    "for i, doc in enumerate(input_docs):\n",
    "    print(f\"\\nProcessing document {i+1}/{len(input_docs)}: {doc.title}\")\n",
    "    \n",
    "    # Step 1: Chunk the document\n",
    "    chunked_doc = await text_splitter.process(doc)\n",
    "    if not chunked_doc or not chunked_doc.chunks:\n",
    "        print(f\"  ‚ö†Ô∏è Failed to chunk document {doc.record_id}\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"  üìÑ Created {len(chunked_doc.chunks)} chunks\")\n",
    "    \n",
    "    # Step 2: Embed and store in ChromaDB\n",
    "    processed_doc = await vector_store.process(chunked_doc)\n",
    "    if processed_doc:\n",
    "        successful_docs += 1\n",
    "        total_chunks += len(processed_doc.chunks)\n",
    "        print(f\"  ‚úÖ Successfully embedded and stored {len(processed_doc.chunks)} chunks\")\n",
    "    else:\n",
    "        print(f\"  ‚ùå Failed to process document {doc.record_id}\")\n",
    "\n",
    "print(f\"\\nüéâ Pipeline complete!\")\n",
    "print(f\"Successfully processed {successful_docs}/{len(input_docs)} documents\")\n",
    "print(f\"Total chunks stored: {total_chunks}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Performing Semantic Search\n",
    "\n",
    "Now let's test the vector store by performing semantic searches on the OSB data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test semantic search\n",
    "def search_osb_cases(query: str, n_results: int = 5):\n",
    "    \"\"\"\n",
    "    Search OSB cases using semantic similarity\n",
    "    \"\"\"\n",
    "    try:\n",
    "        results = vector_store.collection.query(\n",
    "            query_texts=[query],\n",
    "            n_results=n_results,\n",
    "            include=[\"documents\", \"metadatas\", \"distances\"]\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nüîç Search results for: '{query}'\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        if results['documents'] and results['documents'][0]:\n",
    "            for i, (doc, metadata, distance) in enumerate(zip(\n",
    "                results['documents'][0],\n",
    "                results['metadatas'][0],\n",
    "                results['distances'][0]\n",
    "            )):\n",
    "                print(f\"\\nüìÑ Result {i+1} (similarity: {1-distance:.3f})\")\n",
    "                print(f\"Document: {metadata.get('document_title', 'Unknown')}\")\n",
    "                print(f\"Chunk: {metadata.get('chunk_index', 'Unknown')}\")\n",
    "                print(f\"Text: {doc[:200]}...\")\n",
    "                print(\"-\" * 40)\n",
    "        else:\n",
    "            print(\"No results found\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Search error: {e}\")\n",
    "\n",
    "# Test different types of legal queries\n",
    "queries = [\n",
    "    \"harmful content moderation\",\n",
    "    \"platform responsibility for user-generated content\",\n",
    "    \"misinformation and disinformation\",\n",
    "    \"child safety online\"\n",
    "]\n",
    "\n",
    "for query in queries:\n",
    "    search_osb_cases(query, n_results=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Using the OSB Expert Prompt Template\n",
    "\n",
    "Let's demonstrate how to use the OSB-specific prompt template for legal analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from buttermilk.utils.templating import render_template\n",
    "from buttermilk._core.types import Record\n",
    "\n",
    "# Search for relevant cases\n",
    "query = \"content moderation appeals process\"\n",
    "search_results = vector_store.collection.query(\n",
    "    query_texts=[query],\n",
    "    n_results=3,\n",
    "    include=[\"documents\", \"metadatas\"]\n",
    ")\n",
    "\n",
    "# Convert search results to the format expected by the template\n",
    "dataset = []\n",
    "if search_results['documents'] and search_results['documents'][0]:\n",
    "    for doc, metadata in zip(search_results['documents'][0], search_results['metadatas'][0]):\n",
    "        case_data = {\n",
    "            'title': metadata.get('document_title', 'Unknown Case'),\n",
    "            'text': doc,\n",
    "            'record_id': metadata.get('document_id', 'unknown')\n",
    "        }\n",
    "        dataset.append(case_data)\n",
    "\n",
    "print(f\"Found {len(dataset)} relevant cases for analysis\")\n",
    "\n",
    "# Render the OSB expert template\n",
    "if dataset:\n",
    "    prompt_context = {\n",
    "        'dataset': dataset,\n",
    "        'formatting': 'Provide a structured analysis with clear conclusions',\n",
    "        'prompt': query\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        rendered_prompt = render_template('osb', prompt_context)\n",
    "        print(\"\\nüìã OSB Expert Prompt:\")\n",
    "        print(\"=\"*80)\n",
    "        print(rendered_prompt)\n",
    "    except Exception as e:\n",
    "        print(f\"Error rendering template: {e}\")\n",
    "        print(\"Template may not be available in current setup\")\n",
    "else:\n",
    "    print(\"No cases found for template demonstration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Advanced Search with Filters\n",
    "\n",
    "ChromaDB supports metadata filtering for more precise searches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of filtered search (if metadata contains relevant fields)\n",
    "def filtered_search(query: str, filters: dict = None, n_results: int = 5):\n",
    "    \"\"\"\n",
    "    Search with metadata filters\n",
    "    \"\"\"\n",
    "    search_params = {\n",
    "        \"query_texts\": [query],\n",
    "        \"n_results\": n_results,\n",
    "        \"include\": [\"documents\", \"metadatas\", \"distances\"]\n",
    "    }\n",
    "    \n",
    "    if filters:\n",
    "        search_params[\"where\"] = filters\n",
    "    \n",
    "    results = vector_store.collection.query(**search_params)\n",
    "    \n",
    "    print(f\"\\nüîç Filtered search: '{query}'\")\n",
    "    if filters:\n",
    "        print(f\"Filters: {filters}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    if results['documents'] and results['documents'][0]:\n",
    "        for i, (doc, metadata, distance) in enumerate(zip(\n",
    "            results['documents'][0],\n",
    "            results['metadatas'][0], \n",
    "            results['distances'][0]\n",
    "        )):\n",
    "            print(f\"\\nüìÑ Result {i+1} (similarity: {1-distance:.3f})\")\n",
    "            print(f\"Text: {doc[:150]}...\")\n",
    "    else:\n",
    "        print(\"No results found\")\n",
    "\n",
    "# Try different searches\n",
    "filtered_search(\"online safety\", n_results=3)\n",
    "\n",
    "# Example with filters (adjust based on actual metadata structure)\n",
    "# filtered_search(\"platform liability\", filters={\"document_title\": {\"$contains\": \"Case\"}}, n_results=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Collection Statistics and Management\n",
    "\n",
    "Let's examine what we've created and learn about managing the vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get collection statistics\n",
    "collection = vector_store.collection\n",
    "count = collection.count()\n",
    "\n",
    "print(f\"üìä ChromaDB Collection Statistics:\")\n",
    "print(f\"Collection name: {collection.name}\")\n",
    "print(f\"Total chunks: {count}\")\n",
    "print(f\"Storage directory: {vector_store.persist_directory}\")\n",
    "\n",
    "# Sample some records to see metadata structure\n",
    "if count > 0:\n",
    "    sample = collection.get(limit=3, include=[\"metadatas\"])\n",
    "    print(f\"\\nSample metadata keys:\")\n",
    "    for i, metadata in enumerate(sample['metadatas']):\n",
    "        print(f\"  Record {i+1}: {list(metadata.keys())}\")\n",
    "\n",
    "# Show storage directory contents\n",
    "storage_path = Path(vector_store.persist_directory)\n",
    "if storage_path.exists():\n",
    "    print(f\"\\nüìÅ Storage directory contents:\")\n",
    "    for item in storage_path.iterdir():\n",
    "        if item.is_file():\n",
    "            size = item.stat().st_size\n",
    "            print(f\"  {item.name}: {size:,} bytes\")\n",
    "        else:\n",
    "            print(f\"  {item.name}/ (directory)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Cleanup and Next Steps\n",
    "\n",
    "Finally, let's clean up and discuss next steps for production usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "print(\"üßπ Cleanup options:\")\n",
    "print(f\"To remove temporary vector store: rm -rf {temp_dir}\")\n",
    "print(f\"To keep vector store for future use, move it to a permanent location\")\n",
    "\n",
    "# Uncomment to actually clean up\n",
    "# shutil.rmtree(temp_dir)\n",
    "# print(\"‚úÖ Temporary files cleaned up\")\n",
    "\n",
    "print(\"\\nüöÄ Next Steps:\")\n",
    "print(\"1. For production: use a permanent storage location\")\n",
    "print(\"2. Process the full OSB dataset (not just first 10 records)\")\n",
    "print(\"3. Integrate with Buttermilk flows for automated processing\")\n",
    "print(\"4. Use pre-computed embeddings from gs://prosocial-public/osb/04_osb_embeddings_vertex-005.json\")\n",
    "print(\"5. Implement custom metadata filtering based on case types\")\n",
    "print(\"6. Create custom prompt templates for specific legal analysis tasks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "‚úÖ **Loading OSB JSON data** using Buttermilk's data loading infrastructure  \n",
    "‚úÖ **Creating ChromaDB vector store** with proper configuration for legal text  \n",
    "‚úÖ **Processing documents** through chunking, embedding, and storage pipeline  \n",
    "‚úÖ **Performing semantic search** to find relevant legal cases  \n",
    "‚úÖ **Using OSB expert template** for structured legal analysis  \n",
    "‚úÖ **Advanced filtering** and collection management  \n",
    "\n",
    "### Key Benefits of Buttermilk's Vector Store Infrastructure:\n",
    "\n",
    "- **Async processing** for scalable document ingestion\n",
    "- **Cloud storage integration** with automatic caching\n",
    "- **Vertex AI embeddings** for high-quality semantic search\n",
    "- **Flexible configuration** through Hydra configs\n",
    "- **Built-in error handling** and retry mechanisms\n",
    "- **Metadata preservation** for rich filtering capabilities\n",
    "\n",
    "### Production Considerations:\n",
    "\n",
    "1. **Use persistent storage** locations instead of temporary directories\n",
    "2. **Leverage pre-computed embeddings** to save time and costs\n",
    "3. **Configure appropriate chunk sizes** based on your use case\n",
    "4. **Implement monitoring** for embedding costs and performance\n",
    "5. **Consider backup strategies** for valuable vector stores"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}