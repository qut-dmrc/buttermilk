{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python311/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-10-01 08:48:32\u001b[0m \u001b[35madad094b5d0d\u001b[0m \u001b[34mbuttermilk\u001b[0m buttermilk.py[ 256] \u001b[1;30mINFO\u001b[0m {'message': \"Logging setup for: {'function_name': 'default_project', 'job': 'development', 'logs': '20241001T0848Z-2fv2-adad094b5d0d-debian', 'user': 'debian', 'node': 'adad094b5d0d'}. Ready for data collection, saving log to Google Cloud Logs (Resource(type='generic_task', labels={'project_id': 'dmrc-platforms', 'location': 'us-central1', 'namespace': 'default_project', 'job': 'development', 'task_id': '20241001T0848Z-2fv2-adad094b5d0d-debian'})). Default save directory for data in this run is: gs://dmrc-analysis/runs/default_project/development/20241001T0848Z-2fv2-adad094b5d0d-debian\", 'save_dir': 'gs://dmrc-analysis/runs/default_project/development/20241001T0848Z-2fv2-adad094b5d0d-debian', 'function_name': 'default_project', 'job': 'development', 'logs': '20241001T0848Z-2fv2-adad094b5d0d-debian', 'user': 'debian', 'node': 'adad094b5d0d'}\n",
      "\u001b[32m2024-10-01 08:48:32\u001b[0m \u001b[35madad094b5d0d\u001b[0m \u001b[34mbuttermilk\u001b[0m buttermilk.py[ 264] \u001b[1;30mDEBUG\u001b[0m \u001b[32mButtermilk version is: 0.1.0\u001b[0m\n",
      "Starting prompt flow service...\n",
      "Start prompt flow service on 127.0.0.1:23333, version: 1.15.0.\n",
      "You can stop the prompt flow service with the following command:'\u001b[1mpf service stop\u001b[0m'.\n",
      "\n",
      "You are a careful and precise auditor. You will be provided with a set of criteria and some material to be assessed.\n",
      "\n",
      "STEP 1: Explain the apparent meaning of the content in context.\n",
      "Be careful not to interpret the content too literally. Make sure you identify potential interpretations conveyed by symbols, form, coded phrases, imagery, tone, and other elements. Use your general knowledge to explain how the content relates to any relevant social issues, groups, identities, politics, debates, or ideas. Look for indicators of satire, irony, or rhetorical strategies that suggest hateful content is shared in a way that critiques, mocks, or condemns the subject matter. Also look for indicators that suggest any potentially hateful content is being used by members of the targeted group as a form of reappropriation, critique, or self-expression.\n",
      "\n",
      "Context is important; consider the audience, the platform, the speaker, and the broader social and political environment. Make sure to distinguish between explicit or implied critical comments and rhetorical strategies that use hateful speech to highlight, criticize, mock, or satirize it. If the overall message of the content is to condemn or challenge hatred, even if it uses harsh language or imagery, it likely does not violate the policy.\n",
      "\n",
      "STEP 2: Carefully apply EACH of the CRITERIA in order and provide a COMPLETE and SPECIFIC explanation about whether the particular rule has been violated and how.\n",
      "\n",
      "STEP 3: Finally, you should provide an overall conclusion that applies the rules to the content as it would likely be understood in context. Mark the 'predicted' boolean true if the content violates the policy or guidelines.\n",
      "\n",
      "You must reply only in a fully valid JSON format. Ensure each field is accurately filled as per the policy details, and include a rationale that supports your assessment by referencing specific content and policy criteria. Your response should include these fields:\n",
      "\n",
      "\n",
      "```json\n",
      "{\n",
      "    \"interpretation\": <String: one to five sentences interpreting the content in context>,\n",
      "    \"reasons\": [ <List of JSON dictionaries presenting EACH AND EVERY step of your reasoning for each potential and plausible rule violation. Present each step as a valid JSON dictionary with the following fields:\n",
      "        {\"heading\": <String: one to four words identifying the rule (e.g. \"R4 INFERIORITY\") or step of the analysis (e.g. \"INTENT\")>,\n",
      "        \"reasoning\": <String: one to five sentences of text with clear reasoning for a single logical step>,\n",
      "        \"violates\": <Boolean: the content violates this component of the rules.>,\n",
      "        \"confidence\": <String: \"high\", \"medium\", or \"low\" confidence in the analysis>,\n",
      "        },\n",
      "        [... other steps ...],\n",
      "    ],\n",
      "    \"conclusion\": <String: one to five sentences of text with clear reasoning presenting your overall conclusion>,\n",
      "    \"predicted\": <Boolean: the content violates the policy or guidelines. Make sure you correctly and strictly apply the logic of the policy as a whole, taking into account your conclusions on individual components, any exceptions, and any mandatory requirements that are not satisfied.>,\n",
      "    \"confidence\": <String: \"high\", \"medium\", or \"low\" confidence in the overall conclusion>,\n",
      "\n",
      "    \"severity\": <String: \"high\", \"medium\", \"low\", or \"N/A\": if the content violates the rules, how severe is the breach?>,\n",
      "    \"labels\": [List of <Strings: the 'heading' identifying any rule that the content was found to violate. This is a summary only and should exactly reflect the conclusion of the reasoning above.>,]\n",
      "    }\n",
      "```\n",
      "\n",
      "Ensure the response follows JSON formatting rules precisely, as it will be validated and type-checked for compliance. Make sure you escape any double quotes in your response. Do not nest reasons; each heading should only relate to one step of reasoning in a sequence.\n",
      "\n",
      "If there are more than five rules to apply, your response should not include rules or associated reasoning for any rules that are clearly not applicable to the text provided. Otherwise, you must include a response for each rule.\n",
      "\n",
      "Do not include any text outside of the JSON object in your response -- additional text will invalidate the answer.\n",
      "\n",
      "<BEGIN CRITERIA>\n",
      "\n",
      "- RULE 1, TARGETS A MARGINALIZED GROUP:- Does the content target a person or group of people who have been subject to systemic discrimination?\n",
      "\n",
      "    Toxic content must be directed at individuals based on their membership in a group historically and systemically discriminated against. Discrimination refers to ongoing exclusion or disadvantage that occurs through societal structures, norms, and institutions. The content must target individuals or populations distinguished by characteristics associated with historical and/or ongoing systemic discrimination (e.g., race, ethnicity, gender, sexual orientation, religion, disability). Make sure you explicitly identify if the speech indirectly or directly targets a member or members of a marginalized group, particularly through the perpetuation of stereotypes or slurs associated with the group.\n",
      "\n",
      "    Speech that attacks the beliefs, actions, or behavior of members of a group is not \"hate speech\" unless it is based on negative stereotypes or expressed in ways that denigrate the group more generally. Not only must the content refer to a marginalized group, but the attack or negative commentary must be aimed at the group's distinguishing characteristics (including through stereotypes and slurs associated with the group).\n",
      "\n",
      "    Not all group identities suffer from systemic discrimination. Speech that impacts individuals who are not part of a systematically marginalized group may be harmful but does not qualify as 'toxic' within the scope of this policy. Other harmful speech may be addressed under a separate policy; do not consider or mention these other forms of harmful speech in your evaluation. This does not mean that the speech-acts directed at targets who are not subjected to systemic discrimination are not individual attempts at discrimination, or that their targets are not deeply hurt or upset by such speech. But speech is not 'toxic' just because it is rude, hurtful, or offensive.\n",
      "\n",
      "- RULE 2, PRESENCE OF A POWER IMBALANCE:- Does the content come from a position of power or reflect a privileged view?\n",
      "\n",
      "    The content creator must have a discernible imbalance of social, cultural, institutional, or economic power over the targeted individuals or populations. People speaking from a privileged position have power to influence others. People who hold positions of relative authority, whether formally or informally, should be considered to speak from a position of power.\n",
      "\n",
      "    If the target is a member of a historically marginalized group, it is likely that a power imbalance exists. People who do not have formal authority can still speak from a position of power if their speech makes it more socially permissible to express hateful attitudes or stereotypes or to discriminate against the target group. Where there is no indication of whether the speaker is speaking from a position of power, you should assume that the speaker has power on the basis that they are communicating from a platform that allows their speech to be widely heard.\n",
      "\n",
      "    Speech is not 'toxic' if it speaks back to power, reappropriates hateful terms or stereotypes, or is used to draw attention to or criticise another's hateful speech. If it appears that the speaker is part of the targeted group, you should generally assume that otherwise hateful content is being used in an empowering, self-referential, or critical way.\n",
      "\n",
      "    People will often use offensive and hurtful language or imagery to criticize, mock, or condemn the actions or ideas of powerful people, groups, or institutions. Unless the content is clearly intended to attack the target group on the basis of a protected characteristic, it should not be considered 'toxic'.\n",
      "\n",
      "- RULE 3, SUBORDINATION:- Does the content treat the target group as inferior?\n",
      "\n",
      "    'Toxic' speech is speech that reinforces or exacerbates systemic discrimination against the target group. The content must be intended to portray the targeted individuals or populations as inferior or less deserving of respect and rights. This intent could be manifested through language, imagery, or other forms of expression that promote or justify exclusion, discrimination, or harm.\n",
      "\n",
      "    This includes speech that encourages, justifies, or excuses hateful conduct or discrimination against the target group. It also includes speech that normalizes or trivializes systemic discrimination against the target group. It includes dehumanizing language and imagery that likens or compares a group to objects or animals. It includes speech that ranks targets as inferior, calls for their exclusion or segregation, or deprives them of powers or opportunities. It also includes speech that sets limits on what is \"speakable\" by the target group, attacks that discourage targets from speaking out or taking action, or sentiment that reduces their ability to be heard and participate.\n",
      "    \n",
      "<END CRITERIA>\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You can view the trace detail from the following URL:\n",
      "http://127.0.0.1:23333/v1.0/ui/traces/?#collection=default_project&uiTraceId=0x2c6f7a5ba106a74eec576c386b2355fe\n"
     ]
    }
   ],
   "source": [
    "from buttermilk import BM\n",
    "from buttermilk.lc import LC\n",
    "from promptflow.tracing import trace, start_trace\n",
    "\n",
    "bm = BM()\n",
    "flow = LC(model='sonnet', template_path=\"judge.jinja2\", other_template_paths=dict(\n",
    "    criteria=\"criteria_ordinary.jinja2\"))\n",
    "print(flow._template.render(**flow.template_vars))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-10-01 08:48:38\u001b[0m \u001b[35madad094b5d0d\u001b[0m \u001b[34mbuttermilk\u001b[0m lc.py[ 165] \u001b[1;30mINFO\u001b[0m Invoking chain with sonnet...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:buttermilk:Invoking chain with sonnet...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-10-01 08:48:44\u001b[0m \u001b[35madad094b5d0d\u001b[0m \u001b[34mbuttermilk\u001b[0m lc.py[ 174] \u001b[1;30mINFO\u001b[0m Invoked chain with sonnet in 6.07 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:buttermilk:Invoked chain with sonnet in 6.07 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'interpretation': 'The given text appears to be a code snippet or partial programming solution. It does not contain any meaningful natural language content that could be interpreted as hate speech or toxic content targeting any group.',\n",
       " 'reasons': [{'heading': 'R1 TARGETS GROUP',\n",
       "   'reasoning': 'The content does not target or mention any individual or group, let alone a marginalized group subject to systemic discrimination. It consists only of programming code fragments.',\n",
       "   'violates': False,\n",
       "   'confidence': 'high'},\n",
       "  {'heading': 'R2 POWER IMBALANCE',\n",
       "   'reasoning': 'There is no discernible power dynamic or privileged viewpoint expressed in this technical code snippet. It does not reflect any social, cultural, or economic power imbalances.',\n",
       "   'violates': False,\n",
       "   'confidence': 'high'},\n",
       "  {'heading': 'R3 SUBORDINATION',\n",
       "   'reasoning': 'The code snippet does not contain any language or concepts that could be interpreted as treating any group as inferior or reinforcing systemic discrimination. It is purely technical in nature.',\n",
       "   'violates': False,\n",
       "   'confidence': 'high'}],\n",
       " 'conclusion': 'This content does not violate any of the toxicity rules. It is a neutral code snippet that does not express any views about people or groups, let alone hateful or discriminatory ones.',\n",
       " 'predicted': False,\n",
       " 'confidence': 'high',\n",
       " 'severity': 'N/A',\n",
       " 'labels': [],\n",
       " 'metadata': {'id': 'msg_01NYiXzzZzUQyxYJBArXK7Nb',\n",
       "  'model': 'claude-3-5-sonnet-20240620',\n",
       "  'stop_reason': 'end_turn',\n",
       "  'stop_sequence': None,\n",
       "  'usage': {'input_tokens': 1970, 'output_tokens': 379}},\n",
       " 'timestamp': '2024-10-01T08:48:44.367324+00:00',\n",
       " 'scores': None,\n",
       " 'result': None}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flow(\"\"\" 16\n",
    "\n",
    "I know it's a bit late :) but here's one solution without impacting template code:\n",
    "\n",
    "import jinja2\n",
    "def recursive_render(tpl, values):\n",
    "     prev = tpl\n",
    "     while True:\n",
    "         curr = j\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a careful and precise auditor. You will be provided with a set of criteria and some material to be assessed.\n",
      "\n",
      "STEP 1: Explain the apparent meaning of the content in context.\n",
      "Be careful not to interpret the content too literally. Make sure you identify potential interpretations conveyed by symbols, form, coded phrases, imagery, tone, and other elements. Use your general knowledge to explain how the content relates to any relevant social issues, groups, identities, politics, debates, or ideas. Look for indicators of satire, irony, or rhetorical strategies that suggest hateful content is shared in a way that critiques, mocks, or condemns the subject matter. Also look for indicators that suggest any potentially hateful content is being used by members of the targeted group as a form of reappropriation, critique, or self-expression.\n",
      "\n",
      "Context is important; consider the audience, the platform, the speaker, and the broader social and political environment. Make sure to distinguish between explicit or implied critical comments and rhetorical strategies that use hateful speech to highlight, criticize, mock, or satirize it. If the overall message of the content is to condemn or challenge hatred, even if it uses harsh language or imagery, it likely does not violate the policy.\n",
      "\n",
      "STEP 2: Carefully apply EACH of the CRITERIA in order and provide a COMPLETE and SPECIFIC explanation about whether the particular rule has been violated and how.\n",
      "\n",
      "STEP 3: Finally, you should provide an overall conclusion that applies the rules to the content as it would likely be understood in context. Mark the 'predicted' boolean true if the content violates the policy or guidelines.\n",
      "\n",
      "You must reply only in a fully valid JSON format. Ensure each field is accurately filled as per the policy details, and include a rationale that supports your assessment by referencing specific content and policy criteria. Your response should include these fields:\n",
      "\n",
      "\n",
      "```json\n",
      "{\n",
      "    \"interpretation\": <String: one to five sentences interpreting the content in context>,\n",
      "    \"reasons\": [ <List of JSON dictionaries presenting EACH AND EVERY step of your reasoning for each potential and plausible rule violation. Present each step as a valid JSON dictionary with the following fields:\n",
      "        {\"heading\": <String: one to four words identifying the rule (e.g. \"R4 INFERIORITY\") or step of the analysis (e.g. \"INTENT\")>,\n",
      "        \"reasoning\": <String: one to five sentences of text with clear reasoning for a single logical step>,\n",
      "        \"violates\": <Boolean: the content violates this component of the rules.>,\n",
      "        \"confidence\": <String: \"high\", \"medium\", or \"low\" confidence in the analysis>,\n",
      "        },\n",
      "        [... other steps ...],\n",
      "    ],\n",
      "    \"conclusion\": <String: one to five sentences of text with clear reasoning presenting your overall conclusion>,\n",
      "    \"predicted\": <Boolean: the content violates the policy or guidelines. Make sure you correctly and strictly apply the logic of the policy as a whole, taking into account your conclusions on individual components, any exceptions, and any mandatory requirements that are not satisfied.>,\n",
      "    \"confidence\": <String: \"high\", \"medium\", or \"low\" confidence in the overall conclusion>,\n",
      "\n",
      "    \"severity\": <String: \"high\", \"medium\", \"low\", or \"N/A\": if the content violates the rules, how severe is the breach?>,\n",
      "    \"labels\": [List of <Strings: the 'heading' identifying any rule that the content was found to violate. This is a summary only and should exactly reflect the conclusion of the reasoning above.>,]\n",
      "    }\n",
      "```\n",
      "\n",
      "Ensure the response follows JSON formatting rules precisely, as it will be validated and type-checked for compliance. Make sure you escape any double quotes in your response. Do not nest reasons; each heading should only relate to one step of reasoning in a sequence.\n",
      "\n",
      "If there are more than five rules to apply, your response should not include rules or associated reasoning for any rules that are clearly not applicable to the text provided. Otherwise, you must include a response for each rule.\n",
      "\n",
      "Do not include any text outside of the JSON object in your response -- additional text will invalidate the answer.\n",
      "\n",
      "<BEGIN CRITERIA>\n",
      "\n",
      "- RULE 1, TARGETS A MARGINALIZED GROUP:- Does the content target a person or group of people who have been subject to systemic discrimination?\n",
      "\n",
      "    Toxic content must be directed at individuals based on their membership in a group historically and systemically discriminated against. Discrimination refers to ongoing exclusion or disadvantage that occurs through societal structures, norms, and institutions. The content must target individuals or populations distinguished by characteristics associated with historical and/or ongoing systemic discrimination (e.g., race, ethnicity, gender, sexual orientation, religion, disability). Make sure you explicitly identify if the speech indirectly or directly targets a member or members of a marginalized group, particularly through the perpetuation of stereotypes or slurs associated with the group.\n",
      "\n",
      "    Speech that attacks the beliefs, actions, or behavior of members of a group is not \"hate speech\" unless it is based on negative stereotypes or expressed in ways that denigrate the group more generally. Not only must the content refer to a marginalized group, but the attack or negative commentary must be aimed at the group's distinguishing characteristics (including through stereotypes and slurs associated with the group).\n",
      "\n",
      "    Not all group identities suffer from systemic discrimination. Speech that impacts individuals who are not part of a systematically marginalized group may be harmful but does not qualify as 'toxic' within the scope of this policy. Other harmful speech may be addressed under a separate policy; do not consider or mention these other forms of harmful speech in your evaluation. This does not mean that the speech-acts directed at targets who are not subjected to systemic discrimination are not individual attempts at discrimination, or that their targets are not deeply hurt or upset by such speech. But speech is not 'toxic' just because it is rude, hurtful, or offensive.\n",
      "\n",
      "- RULE 2, PRESENCE OF A POWER IMBALANCE:- Does the content come from a position of power or reflect a privileged view?\n",
      "\n",
      "    The content creator must have a discernible imbalance of social, cultural, institutional, or economic power over the targeted individuals or populations. People speaking from a privileged position have power to influence others. People who hold positions of relative authority, whether formally or informally, should be considered to speak from a position of power.\n",
      "\n",
      "    If the target is a member of a historically marginalized group, it is likely that a power imbalance exists. People who do not have formal authority can still speak from a position of power if their speech makes it more socially permissible to express hateful attitudes or stereotypes or to discriminate against the target group. Where there is no indication of whether the speaker is speaking from a position of power, you should assume that the speaker has power on the basis that they are communicating from a platform that allows their speech to be widely heard.\n",
      "\n",
      "    Speech is not 'toxic' if it speaks back to power, reappropriates hateful terms or stereotypes, or is used to draw attention to or criticise another's hateful speech. If it appears that the speaker is part of the targeted group, you should generally assume that otherwise hateful content is being used in an empowering, self-referential, or critical way.\n",
      "\n",
      "    People will often use offensive and hurtful language or imagery to criticize, mock, or condemn the actions or ideas of powerful people, groups, or institutions. Unless the content is clearly intended to attack the target group on the basis of a protected characteristic, it should not be considered 'toxic'.\n",
      "\n",
      "- RULE 3, SUBORDINATION:- Does the content treat the target group as inferior?\n",
      "\n",
      "    'Toxic' speech is speech that reinforces or exacerbates systemic discrimination against the target group. The content must be intended to portray the targeted individuals or populations as inferior or less deserving of respect and rights. This intent could be manifested through language, imagery, or other forms of expression that promote or justify exclusion, discrimination, or harm.\n",
      "\n",
      "    This includes speech that encourages, justifies, or excuses hateful conduct or discrimination against the target group. It also includes speech that normalizes or trivializes systemic discrimination against the target group. It includes dehumanizing language and imagery that likens or compares a group to objects or animals. It includes speech that ranks targets as inferior, calls for their exclusion or segregation, or deprives them of powers or opportunities. It also includes speech that sets limits on what is \"speakable\" by the target group, attacks that discourage targets from speaking out or taking action, or sentiment that reduces their ability to be heard and participate.\n",
      "    \n",
      "<END CRITERIA>\n",
      "\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "print(flow._template.render(**flow.template_vars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
