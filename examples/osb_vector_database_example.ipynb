{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OSB Vector Database Example\n",
    "\n",
    "This notebook demonstrates how to create and use a vector database from Oversight Board full text data using Buttermilk's ChromaDB integration.\n",
    "\n",
    "## Overview\n",
    "\n",
    "We'll show how to:\n",
    "1. Load OSB JSON data using existing data loaders\n",
    "2. Generate embeddings and create a ChromaDB vector store\n",
    "3. Use the generic RAG agent for interactive question answering\n",
    "4. Demonstrate semantic search capabilities\n",
    "\n",
    "This example uses the generic infrastructure that works with any JSON dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration Setup\n",
    "\n",
    "First, let's set up the configuration for our OSB vector database pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-06-16 20:10:38\u001b[0m [] \u001b[1;30mINFO\u001b[0m bm_init.py:778 Logging set up for run: platform='local' name='bm_api' job='osb_vectorise' run_id='20250616T1010Z-UP4j-docker-desktop-debian' ip=None node_name='docker-desktop' save_dir='/tmp/tmpclcw4qip/bm_api/osb_vectorise/20250616T1010Z-UP4j-docker-desktop-debian' flow_api=None. Save directory: /tmp/tmpclcw4qip/bm_api/osb_vectorise/20250616T1010Z-UP4j-docker-desktop-debian\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Initialized Buttermilk <span style=\"font-weight: bold\">(</span>bm<span style=\"font-weight: bold\">)</span> with configuration:\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Initialized Buttermilk \u001b[1m(\u001b[0mbm\u001b[1m)\u001b[0m with configuration:\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'platform'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'local'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'name'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'bm_api'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'job'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'osb_vectorise'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'run_id'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'20250616T1010Z-UP4j-docker-desktop-debian'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'node_name'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'docker-desktop'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'save_dir'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'/tmp/tmpclcw4qip/bm_api/osb_vectorise/20250616T1010Z-UP4j-docker-desktop-debian'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'connections'</span>: <span style=\"font-weight: bold\">[]</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'secret_provider'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'type'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'gcp'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'project'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'prosocial-443205'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'models_secret'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'dev__llm__connections'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'credentials_secret'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'dev__shared_credentials'</span>\n",
       "    <span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'logger_cfg'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'type'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'gcp'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'project'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'prosocial-443205'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'location'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'us-central1'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'verbose'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span><span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'pubsub'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'type'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'gcp'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'project'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'prosocial-443205'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'jobs_subscription'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'jobs-sub'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'status_subscription'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'flow-sub'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'status_topic'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'flow'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'jobs_topic'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'jobs'</span>\n",
       "    <span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'clouds'</span>: <span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'type'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'gcp'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'project'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'prosocial-443205'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'quota_project_id'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'prosocial-443205'</span><span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'type'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'vertex'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'project'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'prosocial-443205'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'region'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'us-central1'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'location'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'us-central1'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'bucket'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'prosocial-de'</span>\n",
       "        <span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">]</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'tracing'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'enabled'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'api_key'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">''</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'provider'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'weave'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'otlp_headers'</span>: <span style=\"font-weight: bold\">{}}</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'datasets'</span>: <span style=\"font-weight: bold\">{}</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'save_dir_base'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'/tmp/tmpclcw4qip'</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m{\u001b[0m\n",
       "    \u001b[32m'platform'\u001b[0m: \u001b[32m'local'\u001b[0m,\n",
       "    \u001b[32m'name'\u001b[0m: \u001b[32m'bm_api'\u001b[0m,\n",
       "    \u001b[32m'job'\u001b[0m: \u001b[32m'osb_vectorise'\u001b[0m,\n",
       "    \u001b[32m'run_id'\u001b[0m: \u001b[32m'20250616T1010Z-UP4j-docker-desktop-debian'\u001b[0m,\n",
       "    \u001b[32m'node_name'\u001b[0m: \u001b[32m'docker-desktop'\u001b[0m,\n",
       "    \u001b[32m'save_dir'\u001b[0m: \u001b[32m'/tmp/tmpclcw4qip/bm_api/osb_vectorise/20250616T1010Z-UP4j-docker-desktop-debian'\u001b[0m,\n",
       "    \u001b[32m'connections'\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m,\n",
       "    \u001b[32m'secret_provider'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "        \u001b[32m'type'\u001b[0m: \u001b[32m'gcp'\u001b[0m,\n",
       "        \u001b[32m'project'\u001b[0m: \u001b[32m'prosocial-443205'\u001b[0m,\n",
       "        \u001b[32m'models_secret'\u001b[0m: \u001b[32m'dev__llm__connections'\u001b[0m,\n",
       "        \u001b[32m'credentials_secret'\u001b[0m: \u001b[32m'dev__shared_credentials'\u001b[0m\n",
       "    \u001b[1m}\u001b[0m,\n",
       "    \u001b[32m'logger_cfg'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'type'\u001b[0m: \u001b[32m'gcp'\u001b[0m, \u001b[32m'project'\u001b[0m: \u001b[32m'prosocial-443205'\u001b[0m, \u001b[32m'location'\u001b[0m: \u001b[32m'us-central1'\u001b[0m, \u001b[32m'verbose'\u001b[0m: \u001b[3;91mFalse\u001b[0m\u001b[1m}\u001b[0m,\n",
       "    \u001b[32m'pubsub'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "        \u001b[32m'type'\u001b[0m: \u001b[32m'gcp'\u001b[0m,\n",
       "        \u001b[32m'project'\u001b[0m: \u001b[32m'prosocial-443205'\u001b[0m,\n",
       "        \u001b[32m'jobs_subscription'\u001b[0m: \u001b[32m'jobs-sub'\u001b[0m,\n",
       "        \u001b[32m'status_subscription'\u001b[0m: \u001b[32m'flow-sub'\u001b[0m,\n",
       "        \u001b[32m'status_topic'\u001b[0m: \u001b[32m'flow'\u001b[0m,\n",
       "        \u001b[32m'jobs_topic'\u001b[0m: \u001b[32m'jobs'\u001b[0m\n",
       "    \u001b[1m}\u001b[0m,\n",
       "    \u001b[32m'clouds'\u001b[0m: \u001b[1m[\u001b[0m\n",
       "        \u001b[1m{\u001b[0m\u001b[32m'type'\u001b[0m: \u001b[32m'gcp'\u001b[0m, \u001b[32m'project'\u001b[0m: \u001b[32m'prosocial-443205'\u001b[0m, \u001b[32m'quota_project_id'\u001b[0m: \u001b[32m'prosocial-443205'\u001b[0m\u001b[1m}\u001b[0m,\n",
       "        \u001b[1m{\u001b[0m\n",
       "            \u001b[32m'type'\u001b[0m: \u001b[32m'vertex'\u001b[0m,\n",
       "            \u001b[32m'project'\u001b[0m: \u001b[32m'prosocial-443205'\u001b[0m,\n",
       "            \u001b[32m'region'\u001b[0m: \u001b[32m'us-central1'\u001b[0m,\n",
       "            \u001b[32m'location'\u001b[0m: \u001b[32m'us-central1'\u001b[0m,\n",
       "            \u001b[32m'bucket'\u001b[0m: \u001b[32m'prosocial-de'\u001b[0m\n",
       "        \u001b[1m}\u001b[0m\n",
       "    \u001b[1m]\u001b[0m,\n",
       "    \u001b[32m'tracing'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'enabled'\u001b[0m: \u001b[3;92mTrue\u001b[0m, \u001b[32m'api_key'\u001b[0m: \u001b[32m''\u001b[0m, \u001b[32m'provider'\u001b[0m: \u001b[32m'weave'\u001b[0m, \u001b[32m'otlp_headers'\u001b[0m: \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\u001b[1m}\u001b[0m,\n",
       "    \u001b[32m'datasets'\u001b[0m: \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "    \u001b[32m'save_dir_base'\u001b[0m: \u001b[32m'/tmp/tmpclcw4qip'\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-06-16 20:10:38\u001b[0m [] \u001b[1;30mINFO\u001b[0m nb.py:59 Starting interactive run for bm_api job osb_vectorise in notebook\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">üöÄ Buttermilk initialized for JSON-to-Vector tutorial\n",
       "</pre>\n"
      ],
      "text/plain": [
       "üöÄ Buttermilk initialized for JSON-to-Vector tutorial\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'osb_json'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'type'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'gcs'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'path'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'gs://prosocial-public/osb/03_osb_fulltext_summaries.json'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'columns'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'record_id'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'id'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'content'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'full_text'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'metadata'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'title'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'title'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'case_number'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'case_number'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'url'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'url'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'summary'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'summary'</span><span style=\"font-weight: bold\">}}}</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'osb_vector'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'type'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'chromadb'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'persist_directory'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'gs://prosocial-public/osb/chromadb'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'collection_name'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'osb_fulltext'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'embedding_model'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'gemini-embedding-001'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'dimensionality'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3072</span><span style=\"font-weight: bold\">}}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m{\u001b[0m\u001b[32m'osb_json'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'type'\u001b[0m: \u001b[32m'gcs'\u001b[0m, \u001b[32m'path'\u001b[0m: \u001b[32m'gs://prosocial-public/osb/03_osb_fulltext_summaries.json'\u001b[0m, \u001b[32m'columns'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'record_id'\u001b[0m: \u001b[32m'id'\u001b[0m, \u001b[32m'content'\u001b[0m: \u001b[32m'full_text'\u001b[0m, \u001b[32m'metadata'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'title'\u001b[0m: \u001b[32m'title'\u001b[0m, \u001b[32m'case_number'\u001b[0m: \u001b[32m'case_number'\u001b[0m, \u001b[32m'url'\u001b[0m: \u001b[32m'url'\u001b[0m, \u001b[32m'summary'\u001b[0m: \u001b[32m'summary'\u001b[0m\u001b[1m}\u001b[0m\u001b[1m}\u001b[0m\u001b[1m}\u001b[0m, \u001b[32m'osb_vector'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'type'\u001b[0m: \u001b[32m'chromadb'\u001b[0m, \u001b[32m'persist_directory'\u001b[0m: \u001b[32m'gs://prosocial-public/osb/chromadb'\u001b[0m, \u001b[32m'collection_name'\u001b[0m: \u001b[32m'osb_fulltext'\u001b[0m, \u001b[32m'embedding_model'\u001b[0m: \u001b[32m'gemini-embedding-001'\u001b[0m, \u001b[32m'dimensionality'\u001b[0m: \u001b[1;36m3072\u001b[0m\u001b[1m}\u001b[0m\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-06-16 20:10:38\u001b[0m [] \u001b[1;30mINFO\u001b[0m save.py:641 Successfully dumped data to local disk (JSON): /tmp/tmpclcw4qip/bm_api/osb_vectorise/20250616T1010Z-UP4j-docker-desktop-debian/tmpcw2gxev0.json.\n",
      "\u001b[32m2025-06-16 20:10:38\u001b[0m [] \u001b[1;30mINFO\u001b[0m save.py:215 Successfully saved data using dump_to_disk to: /tmp/tmpclcw4qip/bm_api/osb_vectorise/20250616T1010Z-UP4j-docker-desktop-debian/tmpcw2gxev0.json.\n",
      "\u001b[32m2025-06-16 20:10:38\u001b[0m [] \u001b[1;30mINFO\u001b[0m bm_init.py:864 {'message': 'Successfully saved data to: /tmp/tmpclcw4qip/bm_api/osb_vectorise/20250616T1010Z-UP4j-docker-desktop-debian/tmpcw2gxev0.json', 'uri': '/tmp/tmpclcw4qip/bm_api/osb_vectorise/20250616T1010Z-UP4j-docker-desktop-debian/tmpcw2gxev0.json', 'run_id': '20250616T1010Z-UP4j-docker-desktop-debian'}\n"
     ]
    }
   ],
   "source": [
    "from rich import print\n",
    "from rich.pretty import pprint\n",
    "import asyncio\n",
    "import json\n",
    "from pathlib import Path\n",
    "import hydra\n",
    "from hydra import compose, initialize_config_dir\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "\n",
    "# Buttermilk imports\n",
    "from buttermilk import logger\n",
    "from buttermilk.data.vector import ChromaDBEmbeddings, InputDocument, DefaultTextSplitter\n",
    "from buttermilk.agents.rag.rag_agent import RagAgent\n",
    "from buttermilk._core.config import AgentConfig, DataSourceConfig\n",
    "\n",
    "from buttermilk.utils.nb import init\n",
    "from buttermilk._core.dmrc import get_bm, set_bm\n",
    "from buttermilk._core.config import DataSourceConfig\n",
    "from buttermilk._core.types import Record\n",
    "from buttermilk.data.loaders import create_data_loader\n",
    "from buttermilk.data.vector import ChromaDBEmbeddings, InputDocument, DefaultTextSplitter, list_to_async_iterator\n",
    "\n",
    "# Initialize Buttermilk\n",
    "cfg = init(job=\"osb_vectorise\", overrides=[\"+storage=osb\"])\n",
    "bm = get_bm()\n",
    "\n",
    "print(\"üöÄ Buttermilk initialized for JSON-to-Vector tutorial\")\n",
    "pprint(cfg.storage)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initialize Components\n",
    "\n",
    "Let's create the storage, vector store, and text splitter components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-06-16 20:10:48\u001b[0m [] \u001b[1;30mINFO\u001b[0m vector.py:249 Loading embedding model: gemini-embedding-001\n",
      "\u001b[32m2025-06-16 20:10:52\u001b[0m [] \u001b[1;30mINFO\u001b[0m vector.py:257 Initializing ChromaDB client at: gs://prosocial-public/osb/chromadb\n",
      "\u001b[32m2025-06-16 20:10:52\u001b[0m [] \u001b[1;30mINFO\u001b[0m vector.py:262 Using ChromaDB collection: osb_fulltext\n",
      "\u001b[32m2025-06-16 20:10:52\u001b[0m [] \u001b[1;30mINFO\u001b[0m vector.py:154 Initialized RecursiveCharacterTextSplitter (chunk_size=2000, chunk_overlap=500)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">‚úÖ All storage components initialized via BM API\n",
       "</pre>\n"
      ],
      "text/plain": [
       "‚úÖ All storage components initialized via BM API\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Source: FileStorage\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Source: FileStorage\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Vector store: ChromaDBEmbeddings\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Vector store: ChromaDBEmbeddings\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Text splitter: DefaultTextSplitter\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Text splitter: DefaultTextSplitter\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Now we can use the clean BM API for all storage types\n",
    "source = bm.get_storage(cfg.storage.osb_json)\n",
    "vectorstore = bm.get_storage(cfg.storage.osb_vector)\n",
    "\n",
    "# Create text splitter\n",
    "chunker = DefaultTextSplitter(chunk_size=2000, chunk_overlap=500)\n",
    "\n",
    "print(\"‚úÖ All storage components initialized via BM API\")\n",
    "print(f\"Source: {type(source).__name__}\")\n",
    "print(f\"Vector store: {type(vectorstore).__name__}\")\n",
    "print(f\"Text splitter: {type(chunker).__name__}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-06-16 20:11:01\u001b[0m [] \u001b[1;30mINFO\u001b[0m vector.py:281 üîÑ Downloading remote ChromaDB: gs://prosocial-public/osb/chromadb\n",
      "\u001b[32m2025-06-16 20:11:01\u001b[0m [] \u001b[1;30mINFO\u001b[0m utils.py:645 Downloading ChromaDB from gs://prosocial-public/osb/chromadb to /home/debian/.cache/buttermilk/chromadb/gs___prosocial-public_osb_chromadb\n",
      "\u001b[32m2025-06-16 20:11:03\u001b[0m [] \u001b[1;30mERROR\u001b[0m utils.py:676 \u001b[31mFailed to download ChromaDB from gs://prosocial-public/osb/chromadb: Remote ChromaDB directory does not exist: gs://prosocial-public/osb/chromadb\u001b[0m\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "ChromaDB download failed: Remote ChromaDB directory does not exist: gs://prosocial-public/osb/chromadb",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/buttermilk/buttermilk/utils/utils.py:657\u001b[39m, in \u001b[36mensure_chromadb_cache.<locals>._download_chromadb_sync\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    656\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m remote_path.exists():\n\u001b[32m--> \u001b[39m\u001b[32m657\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRemote ChromaDB directory does not exist: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpersist_directory\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    659\u001b[39m \u001b[38;5;66;03m# Download all files recursively\u001b[39;00m\n",
      "\u001b[31mOSError\u001b[39m: Remote ChromaDB directory does not exist: gs://prosocial-public/osb/chromadb",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Ensure ChromaDB is ready - this handles both creation and reading scenarios\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m vectorstore.ensure_cache_initialized()\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m‚úÖ ChromaDB collection ready for use\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33müìä Collection \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvectorstore.collection_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m statistics:\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/buttermilk/buttermilk/data/vector.py:282\u001b[39m, in \u001b[36mChromaDBEmbeddings.ensure_cache_initialized\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    280\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.persist_directory.startswith((\u001b[33m\"\u001b[39m\u001b[33mgs://\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33ms3://\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mazure://\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mgcs://\u001b[39m\u001b[33m\"\u001b[39m)):\n\u001b[32m    281\u001b[39m     logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33müîÑ Downloading remote ChromaDB: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.persist_directory\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m282\u001b[39m     local_cache_path = \u001b[38;5;28;01mawait\u001b[39;00m ensure_chromadb_cache(\u001b[38;5;28mself\u001b[39m.persist_directory)\n\u001b[32m    284\u001b[39m     \u001b[38;5;66;03m# Update persist_directory to use local cache\u001b[39;00m\n\u001b[32m    285\u001b[39m     \u001b[38;5;28mself\u001b[39m.persist_directory = \u001b[38;5;28mstr\u001b[39m(local_cache_path)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/buttermilk/buttermilk/utils/utils.py:683\u001b[39m, in \u001b[36mensure_chromadb_cache\u001b[39m\u001b[34m(persist_directory)\u001b[39m\n\u001b[32m    680\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mChromaDB download failed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    682\u001b[39m \u001b[38;5;66;03m# Run download in thread to avoid blocking async operations\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m683\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio.to_thread(_download_chromadb_sync)\n\u001b[32m    685\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m local_cache_path\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/asyncio/threads.py:25\u001b[39m, in \u001b[36mto_thread\u001b[39m\u001b[34m(func, *args, **kwargs)\u001b[39m\n\u001b[32m     23\u001b[39m ctx = contextvars.copy_context()\n\u001b[32m     24\u001b[39m func_call = functools.partial(ctx.run, func, *args, **kwargs)\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m loop.run_in_executor(\u001b[38;5;28;01mNone\u001b[39;00m, func_call)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/asyncio/futures.py:287\u001b[39m, in \u001b[36mFuture.__await__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.done():\n\u001b[32m    286\u001b[39m     \u001b[38;5;28mself\u001b[39m._asyncio_future_blocking = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m287\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mself\u001b[39m  \u001b[38;5;66;03m# This tells Task to wait for completion.\u001b[39;00m\n\u001b[32m    288\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.done():\n\u001b[32m    289\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mawait wasn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt used with future\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/asyncio/tasks.py:349\u001b[39m, in \u001b[36mTask.__wakeup\u001b[39m\u001b[34m(self, future)\u001b[39m\n\u001b[32m    347\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__wakeup\u001b[39m(\u001b[38;5;28mself\u001b[39m, future):\n\u001b[32m    348\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m349\u001b[39m         \u001b[43mfuture\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    350\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    351\u001b[39m         \u001b[38;5;66;03m# This may also be a cancellation.\u001b[39;00m\n\u001b[32m    352\u001b[39m         \u001b[38;5;28mself\u001b[39m.__step(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/asyncio/futures.py:203\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    201\u001b[39m \u001b[38;5;28mself\u001b[39m.__log_traceback = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    202\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m203\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception.with_traceback(\u001b[38;5;28mself\u001b[39m._exception_tb)\n\u001b[32m    204\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/concurrent/futures/thread.py:58\u001b[39m, in \u001b[36m_WorkItem.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     55\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     60\u001b[39m     \u001b[38;5;28mself\u001b[39m.future.set_exception(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/buttermilk/buttermilk/utils/utils.py:680\u001b[39m, in \u001b[36mensure_chromadb_cache.<locals>._download_chromadb_sync\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    678\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m local_cache_path.exists():\n\u001b[32m    679\u001b[39m     shutil.rmtree(local_cache_path, ignore_errors=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m680\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mChromaDB download failed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
      "\u001b[31mOSError\u001b[39m: ChromaDB download failed: Remote ChromaDB directory does not exist: gs://prosocial-public/osb/chromadb"
     ]
    }
   ],
   "source": [
    "# Ensure ChromaDB is ready - this handles both creation and reading scenarios\n",
    "await vectorstore.ensure_cache_initialized()\n",
    "\n",
    "print(\"‚úÖ ChromaDB collection ready for use\")\n",
    "print(f\"üìä Collection '{vectorstore.collection_name}' statistics:\")\n",
    "print(f\"   üìÅ Directory: {vectorstore.persist_directory}\")\n",
    "print(f\"   üß† Model: {vectorstore.embedding_model}\")\n",
    "print(f\"   üìê Dimensions: {vectorstore.dimensionality}\")\n",
    "print(f\"   üì¶ Current count: {vectorstore.collection.count()} embeddings\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Safe Create vs Read Behavior\n",
    "\n",
    "The `ensure_cache_initialized()` method intelligently handles both scenarios:\n",
    "\n",
    "### üÜï **First Run (Creation)**\n",
    "- Downloads remote ChromaDB if needed\n",
    "- Creates new collection with proper schema\n",
    "- Logs: \"üÜï Creating new collection 'osb_fulltext'\"\n",
    "\n",
    "### üìñ **Subsequent Runs (Reading)** \n",
    "- Uses existing cached ChromaDB\n",
    "- Validates collection compatibility\n",
    "- Logs: \"üìñ Found existing collection 'osb_fulltext'\" \n",
    "\n",
    "### üîí **Safety Features**\n",
    "- ‚úÖ Never overwrites existing collections\n",
    "- ‚úÖ Same config works for create and read\n",
    "- ‚úÖ Schema validation with helpful warnings\n",
    "- ‚úÖ Clear logging of all operations\n",
    "\n",
    "This means you can:\n",
    "1. **Run this notebook** to create embeddings  \n",
    "2. **Use same config** in production to read embeddings\n",
    "3. **No config changes** needed between scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load live OSB data from GCS\n",
    "print(\"üì• Loading live OSB data from GCS...\")\n",
    "\n",
    "# Create data loader from storage config\n",
    "from buttermilk.data.loaders import create_data_loader\n",
    "from buttermilk._core.config import DataSourceConfig\n",
    "\n",
    "# Convert storage config to data source config\n",
    "source_config = DataSourceConfig(**cfg.storage.osb_json)\n",
    "data_loader = create_data_loader(source_config)\n",
    "\n",
    "print(f\"üîó Data source: {source_config.path}\")\n",
    "print(f\"üìã Field mapping: {source_config.columns}\")\n",
    "\n",
    "# Load documents (limit for demo, remove limit for full production run)\n",
    "documents = []\n",
    "doc_limit = 5  # Set to None for full dataset\n",
    "\n",
    "print(f\"üìö Loading {doc_limit or 'all'} documents from live dataset...\")\n",
    "\n",
    "async for record in data_loader.load():\n",
    "    # Convert Record to InputDocument format for vector processing\n",
    "    doc = InputDocument(\n",
    "        record_id=record.record_id,\n",
    "        title=record.metadata.get('title', 'Untitled OSB Case'),\n",
    "        full_text=record.content if isinstance(record.content, str) else str(record.content),\n",
    "        metadata=record.metadata,\n",
    "        file_path=\"\",\n",
    "    )\n",
    "    documents.append(doc)\n",
    "    \n",
    "    print(f\"   üìÑ Loaded: {doc.record_id} - {doc.title[:60]}...\")\n",
    "    \n",
    "    if doc_limit and len(documents) >= doc_limit:\n",
    "        break\n",
    "\n",
    "print(f\"\\n‚úÖ Loaded {len(documents)} live OSB documents for vector processing\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Configuration-Driven Multi-Field Vector Store\n\nThis notebook demonstrates a **configuration-driven approach** for multi-field vector embeddings that works across any data source.\n\n### üß† **The Problem**\nTraditional vector stores only embed the main content, leaving rich metadata unsearchable:\n```python\n# Traditional approach - metadata trapped\ndoc.content = \"Long text...\"        # ‚Üí Gets embedded ‚úÖ\ndoc.metadata.summary = \"Key points\"  # ‚Üí Not searchable ‚ùå\n```\n\n### üéØ **Our Solution: Configuration-Driven Multi-Field Embeddings**\nDefine which fields to embed in the storage configuration:\n```yaml\n# conf/storage/osb.yaml\nosb_vector:\n  type: chromadb\n  # ... basic config\n  multi_field_embedding:\n    content_field: \"content\"\n    additional_fields:\n      - source_field: \"summary\"\n        chunk_type: \"summary\"\n        min_length: 50\n      - source_field: \"title\"\n        chunk_type: \"title\"\n        min_length: 10\n```\n\n### üîç **Search Capabilities**\n\n| Search Type | Use Case | Example Query |\n|-------------|----------|---------------|\n| **Summary-Only** | High-level concepts | `where={\"content_type\": \"summary\"}` |\n| **Title-Only** | Topic matching | `where={\"content_type\": \"title\"}` |\n| **Content-Only** | Detailed analysis | `where={\"content_type\": \"content\"}` |\n| **Cross-Field** | Comprehensive search | No filter = search everything |\n| **Hybrid** | Semantic + exact match | `query + where={\"case_number\": \"2024\"}` |\n\n### üèóÔ∏è **Benefits**\n- ‚úÖ **Configuration-Driven**: No hardcoded field names\n- ‚úÖ **Data Source Agnostic**: Works with any Record structure\n- ‚úÖ **Same Config**: Creation and reading use identical configuration\n- ‚úÖ **Extensible**: Easy to add new field types for any dataset",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "async def create_production_vector_store():\n    \"\"\"Production pipeline: Process live OSB data with configuration-driven multi-field embedding.\"\"\"\n    \n    print(\"üè≠ Starting production vector store (configuration-driven)...\")\n    print(f\"üìä Processing {len(documents)} live OSB documents\")\n    \n    successful_embeddings = 0\n    failed_embeddings = 0\n    total_chunks = 0\n    \n    for i, doc in enumerate(documents):\n        print(f\"\\nüìÑ [{i+1}/{len(documents)}] Processing: {doc.title[:50]}...\")\n        \n        try:\n            # Simple process call - multi-field chunking happens automatically via config!\n            processed_doc = await vectorstore.process(doc)\n            if processed_doc:\n                successful_embeddings += 1\n                chunk_count = len(processed_doc.chunks)\n                total_chunks += chunk_count\n                \n                # Count chunk types for display\n                chunk_types = {}\n                for chunk in processed_doc.chunks:\n                    content_type = chunk.metadata.get('content_type', 'content')\n                    chunk_types[content_type] = chunk_types.get(content_type, 0) + 1\n                \n                breakdown = \", \".join([f\"{count} {ctype}\" for ctype, count in chunk_types.items()])\n                print(f\"   ‚úÖ Embedded {chunk_count} chunks: {breakdown}\")\n            else:\n                failed_embeddings += 1\n                print(f\"   ‚ùå Failed to process document\")\n                \n        except Exception as e:\n            failed_embeddings += 1\n            print(f\"   ‚ùå Error processing document: {e}\")\n    \n    # Final results\n    final_count = vectorstore.collection.count()\n    \n    print(f\"\\nüéâ Configuration-Driven Vector Store Created!\")\n    print(f\"   üìä Documents processed: {successful_embeddings + failed_embeddings}\")\n    print(f\"   ‚úÖ Successfully embedded: {successful_embeddings}\")\n    print(f\"   ‚ùå Failed: {failed_embeddings}\")\n    print(f\"   üì¶ Total chunks: {total_chunks}\")\n    print(f\"   üî¢ Total embeddings in collection: {final_count}\")\n    print(f\"   üè™ Collection: '{vectorstore.collection_name}'\")\n    print(f\"   ‚öôÔ∏è  Multi-field config: {vectorstore.multi_field_config is not None}\")\n    \n    return {\n        'successful_docs': successful_embeddings,\n        'failed_docs': failed_embeddings, \n        'total_chunks': total_chunks,\n        'final_embedding_count': final_count\n    }\n\n# Create the production vector store using configuration\nresults = await create_production_vector_store()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Test configuration-driven multi-field search capabilities\nprint(\"üîç Testing Configuration-Driven Multi-Field Search...\")\n\n# The content_type values come from our configuration:\n# - \"content\" (main content field)\n# - \"summary\" (from additional_fields config)\n# - \"title\" (from additional_fields config)\n\n# 1. Search summaries only (high-level concepts)\nprint(\"\\nüéØ 1. SUMMARY-ONLY SEARCH:\")\nprint(\"   Query: 'platform safety measures'\")\nsummary_results = vectorstore.collection.query(\n    query_texts=[\"platform safety measures\"],\n    where={\"content_type\": \"summary\"},  # Based on config: source_field=\"summary\"\n    n_results=3,\n    include=[\"documents\", \"metadatas\", \"distances\"]\n)\n\nif summary_results[\"ids\"] and summary_results[\"ids\"][0]:\n    for i, (doc, metadata, distance) in enumerate(zip(\n        summary_results[\"documents\"][0], \n        summary_results[\"metadatas\"][0], \n        summary_results[\"distances\"][0]\n    )):\n        similarity = 1 - distance\n        title = metadata.get(\"title\", \"Untitled\")\n        print(f\"   üìã Result {i+1}: {title[:40]}... (similarity: {similarity:.3f})\")\n        print(f\"      üìù Summary: {doc[:80]}...\")\n\n# 2. Search titles only (specific topics)\nprint(\"\\nüéØ 2. TITLE-ONLY SEARCH:\")\nprint(\"   Query: 'content moderation'\")\ntitle_results = vectorstore.collection.query(\n    query_texts=[\"content moderation\"],\n    where={\"content_type\": \"title\"},  # Based on config: source_field=\"title\"\n    n_results=3,\n    include=[\"documents\", \"metadatas\", \"distances\"]\n)\n\nif title_results[\"ids\"] and title_results[\"ids\"][0]:\n    for i, (doc, metadata, distance) in enumerate(zip(\n        title_results[\"documents\"][0], \n        title_results[\"metadatas\"][0], \n        title_results[\"distances\"][0]\n    )):\n        similarity = 1 - distance\n        case_number = metadata.get(\"case_number\", \"Unknown\")\n        print(f\"   üìã Result {i+1}: {case_number} (similarity: {similarity:.3f})\")\n        print(f\"      üè∑Ô∏è  Title: {doc}\")\n\n# 3. Search main content only (detailed analysis)\nprint(\"\\nüéØ 3. CONTENT-ONLY SEARCH:\")\nprint(\"   Query: 'automated detection algorithms'\")\ncontent_results = vectorstore.collection.query(\n    query_texts=[\"automated detection algorithms\"],\n    where={\"content_type\": \"content\"},  # Based on config: content_field=\"content\"\n    n_results=2,\n    include=[\"documents\", \"metadatas\", \"distances\"]\n)\n\nif content_results[\"ids\"] and content_results[\"ids\"][0]:\n    for i, (doc, metadata, distance) in enumerate(zip(\n        content_results[\"documents\"][0], \n        content_results[\"metadatas\"][0], \n        content_results[\"distances\"][0]\n    )):\n        similarity = 1 - distance\n        title = metadata.get(\"title\", \"Untitled\")\n        chunk_seq = metadata.get(\"chunk_sequence\", \"?\")\n        print(f\"   üìã Result {i+1}: {title[:30]}... chunk {chunk_seq} (similarity: {similarity:.3f})\")\n        print(f\"      üìù Content: {doc[:100]}...\")\n\n# 4. Cross-field search (search all content types)\nprint(\"\\nüéØ 4. CROSS-FIELD SEARCH:\")\nprint(\"   Query: 'user protection mechanisms'\")\nall_results = vectorstore.collection.query(\n    query_texts=[\"user protection mechanisms\"],\n    n_results=5,  # No content_type filter = search everything\n    include=[\"documents\", \"metadatas\", \"distances\"]\n)\n\nif all_results[\"ids\"] and all_results[\"ids\"][0]:\n    for i, (doc, metadata, distance) in enumerate(zip(\n        all_results[\"documents\"][0], \n        all_results[\"metadatas\"][0], \n        all_results[\"distances\"][0]\n    )):\n        similarity = 1 - distance\n        content_type = metadata.get(\"content_type\", \"unknown\")\n        title = metadata.get(\"title\", \"Untitled\")\n        print(f\"   üìã Result {i+1}: {content_type.upper()} from '{title[:30]}...' (similarity: {similarity:.3f})\")\n        print(f\"      üìù Text: {doc[:80]}...\")\n\nprint(f\"\\n‚úÖ Configuration-driven multi-field search demonstrated!\")\nprint(f\"üéØ This approach works with ANY data source - just update the config!\")\nprint(f\"üìù For other datasets, modify conf/storage/[dataset].yaml:\")\nprint(f\"   additional_fields:\")\nprint(f\"     - source_field: 'abstract'  # For academic papers\")\nprint(f\"     - source_field: 'keywords'  # For any dataset with keywords\")\nprint(f\"     - source_field: 'category'  # For categorized content\")",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data source configuration for the RAG agent\n",
    "data_config = DataSourceConfig(\n",
    "    type=\"chromadb\", persist_directory=\"./data/osb_chromadb\", collection_name=\"osb_fulltext\", embedding_model=\"text-embedding-005\", dimensionality=768\n",
    ")\n",
    "\n",
    "# Create agent configuration\n",
    "agent_config = AgentConfig(\n",
    "    role=\"RESEARCHER\",\n",
    "    agent_obj=\"RagAgent\",\n",
    "    description=\"OSB Research Assistant\",\n",
    "    data={\"osb_vector\": data_config},\n",
    "    variants={\"model\": \"gemini-1.5-flash\"},\n",
    "    parameters={\"template\": \"rag_research\", \"n_results\": 10, \"no_duplicates\": False, \"max_queries\": 3},\n",
    ")\n",
    "\n",
    "# Initialize the RAG agent\n",
    "rag_agent = RagAgent(**agent_config.model_dump())\n",
    "print(\"RAG agent initialized successfully\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Semantic Search Examples\n",
    "\n",
    "Let's demonstrate the semantic search capabilities of our OSB vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def search_osb_database(queries):\n",
    "    \"\"\"Search the OSB database with multiple queries.\"\"\"\n",
    "    print(\"\\n=== OSB Database Search Results ===\")\n",
    "\n",
    "    results = await rag_agent.fetch(queries)\n",
    "\n",
    "    for i, (query, result) in enumerate(zip(queries, results)):\n",
    "        print(f\"\\n--- Query {i+1}: {query} ---\")\n",
    "        print(f\"Found {len(result.results)} relevant chunks\")\n",
    "\n",
    "        if result.results:\n",
    "            # Show the top result\n",
    "            top_result = result.results[0]\n",
    "            print(f\"\\nTop Result:\")\n",
    "            print(f\"Document: {top_result.document_title}\")\n",
    "            print(f\"Case Number: {top_result.metadata.get('case_number', 'N/A')}\")\n",
    "            print(f\"Text: {top_result.full_text[:300]}...\")\n",
    "        else:\n",
    "            print(\"No results found\")\n",
    "\n",
    "\n",
    "# Example search queries\n",
    "search_queries = [\n",
    "    \"What are the challenges with automated content moderation?\",\n",
    "    \"How effective are age verification systems?\",\n",
    "    \"What techniques are used to spread misinformation?\",\n",
    "]\n",
    "\n",
    "await search_osb_database(search_queries)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Interactive Chat Interface\n",
    "\n",
    "Now let's create an interactive interface to chat with our OSB knowledge base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def chat_with_osb(user_question):\n",
    "    \"\"\"Interactive chat with OSB knowledge base.\"\"\"\n",
    "    print(f\"\\nüîç User Question: {user_question}\")\n",
    "\n",
    "    # Search for relevant context\n",
    "    search_results = await rag_agent.fetch([user_question])\n",
    "\n",
    "    if search_results and search_results[0].results:\n",
    "        context = search_results[0]\n",
    "        print(f\"\\nüìö Found {len(context.results)} relevant documents\")\n",
    "\n",
    "        # Display relevant chunks\n",
    "        print(\"\\nüìã Relevant Information:\")\n",
    "        for i, result in enumerate(context.results[:3]):  # Show top 3\n",
    "            print(f\"\\n{i+1}. {result.document_title} ({result.metadata.get('case_number', 'N/A')})\")\n",
    "            print(f\"   {result.full_text[:200]}...\")\n",
    "\n",
    "        # In a real implementation, this would be sent to an LLM for synthesis\n",
    "        print(\"\\nü§ñ AI Response: [In a real implementation, the retrieved context would be sent to an LLM to generate a synthesized response]\")\n",
    "    else:\n",
    "        print(\"\\n‚ùå No relevant information found in the OSB database\")\n",
    "\n",
    "\n",
    "# Example chat interactions\n",
    "example_questions = [\n",
    "    \"What are the main issues with current content moderation approaches?\",\n",
    "    \"What recommendations exist for age verification?\",\n",
    "    \"How do platforms detect and counter misinformation?\",\n",
    "]\n",
    "\n",
    "for question in example_questions:\n",
    "    await chat_with_osb(question)\n",
    "    print(\"\\n\" + \"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Vector Store Analysis\n",
    "\n",
    "Let's analyze our vector store to understand what we've created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get collection statistics\n",
    "collection = vectorstore.collection\n",
    "count = collection.count()\n",
    "\n",
    "print(f\"\\n=== OSB Vector Store Statistics ===\")\n",
    "print(f\"Collection Name: {vectorstore.collection_name}\")\n",
    "print(f\"Total Chunks: {count}\")\n",
    "print(f\"Embedding Dimensions: {vectorstore.dimensionality}\")\n",
    "print(f\"Embedding Model: {vectorstore.embedding_model}\")\n",
    "\n",
    "# Get a sample of metadata to understand the structure\n",
    "sample_results = collection.get(limit=3, include=[\"metadatas\", \"documents\"])\n",
    "\n",
    "print(f\"\\n=== Sample Metadata Structure ===\")\n",
    "if sample_results[\"metadatas\"]:\n",
    "    sample_metadata = sample_results[\"metadatas\"][0]\n",
    "    print(\"Available metadata fields:\")\n",
    "    for key, value in sample_metadata.items():\n",
    "        print(f\"  - {key}: {type(value).__name__} = {str(value)[:50]}...\")\n",
    "\n",
    "print(f\"\\n=== Storage Locations ===\")\n",
    "print(f\"ChromaDB Directory: {vectorstore.persist_directory}\")\n",
    "print(f\"Embeddings Directory: {vectorstore.arrow_save_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Advanced Search Examples\n",
    "\n",
    "Let's explore some advanced search patterns and filtering capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Direct ChromaDB queries with metadata filtering\n",
    "async def advanced_search_examples():\n",
    "    \"\"\"Demonstrate advanced search capabilities.\"\"\"\n",
    "    print(\"\\n=== Advanced Search Examples ===\")\n",
    "\n",
    "    # 1. Search with metadata filtering\n",
    "    print(\"\\n1. Search within specific case:\")\n",
    "    results = collection.query(\n",
    "        query_texts=[\"content moderation challenges\"], n_results=5, where={\"case_number\": \"OSB-2024-001\"}, include=[\"documents\", \"metadatas\"]\n",
    "    )\n",
    "    print(f\"   Found {len(results['ids'][0]) if results['ids'] else 0} results in OSB-2024-001\")\n",
    "\n",
    "    # 2. Similarity search across all documents\n",
    "    print(\"\\n2. General similarity search:\")\n",
    "    results = collection.query(query_texts=[\"artificial intelligence and safety\"], n_results=5, include=[\"documents\", \"metadatas\", \"distances\"])\n",
    "\n",
    "    if results[\"ids\"] and results[\"ids\"][0]:\n",
    "        print(f\"   Found {len(results['ids'][0])} results\")\n",
    "        for i, (doc, metadata, distance) in enumerate(zip(results[\"documents\"][0][:3], results[\"metadatas\"][0][:3], results[\"distances\"][0][:3])):\n",
    "            print(f\"   Result {i+1} (similarity: {1-distance:.3f}): {metadata.get('title', 'N/A')}\")\n",
    "            print(f\"     {doc[:100]}...\")\n",
    "\n",
    "    # 3. Multi-query search\n",
    "    print(\"\\n3. Multi-query search:\")\n",
    "    multi_queries = [\"platform safety measures\", \"user protection mechanisms\", \"digital safety standards\"]\n",
    "\n",
    "    for query in multi_queries:\n",
    "        results = collection.query(query_texts=[query], n_results=2, include=[\"metadatas\"])\n",
    "        count = len(results[\"ids\"][0]) if results[\"ids\"] else 0\n",
    "        print(f\"   '{query}': {count} results\")\n",
    "\n",
    "\n",
    "await advanced_search_examples()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Production Considerations\n",
    "\n",
    "Here are key considerations for using this in production:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    \"\"\"\n",
    "=== Production Deployment Checklist ===\n",
    "\n",
    "üîß Configuration:\n",
    "   ‚úì Use GCS for persist_directory: gs://your-bucket/chromadb\n",
    "   ‚úì Configure appropriate chunk_size for your content\n",
    "   ‚úì Set concurrency based on your compute resources\n",
    "   ‚úì Use production embedding models (text-embedding-004/005)\n",
    "\n",
    "üìä Performance:\n",
    "   ‚úì Monitor embedding generation costs\n",
    "   ‚úì Implement caching for frequently accessed data\n",
    "   ‚úì Use batch processing for large datasets\n",
    "   ‚úì Configure appropriate timeout values\n",
    "\n",
    "üîí Security:\n",
    "   ‚úì Secure GCS bucket access with proper IAM\n",
    "   ‚úì Implement data access controls\n",
    "   ‚úì Audit vector store queries\n",
    "   ‚úì Protect sensitive metadata\n",
    "\n",
    "üöÄ Scalability:\n",
    "   ‚úì Plan for vector store size growth\n",
    "   ‚úì Implement horizontal scaling for embeddings\n",
    "   ‚úì Monitor query performance\n",
    "   ‚úì Set up proper logging and monitoring\n",
    "\n",
    "üîÑ Maintenance:\n",
    "   ‚úì Plan for data updates and reindexing\n",
    "   ‚úì Implement backup strategies\n",
    "   ‚úì Version control for embeddings and metadata\n",
    "   ‚úì Regular quality assessments\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# Show next steps\n",
    "print(\n",
    "    \"\"\"\n",
    "=== Next Steps ===\n",
    "\n",
    "1. Scale to Full Dataset:\n",
    "   - Use the osb_vectorize.yaml configuration\n",
    "   - Run: uv run python -m buttermilk.data.vector +run=osb_vectorize\n",
    "\n",
    "2. Deploy RAG Flow:\n",
    "   - Use the osb_rag.yaml flow configuration\n",
    "   - Run: uv run python -m buttermilk.runner.cli +flow=osb_rag +run=api\n",
    "\n",
    "3. Integrate with Frontend:\n",
    "   - Use the Buttermilk web interface\n",
    "   - Connect to WebSocket endpoints for real-time chat\n",
    "\n",
    "4. Monitor and Optimize:\n",
    "   - Track query performance\n",
    "   - Monitor embedding costs\n",
    "   - Tune chunk sizes and retrieval parameters\n",
    "\"\"\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Production Deployment Guide\n",
    "\n",
    "This vector store is now ready for production use. Here's how to deploy and use it:\n",
    "\n",
    "### üìã **For Full Dataset Processing**\n",
    "```python\n",
    "# In cell 7, change this line:\n",
    "doc_limit = 5  # Set to None for full dataset\n",
    "\n",
    "# To:\n",
    "doc_limit = None  # Processes all OSB documents\n",
    "```\n",
    "\n",
    "### üè≠ **Production Usage Examples**\n",
    "\n",
    "#### **Option 1: RAG Agent Integration**\n",
    "```python\n",
    "from buttermilk.agents.rag.rag_agent import RagAgent\n",
    "from buttermilk._core.config import AgentConfig, DataSourceConfig\n",
    "\n",
    "# Same config as creation - no changes needed!\n",
    "data_config = DataSourceConfig(**cfg.storage.osb_vector)\n",
    "\n",
    "agent_config = AgentConfig(\n",
    "    role=\"RESEARCHER\",\n",
    "    agent_obj=\"RagAgent\", \n",
    "    description=\"OSB Knowledge Assistant\",\n",
    "    data={\"osb_vector\": data_config},\n",
    "    parameters={\"n_results\": 10, \"max_queries\": 3}\n",
    ")\n",
    "\n",
    "rag_agent = RagAgent(**agent_config.model_dump())\n",
    "```\n",
    "\n",
    "#### **Option 2: Direct ChromaDB Access**\n",
    "```python\n",
    "# Create vector store instance (reads existing embeddings)\n",
    "production_vectorstore = bm.get_storage(cfg.storage.osb_vector)\n",
    "await production_vectorstore.ensure_cache_initialized()\n",
    "\n",
    "# Perform semantic search\n",
    "results = production_vectorstore.collection.query(\n",
    "    query_texts=[\"platform safety policies\"],\n",
    "    n_results=5\n",
    ")\n",
    "```\n",
    "\n",
    "#### **Option 3: Flow Integration**\n",
    "```yaml\n",
    "# conf/flows/osb_rag.yaml\n",
    "defaults:\n",
    "  - base_flow\n",
    "\n",
    "orchestrator: buttermilk.orchestrators.groupchat.AutogenOrchestrator\n",
    "data: osb_vector  # References the same storage config\n",
    "agents: [rag_agent, host/sequencer]\n",
    "```\n",
    "\n",
    "### üîí **Production Considerations**\n",
    "- ‚úÖ **Persistent Storage**: Vector store saved to `gs://prosocial-public/osb/chromadb`  \n",
    "- ‚úÖ **Config Reuse**: Same `osb.yaml` works for both creation and reading\n",
    "- ‚úÖ **Scalability**: ChromaDB handles thousands of documents efficiently\n",
    "- ‚úÖ **Monitoring**: Check collection count and performance metrics\n",
    "- ‚úÖ **Updates**: Re-run this notebook to add new OSB documents\n",
    "\n",
    "### üí° **Next Steps**\n",
    "1. **Scale Up**: Remove `doc_limit` to process full OSB dataset\n",
    "2. **Deploy**: Use in RAG agents, search APIs, or analytical workflows  \n",
    "3. **Monitor**: Track embedding quality and search relevance\n",
    "4. **Iterate**: Add new documents by re-running the pipeline"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}