{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OSB Vector Database Example\n",
    "\n",
    "This notebook demonstrates how to create and use a vector database from OSB (Online Safety Bureau) full text data using Buttermilk's ChromaDB integration.\n",
    "\n",
    "## Overview\n",
    "\n",
    "We'll show how to:\n",
    "1. Load OSB JSON data using existing data loaders\n",
    "2. Generate embeddings and create a ChromaDB vector store\n",
    "3. Use the generic RAG agent for interactive question answering\n",
    "4. Demonstrate semantic search capabilities\n",
    "\n",
    "This example uses the generic infrastructure that works with any JSON dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import json\n",
    "from pathlib import Path\n",
    "import hydra\n",
    "from hydra import compose, initialize_config_dir\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "\n",
    "# Buttermilk imports\n",
    "from buttermilk import logger\n",
    "from buttermilk.data.vector import ChromaDBEmbeddings, InputDocument, DefaultTextSplitter\n",
    "from buttermilk.data.loaders.json_loader import JsonDataLoader\n",
    "from buttermilk.agents.rag.rag_agent import RagAgent\n",
    "from buttermilk._core.config import AgentConfig, DataSourceConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration Setup\n",
    "\n",
    "First, let's set up the configuration for our OSB vector database pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for OSB data processing\n",
    "config = {\n",
    "    \"vectoriser\": {\n",
    "        \"_target_\": \"buttermilk.data.vector.ChromaDBEmbeddings\",\n",
    "        \"persist_directory\": \"./data/osb_chromadb\",  # Local for example\n",
    "        \"collection_name\": \"osb_fulltext\",\n",
    "        \"embedding_model\": \"text-embedding-005\",\n",
    "        \"dimensionality\": 768,\n",
    "        \"arrow_save_dir\": \"./data/osb_embeddings\",\n",
    "        \"concurrency\": 5,  # Reduced for example\n",
    "        \"upsert_batch_size\": 10\n",
    "    },\n",
    "    \"chunker\": {\n",
    "        \"_target_\": \"buttermilk.data.vector.DefaultTextSplitter\",\n",
    "        \"chunk_size\": 2000,\n",
    "        \"chunk_overlap\": 500\n",
    "    },\n",
    "    \"data_loader\": {\n",
    "        \"uri\": \"gs://prosocial-public/osb/03_osb_fulltext_summaries.json\",\n",
    "        \"field_mapping\": {\n",
    "            \"record_id\": \"id\",\n",
    "            \"content\": \"full_text\",\n",
    "            \"metadata\": {\n",
    "                \"title\": \"title\",\n",
    "                \"case_number\": \"case_number\",\n",
    "                \"url\": \"url\",\n",
    "                \"summary\": \"summary\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Configuration loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initialize Components\n",
    "\n",
    "Let's create the vector store and text splitter components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directories\n",
    "Path(\"./data/osb_chromadb\").mkdir(parents=True, exist_ok=True)\n",
    "Path(\"./data/osb_embeddings\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Initialize ChromaDB vector store\n",
    "vectorstore = ChromaDBEmbeddings(\n",
    "    persist_directory=\"./data/osb_chromadb\",\n",
    "    collection_name=\"osb_fulltext\",\n",
    "    embedding_model=\"text-embedding-005\",\n",
    "    dimensionality=768,\n",
    "    arrow_save_dir=\"./data/osb_embeddings\",\n",
    "    concurrency=5,\n",
    "    upsert_batch_size=10\n",
    ")\n",
    "\n",
    "# Initialize text splitter\n",
    "chunker = DefaultTextSplitter(\n",
    "    chunk_size=2000,\n",
    "    chunk_overlap=500\n",
    ")\n",
    "\n",
    "print(\"Components initialized successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load OSB Data\n",
    "\n",
    "Now let's load the OSB JSON data and convert it to InputDocument format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For this example, let's simulate loading a subset of OSB data\n",
    "# In practice, you would use the JsonDataLoader with the GCS URI\n",
    "\n",
    "sample_osb_data = [\n",
    "    {\n",
    "        \"id\": \"osb_case_001\",\n",
    "        \"title\": \"Platform Safety Measures Review\",\n",
    "        \"case_number\": \"OSB-2024-001\",\n",
    "        \"url\": \"https://example.com/case1\",\n",
    "        \"summary\": \"Review of content moderation policies\",\n",
    "        \"full_text\": \"This case examines the effectiveness of automated content moderation systems in detecting harmful content across social media platforms. The analysis reveals significant gaps in current detection algorithms, particularly for context-dependent harmful speech. Recommendations include implementing hybrid human-AI moderation systems and developing more sophisticated natural language processing capabilities.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"osb_case_002\",\n",
    "        \"title\": \"Age Verification Systems\",\n",
    "        \"case_number\": \"OSB-2024-002\",\n",
    "        \"url\": \"https://example.com/case2\",\n",
    "        \"summary\": \"Assessment of age verification technologies\",\n",
    "        \"full_text\": \"This report evaluates various age verification technologies deployed by online platforms to protect minors. The study compares biometric verification, document-based verification, and behavioral analysis methods. Key findings indicate that multi-factor verification approaches provide the highest accuracy while maintaining user privacy. The report recommends establishing industry standards for age verification and regular auditing of verification systems.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"osb_case_003\",\n",
    "        \"title\": \"Misinformation Detection\",\n",
    "        \"case_number\": \"OSB-2024-003\",\n",
    "        \"url\": \"https://example.com/case3\",\n",
    "        \"summary\": \"Analysis of misinformation spread patterns\",\n",
    "        \"full_text\": \"This analysis investigates how misinformation spreads across digital platforms and the effectiveness of current countermeasures. The study identifies rapid amplification through bot networks and coordinated inauthentic behavior as primary vectors for misinformation dissemination. Proposed interventions include real-time fact-checking integration, source credibility scoring, and user education programs about information literacy.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Convert to InputDocument format\n",
    "documents = []\n",
    "for item in sample_osb_data:\n",
    "    doc = InputDocument(\n",
    "        record_id=item[\"id\"],\n",
    "        title=item[\"title\"],\n",
    "        full_text=item[\"full_text\"],\n",
    "        metadata={\n",
    "            \"title\": item[\"title\"],\n",
    "            \"case_number\": item[\"case_number\"],\n",
    "            \"url\": item[\"url\"],\n",
    "            \"summary\": item[\"summary\"]\n",
    "        },\n",
    "        file_path=\"\"\n",
    "    )\n",
    "    documents.append(doc)\n",
    "\n",
    "print(f\"Loaded {len(documents)} OSB documents\")\n",
    "for doc in documents:\n",
    "    print(f\"- {doc.record_id}: {doc.title}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Process Documents: Chunking and Embedding\n",
    "\n",
    "Now we'll chunk the documents and generate embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def process_documents():\n",
    "    \"\"\"Process documents through the complete pipeline.\"\"\"\n",
    "    processed_docs = []\n",
    "    \n",
    "    for doc in documents:\n",
    "        print(f\"\\nProcessing document: {doc.record_id}\")\n",
    "        \n",
    "        # Step 1: Chunk the document\n",
    "        chunked_doc = await chunker.process(doc)\n",
    "        if not chunked_doc:\n",
    "            print(f\"Failed to chunk document {doc.record_id}\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"Created {len(chunked_doc.chunks)} chunks\")\n",
    "        \n",
    "        # Step 2: Generate embeddings and store\n",
    "        processed_doc = await vectorstore.process(chunked_doc)\n",
    "        if processed_doc:\n",
    "            processed_docs.append(processed_doc)\n",
    "            print(f\"Successfully embedded and stored document {doc.record_id}\")\n",
    "        else:\n",
    "            print(f\"Failed to process document {doc.record_id}\")\n",
    "    \n",
    "    return processed_docs\n",
    "\n",
    "# Run the processing pipeline\n",
    "processed_documents = await process_documents()\n",
    "print(f\"\\nProcessed {len(processed_documents)} documents successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Initialize RAG Agent\n",
    "\n",
    "Now let's create a generic RAG agent to query our OSB vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data source configuration for the RAG agent\n",
    "data_config = DataSourceConfig(\n",
    "    type=\"chromadb\",\n",
    "    persist_directory=\"./data/osb_chromadb\",\n",
    "    collection_name=\"osb_fulltext\",\n",
    "    embedding_model=\"text-embedding-005\",\n",
    "    dimensionality=768\n",
    ")\n",
    "\n",
    "# Create agent configuration\n",
    "agent_config = AgentConfig(\n",
    "    role=\"RESEARCHER\",\n",
    "    agent_obj=\"RagAgent\",\n",
    "    description=\"OSB Research Assistant\",\n",
    "    data={\"osb_vector\": data_config},\n",
    "    variants={\"model\": \"gemini-1.5-flash\"},\n",
    "    parameters={\n",
    "        \"template\": \"rag_research\",\n",
    "        \"n_results\": 10,\n",
    "        \"no_duplicates\": False,\n",
    "        \"max_queries\": 3\n",
    "    }\n",
    ")\n",
    "\n",
    "# Initialize the RAG agent\n",
    "rag_agent = RagAgent(**agent_config.model_dump())\n",
    "print(\"RAG agent initialized successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Semantic Search Examples\n",
    "\n",
    "Let's demonstrate the semantic search capabilities of our OSB vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def search_osb_database(queries):\n",
    "    \"\"\"Search the OSB database with multiple queries.\"\"\"\n",
    "    print(\"\\n=== OSB Database Search Results ===\")\n",
    "    \n",
    "    results = await rag_agent.fetch(queries)\n",
    "    \n",
    "    for i, (query, result) in enumerate(zip(queries, results)):\n",
    "        print(f\"\\n--- Query {i+1}: {query} ---\")\n",
    "        print(f\"Found {len(result.results)} relevant chunks\")\n",
    "        \n",
    "        if result.results:\n",
    "            # Show the top result\n",
    "            top_result = result.results[0]\n",
    "            print(f\"\\nTop Result:\")\n",
    "            print(f\"Document: {top_result.document_title}\")\n",
    "            print(f\"Case Number: {top_result.metadata.get('case_number', 'N/A')}\")\n",
    "            print(f\"Text: {top_result.full_text[:300]}...\")\n",
    "        else:\n",
    "            print(\"No results found\")\n",
    "\n",
    "# Example search queries\n",
    "search_queries = [\n",
    "    \"What are the challenges with automated content moderation?\",\n",
    "    \"How effective are age verification systems?\", \n",
    "    \"What techniques are used to spread misinformation?\"\n",
    "]\n",
    "\n",
    "await search_osb_database(search_queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Interactive Chat Interface\n",
    "\n",
    "Now let's create an interactive interface to chat with our OSB knowledge base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def chat_with_osb(user_question):\n",
    "    \"\"\"Interactive chat with OSB knowledge base.\"\"\"\n",
    "    print(f\"\\n🔍 User Question: {user_question}\")\n",
    "    \n",
    "    # Search for relevant context\n",
    "    search_results = await rag_agent.fetch([user_question])\n",
    "    \n",
    "    if search_results and search_results[0].results:\n",
    "        context = search_results[0]\n",
    "        print(f\"\\n📚 Found {len(context.results)} relevant documents\")\n",
    "        \n",
    "        # Display relevant chunks\n",
    "        print(\"\\n📋 Relevant Information:\")\n",
    "        for i, result in enumerate(context.results[:3]):  # Show top 3\n",
    "            print(f\"\\n{i+1}. {result.document_title} ({result.metadata.get('case_number', 'N/A')})\")\n",
    "            print(f\"   {result.full_text[:200]}...\")\n",
    "            \n",
    "        # In a real implementation, this would be sent to an LLM for synthesis\n",
    "        print(\"\\n🤖 AI Response: [In a real implementation, the retrieved context would be sent to an LLM to generate a synthesized response]\")\n",
    "    else:\n",
    "        print(\"\\n❌ No relevant information found in the OSB database\")\n",
    "\n",
    "# Example chat interactions\n",
    "example_questions = [\n",
    "    \"What are the main issues with current content moderation approaches?\",\n",
    "    \"What recommendations exist for age verification?\",\n",
    "    \"How do platforms detect and counter misinformation?\"\n",
    "]\n",
    "\n",
    "for question in example_questions:\n",
    "    await chat_with_osb(question)\n",
    "    print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Vector Store Analysis\n",
    "\n",
    "Let's analyze our vector store to understand what we've created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get collection statistics\n",
    "collection = vectorstore.collection\n",
    "count = collection.count()\n",
    "\n",
    "print(f\"\\n=== OSB Vector Store Statistics ===\")\n",
    "print(f\"Collection Name: {vectorstore.collection_name}\")\n",
    "print(f\"Total Chunks: {count}\")\n",
    "print(f\"Embedding Dimensions: {vectorstore.dimensionality}\")\n",
    "print(f\"Embedding Model: {vectorstore.embedding_model}\")\n",
    "\n",
    "# Get a sample of metadata to understand the structure\n",
    "sample_results = collection.get(limit=3, include=[\"metadatas\", \"documents\"])\n",
    "\n",
    "print(f\"\\n=== Sample Metadata Structure ===\")\n",
    "if sample_results['metadatas']:\n",
    "    sample_metadata = sample_results['metadatas'][0]\n",
    "    print(\"Available metadata fields:\")\n",
    "    for key, value in sample_metadata.items():\n",
    "        print(f\"  - {key}: {type(value).__name__} = {str(value)[:50]}...\")\n",
    "\n",
    "print(f\"\\n=== Storage Locations ===\")\n",
    "print(f\"ChromaDB Directory: {vectorstore.persist_directory}\")\n",
    "print(f\"Embeddings Directory: {vectorstore.arrow_save_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Advanced Search Examples\n",
    "\n",
    "Let's explore some advanced search patterns and filtering capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Direct ChromaDB queries with metadata filtering\n",
    "async def advanced_search_examples():\n",
    "    \"\"\"Demonstrate advanced search capabilities.\"\"\"\n",
    "    print(\"\\n=== Advanced Search Examples ===\")\n",
    "    \n",
    "    # 1. Search with metadata filtering\n",
    "    print(\"\\n1. Search within specific case:\")\n",
    "    results = collection.query(\n",
    "        query_texts=[\"content moderation challenges\"],\n",
    "        n_results=5,\n",
    "        where={\"case_number\": \"OSB-2024-001\"},\n",
    "        include=[\"documents\", \"metadatas\"]\n",
    "    )\n",
    "    print(f\"   Found {len(results['ids'][0]) if results['ids'] else 0} results in OSB-2024-001\")\n",
    "    \n",
    "    # 2. Similarity search across all documents\n",
    "    print(\"\\n2. General similarity search:\")\n",
    "    results = collection.query(\n",
    "        query_texts=[\"artificial intelligence and safety\"],\n",
    "        n_results=5,\n",
    "        include=[\"documents\", \"metadatas\", \"distances\"]\n",
    "    )\n",
    "    \n",
    "    if results['ids'] and results['ids'][0]:\n",
    "        print(f\"   Found {len(results['ids'][0])} results\")\n",
    "        for i, (doc, metadata, distance) in enumerate(zip(\n",
    "            results['documents'][0][:3], \n",
    "            results['metadatas'][0][:3],\n",
    "            results['distances'][0][:3]\n",
    "        )):\n",
    "            print(f\"   Result {i+1} (similarity: {1-distance:.3f}): {metadata.get('title', 'N/A')}\")\n",
    "            print(f\"     {doc[:100]}...\")\n",
    "    \n",
    "    # 3. Multi-query search\n",
    "    print(\"\\n3. Multi-query search:\")\n",
    "    multi_queries = [\n",
    "        \"platform safety measures\",\n",
    "        \"user protection mechanisms\",\n",
    "        \"digital safety standards\"\n",
    "    ]\n",
    "    \n",
    "    for query in multi_queries:\n",
    "        results = collection.query(\n",
    "            query_texts=[query],\n",
    "            n_results=2,\n",
    "            include=[\"metadatas\"]\n",
    "        )\n",
    "        count = len(results['ids'][0]) if results['ids'] else 0\n",
    "        print(f\"   '{query}': {count} results\")\n",
    "\n",
    "await advanced_search_examples()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Production Considerations\n",
    "\n",
    "Here are key considerations for using this in production:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "=== Production Deployment Checklist ===\n",
    "\n",
    "🔧 Configuration:\n",
    "   ✓ Use GCS for persist_directory: gs://your-bucket/chromadb\n",
    "   ✓ Configure appropriate chunk_size for your content\n",
    "   ✓ Set concurrency based on your compute resources\n",
    "   ✓ Use production embedding models (text-embedding-004/005)\n",
    "\n",
    "📊 Performance:\n",
    "   ✓ Monitor embedding generation costs\n",
    "   ✓ Implement caching for frequently accessed data\n",
    "   ✓ Use batch processing for large datasets\n",
    "   ✓ Configure appropriate timeout values\n",
    "\n",
    "🔒 Security:\n",
    "   ✓ Secure GCS bucket access with proper IAM\n",
    "   ✓ Implement data access controls\n",
    "   ✓ Audit vector store queries\n",
    "   ✓ Protect sensitive metadata\n",
    "\n",
    "🚀 Scalability:\n",
    "   ✓ Plan for vector store size growth\n",
    "   ✓ Implement horizontal scaling for embeddings\n",
    "   ✓ Monitor query performance\n",
    "   ✓ Set up proper logging and monitoring\n",
    "\n",
    "🔄 Maintenance:\n",
    "   ✓ Plan for data updates and reindexing\n",
    "   ✓ Implement backup strategies\n",
    "   ✓ Version control for embeddings and metadata\n",
    "   ✓ Regular quality assessments\n",
    "\"\"\")\n",
    "\n",
    "# Show next steps\n",
    "print(\"\"\"\n",
    "=== Next Steps ===\n",
    "\n",
    "1. Scale to Full Dataset:\n",
    "   - Use the osb_vectorize.yaml configuration\n",
    "   - Run: uv run python -m buttermilk.data.vector +run=osb_vectorize\n",
    "\n",
    "2. Deploy RAG Flow:\n",
    "   - Use the osb_rag.yaml flow configuration\n",
    "   - Run: uv run python -m buttermilk.runner.cli +flow=osb_rag +run=api\n",
    "\n",
    "3. Integrate with Frontend:\n",
    "   - Use the Buttermilk web interface\n",
    "   - Connect to WebSocket endpoints for real-time chat\n",
    "\n",
    "4. Monitor and Optimize:\n",
    "   - Track query performance\n",
    "   - Monitor embedding costs\n",
    "   - Tune chunk sizes and retrieval parameters\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated how to:\n",
    "\n",
    "1. **Load OSB data** using Buttermilk's flexible JSON data loaders\n",
    "2. **Create embeddings** with the ChromaDBEmbeddings pipeline\n",
    "3. **Build a vector database** suitable for semantic search\n",
    "4. **Use the generic RAG agent** for question answering\n",
    "5. **Perform semantic search** with various query patterns\n",
    "6. **Analyze the vector store** structure and contents\n",
    "\n",
    "The key advantage of this approach is that it uses **generic, reusable components** that work with any JSON dataset, not just OSB data. The same patterns can be applied to any document collection for RAG applications.\n",
    "\n",
    "The infrastructure is **production-ready** with features like:\n",
    "- Async processing for scalability\n",
    "- GCS integration for cloud storage\n",
    "- Configurable chunking and embedding parameters\n",
    "- Error handling and retry mechanisms\n",
    "- Comprehensive logging and monitoring\n",
    "\n",
    "This provides a solid foundation for building sophisticated RAG applications with Buttermilk."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}