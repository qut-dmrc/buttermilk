{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OSB Vector Database Example\n",
    "\n",
    "This notebook demonstrates how to create and use a vector database from Oversight Board full text data using Buttermilk's ChromaDB integration.\n",
    "\n",
    "## Overview\n",
    "\n",
    "We'll show how to:\n",
    "1. Load OSB JSON data using existing data loaders\n",
    "2. Generate embeddings and create a ChromaDB vector store\n",
    "3. Use the generic RAG agent for interactive question answering\n",
    "4. Demonstrate semantic search capabilities\n",
    "\n",
    "This example uses the generic infrastructure that works with any JSON dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration Setup\n",
    "\n",
    "First, let's set up the configuration for our OSB vector database pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-06-17 19:19:56\u001b[0m [] \u001b[1;30mINFO\u001b[0m bm_init.py:778 Logging set up for run: platform='local' name='bm_api' job='osb_vectorise' run_id='20250617T0919Z-bhxW-docker-desktop-debian' ip=None node_name='docker-desktop' save_dir='/tmp/tmpv4auodbe/bm_api/osb_vectorise/20250617T0919Z-bhxW-docker-desktop-debian' flow_api=None. Save directory: /tmp/tmpv4auodbe/bm_api/osb_vectorise/20250617T0919Z-bhxW-docker-desktop-debian\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Initialized Buttermilk <span style=\"font-weight: bold\">(</span>bm<span style=\"font-weight: bold\">)</span> with configuration:\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Initialized Buttermilk \u001b[1m(\u001b[0mbm\u001b[1m)\u001b[0m with configuration:\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'platform'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'local'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'name'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'bm_api'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'job'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'osb_vectorise'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'run_id'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'20250617T0919Z-bhxW-docker-desktop-debian'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'node_name'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'docker-desktop'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'save_dir'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'/tmp/tmpv4auodbe/bm_api/osb_vectorise/20250617T0919Z-bhxW-docker-desktop-debian'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'connections'</span>: <span style=\"font-weight: bold\">[]</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'secret_provider'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'type'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'gcp'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'project'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'prosocial-443205'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'models_secret'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'dev__llm__connections'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'credentials_secret'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'dev__shared_credentials'</span>\n",
       "    <span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'logger_cfg'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'type'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'gcp'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'project'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'prosocial-443205'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'location'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'us-central1'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'verbose'</span>: <span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span><span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'pubsub'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'type'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'gcp'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'project'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'prosocial-443205'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'jobs_subscription'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'jobs-sub'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'status_subscription'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'flow-sub'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'status_topic'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'flow'</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'jobs_topic'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'jobs'</span>\n",
       "    <span style=\"font-weight: bold\">}</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'clouds'</span>: <span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'type'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'gcp'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'project'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'prosocial-443205'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'quota_project_id'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'prosocial-443205'</span><span style=\"font-weight: bold\">}</span>,\n",
       "        <span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'type'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'vertex'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'project'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'prosocial-443205'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'region'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'us-central1'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'location'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'us-central1'</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'bucket'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'prosocial-de'</span>\n",
       "        <span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">]</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'tracing'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'enabled'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'api_key'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">''</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'provider'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'weave'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'otlp_headers'</span>: <span style=\"font-weight: bold\">{}}</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'datasets'</span>: <span style=\"font-weight: bold\">{}</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'save_dir_base'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'/tmp/tmpv4auodbe'</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m{\u001b[0m\n",
       "    \u001b[32m'platform'\u001b[0m: \u001b[32m'local'\u001b[0m,\n",
       "    \u001b[32m'name'\u001b[0m: \u001b[32m'bm_api'\u001b[0m,\n",
       "    \u001b[32m'job'\u001b[0m: \u001b[32m'osb_vectorise'\u001b[0m,\n",
       "    \u001b[32m'run_id'\u001b[0m: \u001b[32m'20250617T0919Z-bhxW-docker-desktop-debian'\u001b[0m,\n",
       "    \u001b[32m'node_name'\u001b[0m: \u001b[32m'docker-desktop'\u001b[0m,\n",
       "    \u001b[32m'save_dir'\u001b[0m: \u001b[32m'/tmp/tmpv4auodbe/bm_api/osb_vectorise/20250617T0919Z-bhxW-docker-desktop-debian'\u001b[0m,\n",
       "    \u001b[32m'connections'\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m,\n",
       "    \u001b[32m'secret_provider'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "        \u001b[32m'type'\u001b[0m: \u001b[32m'gcp'\u001b[0m,\n",
       "        \u001b[32m'project'\u001b[0m: \u001b[32m'prosocial-443205'\u001b[0m,\n",
       "        \u001b[32m'models_secret'\u001b[0m: \u001b[32m'dev__llm__connections'\u001b[0m,\n",
       "        \u001b[32m'credentials_secret'\u001b[0m: \u001b[32m'dev__shared_credentials'\u001b[0m\n",
       "    \u001b[1m}\u001b[0m,\n",
       "    \u001b[32m'logger_cfg'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'type'\u001b[0m: \u001b[32m'gcp'\u001b[0m, \u001b[32m'project'\u001b[0m: \u001b[32m'prosocial-443205'\u001b[0m, \u001b[32m'location'\u001b[0m: \u001b[32m'us-central1'\u001b[0m, \u001b[32m'verbose'\u001b[0m: \u001b[3;91mFalse\u001b[0m\u001b[1m}\u001b[0m,\n",
       "    \u001b[32m'pubsub'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "        \u001b[32m'type'\u001b[0m: \u001b[32m'gcp'\u001b[0m,\n",
       "        \u001b[32m'project'\u001b[0m: \u001b[32m'prosocial-443205'\u001b[0m,\n",
       "        \u001b[32m'jobs_subscription'\u001b[0m: \u001b[32m'jobs-sub'\u001b[0m,\n",
       "        \u001b[32m'status_subscription'\u001b[0m: \u001b[32m'flow-sub'\u001b[0m,\n",
       "        \u001b[32m'status_topic'\u001b[0m: \u001b[32m'flow'\u001b[0m,\n",
       "        \u001b[32m'jobs_topic'\u001b[0m: \u001b[32m'jobs'\u001b[0m\n",
       "    \u001b[1m}\u001b[0m,\n",
       "    \u001b[32m'clouds'\u001b[0m: \u001b[1m[\u001b[0m\n",
       "        \u001b[1m{\u001b[0m\u001b[32m'type'\u001b[0m: \u001b[32m'gcp'\u001b[0m, \u001b[32m'project'\u001b[0m: \u001b[32m'prosocial-443205'\u001b[0m, \u001b[32m'quota_project_id'\u001b[0m: \u001b[32m'prosocial-443205'\u001b[0m\u001b[1m}\u001b[0m,\n",
       "        \u001b[1m{\u001b[0m\n",
       "            \u001b[32m'type'\u001b[0m: \u001b[32m'vertex'\u001b[0m,\n",
       "            \u001b[32m'project'\u001b[0m: \u001b[32m'prosocial-443205'\u001b[0m,\n",
       "            \u001b[32m'region'\u001b[0m: \u001b[32m'us-central1'\u001b[0m,\n",
       "            \u001b[32m'location'\u001b[0m: \u001b[32m'us-central1'\u001b[0m,\n",
       "            \u001b[32m'bucket'\u001b[0m: \u001b[32m'prosocial-de'\u001b[0m\n",
       "        \u001b[1m}\u001b[0m\n",
       "    \u001b[1m]\u001b[0m,\n",
       "    \u001b[32m'tracing'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'enabled'\u001b[0m: \u001b[3;92mTrue\u001b[0m, \u001b[32m'api_key'\u001b[0m: \u001b[32m''\u001b[0m, \u001b[32m'provider'\u001b[0m: \u001b[32m'weave'\u001b[0m, \u001b[32m'otlp_headers'\u001b[0m: \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\u001b[1m}\u001b[0m,\n",
       "    \u001b[32m'datasets'\u001b[0m: \u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "    \u001b[32m'save_dir_base'\u001b[0m: \u001b[32m'/tmp/tmpv4auodbe'\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-06-17 19:19:56\u001b[0m [] \u001b[1;30mINFO\u001b[0m nb.py:59 Starting interactive run for bm_api job osb_vectorise in notebook\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">🚀 Buttermilk initialized for JSON-to-Vector tutorial\n",
       "</pre>\n"
      ],
      "text/plain": [
       "🚀 Buttermilk initialized for JSON-to-Vector tutorial\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'osb_json'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'type'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'gcs'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'path'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'gs://prosocial-public/osb/03_osb_fulltext_summaries.json'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'columns'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'record_id'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'record_id'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'content'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'fulltext'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'metadata'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'title'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'title'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'description'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'content'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'result'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'result'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'type'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'type'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'location'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'location'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'case_date'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'case_date'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'topics'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'topics'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'standards'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'standards'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'reasons'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'reasons'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'recommendations'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'recommendations'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'job_id'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'job_id'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'timestamp'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'timestamp'</span><span style=\"font-weight: bold\">}}}</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'osb_vector'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'type'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'chromadb'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'persist_directory'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'gs://prosocial-dev/data/osb/chromadb'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'collection_name'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'osb_fulltext'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'embedding_model'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'gemini-embedding-001'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'dimensionality'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3072</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'multi_field_embedding'</span>: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'content_field'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'content'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'chunk_size'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1200</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'chunk_overlap'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">400</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'additional_fields'</span>: <span style=\"font-weight: bold\">[{</span><span style=\"color: #008000; text-decoration-color: #008000\">'source_field'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'title'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'chunk_type'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'title'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'min_length'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span><span style=\"font-weight: bold\">}</span>, <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'source_field'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'description'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'chunk_type'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'case_description'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'min_length'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">50</span><span style=\"font-weight: bold\">}</span>, <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'source_field'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'reasons'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'chunk_type'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'reasoning'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'min_length'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">100</span><span style=\"font-weight: bold\">}</span>, <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'source_field'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'recommendations'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'chunk_type'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'recommendations'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'min_length'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">50</span><span style=\"font-weight: bold\">}]}}}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m{\u001b[0m\u001b[32m'osb_json'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'type'\u001b[0m: \u001b[32m'gcs'\u001b[0m, \u001b[32m'path'\u001b[0m: \u001b[32m'gs://prosocial-public/osb/03_osb_fulltext_summaries.json'\u001b[0m, \u001b[32m'columns'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'record_id'\u001b[0m: \u001b[32m'record_id'\u001b[0m, \u001b[32m'content'\u001b[0m: \u001b[32m'fulltext'\u001b[0m, \u001b[32m'metadata'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'title'\u001b[0m: \u001b[32m'title'\u001b[0m, \u001b[32m'description'\u001b[0m: \u001b[32m'content'\u001b[0m, \u001b[32m'result'\u001b[0m: \u001b[32m'result'\u001b[0m, \u001b[32m'type'\u001b[0m: \u001b[32m'type'\u001b[0m, \u001b[32m'location'\u001b[0m: \u001b[32m'location'\u001b[0m, \u001b[32m'case_date'\u001b[0m: \u001b[32m'case_date'\u001b[0m, \u001b[32m'topics'\u001b[0m: \u001b[32m'topics'\u001b[0m, \u001b[32m'standards'\u001b[0m: \u001b[32m'standards'\u001b[0m, \u001b[32m'reasons'\u001b[0m: \u001b[32m'reasons'\u001b[0m, \u001b[32m'recommendations'\u001b[0m: \u001b[32m'recommendations'\u001b[0m, \u001b[32m'job_id'\u001b[0m: \u001b[32m'job_id'\u001b[0m, \u001b[32m'timestamp'\u001b[0m: \u001b[32m'timestamp'\u001b[0m\u001b[1m}\u001b[0m\u001b[1m}\u001b[0m\u001b[1m}\u001b[0m, \u001b[32m'osb_vector'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'type'\u001b[0m: \u001b[32m'chromadb'\u001b[0m, \u001b[32m'persist_directory'\u001b[0m: \u001b[32m'gs://prosocial-dev/data/osb/chromadb'\u001b[0m, \u001b[32m'collection_name'\u001b[0m: \u001b[32m'osb_fulltext'\u001b[0m, \u001b[32m'embedding_model'\u001b[0m: \u001b[32m'gemini-embedding-001'\u001b[0m, \u001b[32m'dimensionality'\u001b[0m: \u001b[1;36m3072\u001b[0m, \u001b[32m'multi_field_embedding'\u001b[0m: \u001b[1m{\u001b[0m\u001b[32m'content_field'\u001b[0m: \u001b[32m'content'\u001b[0m, \u001b[32m'chunk_size'\u001b[0m: \u001b[1;36m1200\u001b[0m, \u001b[32m'chunk_overlap'\u001b[0m: \u001b[1;36m400\u001b[0m, \u001b[32m'additional_fields'\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m{\u001b[0m\u001b[32m'source_field'\u001b[0m: \u001b[32m'title'\u001b[0m, \u001b[32m'chunk_type'\u001b[0m: \u001b[32m'title'\u001b[0m, \u001b[32m'min_length'\u001b[0m: \u001b[1;36m10\u001b[0m\u001b[1m}\u001b[0m, \u001b[1m{\u001b[0m\u001b[32m'source_field'\u001b[0m: \u001b[32m'description'\u001b[0m, \u001b[32m'chunk_type'\u001b[0m: \u001b[32m'case_description'\u001b[0m, \u001b[32m'min_length'\u001b[0m: \u001b[1;36m50\u001b[0m\u001b[1m}\u001b[0m, \u001b[1m{\u001b[0m\u001b[32m'source_field'\u001b[0m: \u001b[32m'reasons'\u001b[0m, \u001b[32m'chunk_type'\u001b[0m: \u001b[32m'reasoning'\u001b[0m, \u001b[32m'min_length'\u001b[0m: \u001b[1;36m100\u001b[0m\u001b[1m}\u001b[0m, \u001b[1m{\u001b[0m\u001b[32m'source_field'\u001b[0m: \u001b[32m'recommendations'\u001b[0m, \u001b[32m'chunk_type'\u001b[0m: \u001b[32m'recommendations'\u001b[0m, \u001b[32m'min_length'\u001b[0m: \u001b[1;36m50\u001b[0m\u001b[1m}\u001b[0m\u001b[1m]\u001b[0m\u001b[1m}\u001b[0m\u001b[1m}\u001b[0m\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-06-17 19:19:56\u001b[0m [] \u001b[1;30mINFO\u001b[0m save.py:641 Successfully dumped data to local disk (JSON): /tmp/tmpv4auodbe/bm_api/osb_vectorise/20250617T0919Z-bhxW-docker-desktop-debian/tmpezbjfgxt.json.\n",
      "\u001b[32m2025-06-17 19:19:56\u001b[0m [] \u001b[1;30mINFO\u001b[0m save.py:215 Successfully saved data using dump_to_disk to: /tmp/tmpv4auodbe/bm_api/osb_vectorise/20250617T0919Z-bhxW-docker-desktop-debian/tmpezbjfgxt.json.\n",
      "\u001b[32m2025-06-17 19:19:56\u001b[0m [] \u001b[1;30mINFO\u001b[0m bm_init.py:864 {'message': 'Successfully saved data to: /tmp/tmpv4auodbe/bm_api/osb_vectorise/20250617T0919Z-bhxW-docker-desktop-debian/tmpezbjfgxt.json', 'uri': '/tmp/tmpv4auodbe/bm_api/osb_vectorise/20250617T0919Z-bhxW-docker-desktop-debian/tmpezbjfgxt.json', 'run_id': '20250617T0919Z-bhxW-docker-desktop-debian'}\n"
     ]
    }
   ],
   "source": [
    "from rich import print\n",
    "from rich.pretty import pprint\n",
    "import asyncio\n",
    "import json\n",
    "from pathlib import Path\n",
    "import hydra\n",
    "from hydra import compose, initialize_config_dir\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "\n",
    "# Buttermilk imports - updated for unified storage system\n",
    "from buttermilk import logger\n",
    "from buttermilk.data.vector import ChromaDBEmbeddings, DefaultTextSplitter\n",
    "from buttermilk.agents.rag.rag_agent import RagAgent\n",
    "from buttermilk._core.config import AgentConfig\n",
    "from buttermilk._core.storage_config import StorageConfig  # New unified config\n",
    "from buttermilk._core.types import Record  # Enhanced Record with vector capabilities\n",
    "\n",
    "from buttermilk.utils.nb import init\n",
    "from buttermilk._core.dmrc import get_bm, set_bm\n",
    "\n",
    "# Initialize Buttermilk\n",
    "cfg = init(job=\"osb_vectorise\", overrides=[\"+storage=osb\", \"+agents=rag_generic\", \"+llms=lite\"])\n",
    "bm = get_bm()\n",
    "\n",
    "print(\"🚀 Buttermilk initialized for JSON-to-Vector tutorial\")\n",
    "pprint(cfg.storage)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initialize Components\n",
    "\n",
    "Let's create the storage, vector store, and text splitter components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-06-17 19:20:05\u001b[0m [] \u001b[1;30mINFO\u001b[0m vector.py:351 Loading embedding model: gemini-embedding-001\n",
      "\u001b[32m2025-06-17 19:20:09\u001b[0m [] \u001b[1;30mINFO\u001b[0m vector.py:359 Initializing ChromaDB client at: gs://prosocial-dev/data/osb/chromadb\n",
      "\u001b[32m2025-06-17 19:20:09\u001b[0m [] \u001b[1;30mINFO\u001b[0m vector.py:364 Using ChromaDB collection: osb_fulltext\n",
      "\u001b[32m2025-06-17 19:20:09\u001b[0m [] \u001b[1;30mINFO\u001b[0m vector.py:370 🔄 Auto-sync enabled: every 50 records OR every 10 minutes\n",
      "\u001b[32m2025-06-17 19:20:09\u001b[0m [] \u001b[1;30mINFO\u001b[0m vector.py:375 🔍 Deduplication strategy: both\n",
      "\u001b[32m2025-06-17 19:20:09\u001b[0m [] \u001b[1;30mINFO\u001b[0m bm_init.py:994 🔄 Auto-initializing remote storage: gs://prosocial-dev/data/osb/chromadb\n",
      "\u001b[32m2025-06-17 19:20:09\u001b[0m [] \u001b[1;30mINFO\u001b[0m vector.py:438 ⏰ Local cache is 3.4 hours old, checking for updates...\n",
      "\u001b[32m2025-06-17 19:20:09\u001b[0m [] \u001b[1;30mINFO\u001b[0m vector.py:441 🔄 Syncing remote ChromaDB: gs://prosocial-dev/data/osb/chromadb\n",
      "\u001b[32m2025-06-17 19:20:09\u001b[0m [] \u001b[1;30mINFO\u001b[0m vector.py:397 ✅ ChromaDB cache ready at: /home/debian/.cache/buttermilk/chromadb/gs___prosocial-dev_data_osb_chromadb\n",
      "\u001b[32m2025-06-17 19:20:09\u001b[0m [] \u001b[1;30mINFO\u001b[0m vector.py:758 📖 Found existing collection 'osb_fulltext'\n",
      "\u001b[32m2025-06-17 19:20:09\u001b[0m [] \u001b[1;30mINFO\u001b[0m vector.py:778 ✅ Collection 'osb_fulltext' ready (7270 embeddings)\n",
      "\u001b[32m2025-06-17 19:20:09\u001b[0m [] \u001b[1;30mINFO\u001b[0m bm_init.py:996 ✅ Storage ready for use\n",
      "\u001b[32m2025-06-17 19:20:09\u001b[0m [] \u001b[1;30mINFO\u001b[0m vector.py:231 Initialized RecursiveCharacterTextSplitter (chunk_size=1200, chunk_overlap=400)\n"
     ]
    }
   ],
   "source": [
    "# Now we can use the clean BM API for all storage types\n",
    "source = bm.get_storage(cfg.storage.osb_json)\n",
    "\n",
    "# ✨ NEW: Auto-initialized storage (recommended for ChromaDB with remote storage)\n",
    "vectorstore = await bm.get_storage_async(cfg.storage.osb_vector)\n",
    "\n",
    "\n",
    "# Create text splitter\n",
    "chunker = DefaultTextSplitter(chunk_size=1200, chunk_overlap=400)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">📥 Loading live OSB data from GCS<span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "📥 Loading live OSB data from GCS\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">🔗 Data source: gs:<span style=\"color: #800080; text-decoration-color: #800080\">//prosocial-public/osb/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">03_osb_fulltext_summaries.json</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "🔗 Data source: gs:\u001b[35m/\u001b[0m\u001b[35m/prosocial-public/osb/\u001b[0m\u001b[95m03_osb_fulltext_summaries.json\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">📚 Loading all documents from live dataset<span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "📚 Loading all documents from live dataset\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-06-17 19:20:27\u001b[0m [] \u001b[1;30mWARNING\u001b[0m file.py:298 \u001b[33mError converting data to Record at index 38: 2 validation errors for Record\n",
      "content.str\n",
      "  Input should be a valid string [type=string_type, input_value=None, input_type=NoneType]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/string_type\n",
      "content.json-or-python[json=list[union[str,is-instance[Image]]],python=chain[is-instance[Sequence],function-wrap[sequence_validator()]]]\n",
      "  Input should be an instance of Sequence [type=is_instance_of, input_value=None, input_type=NoneType]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/is_instance_of\u001b[0m\n",
      "\u001b[32m2025-06-17 19:20:27\u001b[0m [] \u001b[1;30mWARNING\u001b[0m file.py:298 \u001b[33mError converting data to Record at index 95: 2 validation errors for Record\n",
      "content.str\n",
      "  Input should be a valid string [type=string_type, input_value=None, input_type=NoneType]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/string_type\n",
      "content.json-or-python[json=list[union[str,is-instance[Image]]],python=chain[is-instance[Sequence],function-wrap[sequence_validator()]]]\n",
      "  Input should be an instance of Sequence [type=is_instance_of, input_value=None, input_type=NoneType]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/is_instance_of\u001b[0m\n",
      "\u001b[32m2025-06-17 19:20:27\u001b[0m [] \u001b[1;30mWARNING\u001b[0m file.py:298 \u001b[33mError converting data to Record at index 110: 2 validation errors for Record\n",
      "content.str\n",
      "  Input should be a valid string [type=string_type, input_value=None, input_type=NoneType]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/string_type\n",
      "content.json-or-python[json=list[union[str,is-instance[Image]]],python=chain[is-instance[Sequence],function-wrap[sequence_validator()]]]\n",
      "  Input should be an instance of Sequence [type=is_instance_of, input_value=None, input_type=NoneType]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/is_instance_of\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "✅ Loaded <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">172</span> live OSB documents for vector processing\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "✅ Loaded \u001b[1;36m172\u001b[0m live OSB documents for vector processing\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load live OSB data from GCS\n",
    "print(\"📥 Loading live OSB data from GCS...\")\n",
    "\n",
    "print(f\"🔗 Data source: {source.path}\")\n",
    "\n",
    "# Load documents (limit for demo, remove limit for full production run)\n",
    "records = []\n",
    "doc_limit = None  # Set to None for full dataset\n",
    "\n",
    "print(f\"📚 Loading {doc_limit or 'all'} documents from live dataset...\")\n",
    "\n",
    "for record in source:\n",
    "    # Enhanced Record already has all needed capabilities - no conversion needed!\n",
    "    # The content field is what gets processed for vectors via text_content property\n",
    "    records.append(record)\n",
    "\n",
    "    if doc_limit and len(records) >= doc_limit:\n",
    "        break\n",
    "\n",
    "\n",
    "print(f\"\\n✅ Loaded {len(records)} live OSB documents for vector processing\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration-Driven Multi-Field Vector Store\n",
    "\n",
    "This notebook demonstrates a **configuration-driven approach** for multi-field vector embeddings that works across any data source.\n",
    "\n",
    "### 🧠 **The Problem**\n",
    "Traditional vector stores only embed the main content, leaving rich metadata unsearchable:\n",
    "```python\n",
    "# Traditional approach - metadata trapped\n",
    "record.content = \"Long text...\"        # → Gets embedded ✅\n",
    "record.metadata.summary = \"Key points\"  # → Not searchable ❌\n",
    "```\n",
    "\n",
    "### 🎯 **Our Solution: Enhanced Record with Configuration-Driven Multi-Field Embeddings**\n",
    "The enhanced Record class provides direct vector processing capabilities:\n",
    "```yaml\n",
    "# conf/storage/osb.yaml\n",
    "osb_vector:\n",
    "  type: chromadb\n",
    "  # ... basic config\n",
    "  multi_field_embedding:\n",
    "    content_field: \"content\"\n",
    "    additional_fields:\n",
    "      - source_field: \"summary\"\n",
    "        chunk_type: \"summary\"\n",
    "        min_length: 50\n",
    "      - source_field: \"title\"\n",
    "        chunk_type: \"title\"\n",
    "        min_length: 10\n",
    "```\n",
    "\n",
    "### 🔍 **Search Capabilities**\n",
    "\n",
    "| Search Type | Use Case | Example Query |\n",
    "|-------------|----------|---------------|\n",
    "| **Summary-Only** | High-level concepts | `where={\"content_type\": \"summary\"}` |\n",
    "| **Title-Only** | Topic matching | `where={\"content_type\": \"title\"}` |\n",
    "| **Content-Only** | Detailed analysis | `where={\"content_type\": \"content\"}` |\n",
    "| **Cross-Field** | Comprehensive search | No filter = search everything |\n",
    "| **Hybrid** | Semantic + exact match | `query + where={\"case_number\": \"2024\"}` |\n",
    "\n",
    "### 🏗️ **Benefits**\n",
    "- ✅ **Enhanced Record**: Direct vector capabilities built into Record class\n",
    "- ✅ **Configuration-Driven**: No hardcoded field names\n",
    "- ✅ **Data Source Agnostic**: Works with any Record structure\n",
    "- ✅ **Same Config**: Creation and reading use identical configuration\n",
    "- ✅ **Extensible**: Easy to add new field types for any dataset"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "async def create_production_vector_store():\n    \"\"\"Production pipeline: Process live OSB data with intelligent batched sync.\"\"\"\n\n    print(\"🏭 Starting production vector store with INTELLIGENT SYNC...\")\n    print(f\"📊 Processing {len(records)} live OSB documents\")\n\n    # Configure sync behavior (can be done via config too)\n    vectorstore.sync_batch_size = 50  # Sync every 10 records for demo (default: 50)\n    vectorstore.sync_interval_minutes = 10  # Sync every 2 minutes for demo (default: 10)\n\n    print(f\"⚙️  Sync Configuration:\")\n    print(f\"   📦 Sync every {vectorstore.sync_batch_size} records\")\n    print(f\"   ⏰ Sync every {vectorstore.sync_interval_minutes} minutes\")\n    print(f\"   🔄 Auto-sync: {not vectorstore.disable_auto_sync}\")\n\n    successful_embeddings = 0\n    failed_embeddings = 0\n    total_chunks = 0\n    sync_count = 0\n\n    for i, record in enumerate(records):\n        print(f\"🔄 Processing record {i+1}/{len(records)}: {record.record_id[:8]}...\")\n\n        try:\n            # NEW API: Use enhanced process_record with ProcessingResult\n            result = await vectorstore.process_record(record, skip_existing=True, validate_before_process=True)\n            \n            if result.status == \"processed\":\n                successful_embeddings += 1\n                chunk_count = result.chunks_created  # Use chunks_created from ProcessingResult\n                total_chunks += chunk_count\n\n                # Check if sync happened (logged by the vectorstore)\n                if vectorstore._processed_records_count == 0:  # Counter resets after sync\n                    sync_count += 1\n\n                print(f\"   ✅ Processed: {chunk_count} chunks ({result.processing_time_ms:.1f}ms)\")\n                \n            elif result.status == \"skipped\":\n                print(f\"   ⏭️  Skipped: {result.reason}\")\n                \n            else:  # failed\n                failed_embeddings += 1\n                print(f\"   ❌ Failed: {result.reason}\")\n\n        except Exception as e:\n            failed_embeddings += 1\n            print(f\"   ❌ Error processing record: {e}\")\n\n    # Final sync to ensure all changes are persisted\n    print(f\"\\n🔄 Performing final sync...\")\n    final_sync_success = await vectorstore.finalize_processing()\n    if final_sync_success:\n        sync_count += 1\n\n    # Final results\n    final_count = vectorstore.collection.count()\n\n    print(f\"\\n🎉 INTELLIGENT SYNC Vector Store Created!\")\n    print(f\"   📊 Records processed: {successful_embeddings + failed_embeddings}\")\n    print(f\"   ✅ Successfully embedded: {successful_embeddings}\")\n    print(f\"   ❌ Failed: {failed_embeddings}\")\n    print(f\"   📦 Total chunks: {total_chunks}\")\n    print(f\"   🔢 Total embeddings in collection: {final_count}\")\n\n\nresults = await create_production_vector_store()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🚀 Intelligent Sync System - Major Performance Improvement\n",
    "\n",
    "### **Problem Solved: Excessive Sync Operations**\n",
    "\n",
    "**Before:** The system was syncing to GCS after **every single record**, which was extremely slow:\n",
    "```python\n",
    "# Old approach - SLOW! 💀\n",
    "for record in records:\n",
    "    await vectorstore.process_record(record)\n",
    "    await sync_to_gcs()  # ← This happened 1000x for 1000 records!\n",
    "```\n",
    "\n",
    "**After:** Smart batched sync with configurable thresholds:\n",
    "```python\n",
    "# New approach - FAST! ⚡\n",
    "for record in records:\n",
    "    await vectorstore.process_record(record)\n",
    "    # Only syncs when batch size reached OR time threshold met\n",
    "```\n",
    "\n",
    "### **🧠 Smart Sync Logic**\n",
    "\n",
    "The system now syncs intelligently based on:\n",
    "\n",
    "| Trigger | Default | Configurable | Purpose |\n",
    "|---------|---------|--------------|---------|\n",
    "| **Batch Size** | 50 records | `sync_batch_size` | Prevent data loss |\n",
    "| **Time Interval** | 10 minutes | `sync_interval_minutes` | Ensure periodic saves |\n",
    "| **Final Sync** | Always | `finalize_processing()` | Guarantee data persistence |\n",
    "| **Manual Sync** | On-demand | `sync_to_remote(force=True)` | User control |\n",
    "\n",
    "### **⚙️ Configuration Options**\n",
    "\n",
    "```yaml\n",
    "# conf/storage/osb.yaml\n",
    "osb_vector:\n",
    "  type: chromadb\n",
    "  # ... other config\n",
    "  sync_batch_size: 50          # Sync every 50 records\n",
    "  sync_interval_minutes: 10    # Sync every 10 minutes  \n",
    "  disable_auto_sync: false     # Enable/disable auto-sync\n",
    "```\n",
    "\n",
    "### **📈 Performance Benefits**\n",
    "\n",
    "For **1000 records**:\n",
    "- **Old System**: 1000 sync operations (~16 minutes of sync overhead)\n",
    "- **New System**: ~20 sync operations (~20 seconds of sync overhead)\n",
    "- **Improvement**: **98% reduction** in sync operations = **48x faster**\n",
    "\n",
    "### **🔒 Data Safety**\n",
    "\n",
    "The intelligent sync system maintains data safety through:\n",
    "- ✅ **Batch Thresholds**: Never lose more than `sync_batch_size` records\n",
    "- ✅ **Time Limits**: Automatic sync every `sync_interval_minutes`\n",
    "- ✅ **Final Guarantee**: `finalize_processing()` ensures no data loss\n",
    "- ✅ **Error Handling**: Failed syncs are logged and retried\n",
    "- ✅ **Manual Override**: Force sync anytime with `sync_to_remote(force=True)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test configuration-driven multi-field search capabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🔍 Testing Configuration-Driven Multi-Field Search...\")\n",
    "\n",
    "# The content_type values come from our configuration:\n",
    "# - \"content\" (main content field)\n",
    "# - \"summary\" (from additional_fields config)\n",
    "# - \"title\" (from additional_fields config)\n",
    "\n",
    "# 1. Search summaries only (high-level concepts)\n",
    "print(\"\\n🎯 1. SUMMARY-ONLY SEARCH:\")\n",
    "print(\"   Query: 'human rights'\")\n",
    "summary_results = vectorstore.collection.query(\n",
    "    query_texts=[\"human rights\"],\n",
    "    # where={\"content_type\": \"summary\"},  # Based on config: source_field=\"summary\"\n",
    "    n_results=3,\n",
    "    include=[\"documents\", \"metadatas\", \"distances\"],\n",
    ")\n",
    "\n",
    "if summary_results[\"ids\"] and summary_results[\"ids\"][0]:\n",
    "    for i, (doc, metadata, distance) in enumerate(\n",
    "        zip(summary_results[\"documents\"][0], summary_results[\"metadatas\"][0], summary_results[\"distances\"][0])\n",
    "    ):\n",
    "        similarity = 1 - distance\n",
    "        title = metadata.get(\"title\", \"Untitled\")\n",
    "        print(f\"   📋 Result {i+1}: {title[:40]}... (similarity: {similarity:.3f})\")\n",
    "        print(f\"      📝 Summary: {doc[:80]}...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create data source configuration for the RAG agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Enhanced RAG Agent with intelligent search capabilities\n",
    "from buttermilk._core.config import (  # Configuration models\n",
    "    AgentVariants,\n",
    ")\n",
    "\n",
    "# IMPORTANT: Use the SAME config as your vectorstore to avoid mismatches!\n",
    "cfg.agents.researcher.data = {\"osb_vector\": cfg.storage.osb_vector}\n",
    "\n",
    "rag_variants = AgentVariants(**cfg.agents.researcher)\n",
    "agents = []\n",
    "for agent_cls, variant_config in rag_variants.get_configs():\n",
    "    print(f\"🔍 Initializing agent: {agent_cls.__name__} with config: {variant_config}\")\n",
    "    agent = agent_cls(\n",
    "        config=variant_config,\n",
    "        storage=vectorstore,\n",
    "        text_splitter=chunker,\n",
    "    )\n",
    "    agents.append(agent)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use agents to answer questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "async def search_osb_database(queries):\n",
    "    \"\"\"Search the OSB database with multiple queries.\"\"\"\n",
    "    print(\"\\n=== OSB Database Search Results ===\")\n",
    "\n",
    "    results = await rag_agent(queries)\n",
    "\n",
    "    for i, (query, result) in enumerate(zip(queries, results)):\n",
    "        print(f\"\\n--- Query {i+1}: {query} ---\")\n",
    "        print(f\"Found {len(result.results)} relevant chunks\")\n",
    "\n",
    "        if result.results:\n",
    "            # Show the top result\n",
    "            top_result = result.results[0]\n",
    "            print(f\"\\nTop Result:\")\n",
    "            print(f\"Document: {top_result.document_title}\")\n",
    "            print(f\"Case Number: {top_result.metadata.get('case_number', 'N/A')}\")\n",
    "            print(f\"Text: {top_result.full_text[:300]}...\")\n",
    "        else:\n",
    "            print(\"No results found\")\n",
    "\n",
    "\n",
    "# Example search queries\n",
    "search_queries = [\n",
    "    \"What are the challenges with automated content moderation?\",\n",
    "    \"How effective are age verification systems?\",\n",
    "    \"What techniques are used to spread misinformation?\",\n",
    "]\n",
    "\n",
    "rag_agent = random.choice(agents)  # Randomly select one agent for this demo\n",
    "await search_osb_database(search_queries)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def demonstrate_enhanced_rag():\n",
    "    \"\"\"Demonstrate Enhanced RAG capabilities with intelligent search planning.\"\"\"\n",
    "\n",
    "    print(\"🎯 ENHANCED RAG DEMONSTRATION\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Test queries that showcase different capabilities\n",
    "    test_queries = [\n",
    "        {\n",
    "            \"query\": \"What are the main challenges with content moderation?\",\n",
    "            \"expected_strategy\": \"Should use hybrid search (title + summary + content)\",\n",
    "            \"focus\": \"Broad exploratory query\",\n",
    "        },\n",
    "        {\n",
    "            \"query\": \"Find cases about misinformation detection algorithms\",\n",
    "            \"expected_strategy\": \"Should use metadata + title search\",\n",
    "            \"focus\": \"Specific case-focused query\",\n",
    "        },\n",
    "        {\n",
    "            \"query\": \"How do platforms protect user privacy?\",\n",
    "            \"expected_strategy\": \"Should use summary + semantic search\",\n",
    "            \"focus\": \"Policy-focused query\",\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    for i, test in enumerate(test_queries, 1):\n",
    "        print(f\"\\n🔍 TEST {i}: {test['focus']}\")\n",
    "        print(f\"Query: '{test['query']}'\")\n",
    "        print(f\"Expected: {test['expected_strategy']}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "        try:\n",
    "            # Create AgentInput for the enhanced RAG agent\n",
    "            from buttermilk._core.contract import AgentInput\n",
    "\n",
    "            agent_input = AgentInput(inputs={\"query\": test[\"query\"]}, parameters={}, context=[], records=[])\n",
    "\n",
    "            # Process with Enhanced RAG\n",
    "            result = await enhanced_rag_agent._process(message=agent_input)\n",
    "\n",
    "            print(f\"✅ RESULT:\")\n",
    "            print(f\"   Response: {result.outputs[:200]}...\")\n",
    "\n",
    "            # Show metadata about the search\n",
    "            metadata = result.metadata\n",
    "            print(f\"\\n📊 SEARCH METADATA:\")\n",
    "            print(f\"   Total Results: {metadata.get('total_results', 0)}\")\n",
    "            print(f\"   Strategies Used: {metadata.get('strategies_used', [])}\")\n",
    "            print(f\"   Confidence Score: {metadata.get('confidence_score', 0.0):.2f}\")\n",
    "            print(f\"   Key Themes: {metadata.get('key_themes', [])}\")\n",
    "\n",
    "            if metadata.get(\"search_explanation\"):\n",
    "                print(f\"   Search Strategy: {metadata['search_explanation']}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ ERROR: {e}\")\n",
    "\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "    print(\"\\n🎉 Enhanced RAG demonstration complete!\")\n",
    "    print(\"\\nKey Benefits Demonstrated:\")\n",
    "    print(\"✅ Intelligent query analysis and search planning\")\n",
    "    print(\"✅ Multi-field search across titles, summaries, and content\")\n",
    "    print(\"✅ LLM-driven result synthesis and ranking\")\n",
    "    print(\"✅ Adaptive search strategies based on query type\")\n",
    "    print(\"✅ Comprehensive metadata and confidence scoring\")\n",
    "    print(\"✅ Smart cache management prevents overwriting local changes\")\n",
    "    print(\"✅ Automatic sync-back to remote storage after embedding operations\")\n",
    "\n",
    "\n",
    "# Run the enhanced RAG demonstration\n",
    "await demonstrate_enhanced_rag()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Interactive Chat Interface\n",
    "\n",
    "Now let's create an interactive interface to chat with our OSB knowledge base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def chat_with_osb(user_question):\n",
    "    \"\"\"Interactive chat with OSB knowledge base.\"\"\"\n",
    "    print(f\"\\n🔍 User Question: {user_question}\")\n",
    "\n",
    "    # Search for relevant context\n",
    "    search_results = await rag_agent.fetch([user_question])\n",
    "\n",
    "    if search_results and search_results[0].results:\n",
    "        context = search_results[0]\n",
    "        print(f\"\\n📚 Found {len(context.results)} relevant documents\")\n",
    "\n",
    "        # Display relevant chunks\n",
    "        print(\"\\n📋 Relevant Information:\")\n",
    "        for i, result in enumerate(context.results[:3]):  # Show top 3\n",
    "            print(f\"\\n{i+1}. {result.document_title} ({result.metadata.get('case_number', 'N/A')})\")\n",
    "            print(f\"   {result.full_text[:200]}...\")\n",
    "\n",
    "        # In a real implementation, this would be sent to an LLM for synthesis\n",
    "        print(\"\\n🤖 AI Response: [In a real implementation, the retrieved context would be sent to an LLM to generate a synthesized response]\")\n",
    "    else:\n",
    "        print(\"\\n❌ No relevant information found in the OSB database\")\n",
    "\n",
    "\n",
    "# Example chat interactions\n",
    "example_questions = [\n",
    "    \"What are the main issues with current content moderation approaches?\",\n",
    "    \"What recommendations exist for age verification?\",\n",
    "    \"How do platforms detect and counter misinformation?\",\n",
    "]\n",
    "\n",
    "for question in example_questions:\n",
    "    await chat_with_osb(question)\n",
    "    print(\"\\n\" + \"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Vector Store Analysis\n",
    "\n",
    "Let's analyze our vector store to understand what we've created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get collection statistics\n",
    "collection = vectorstore.collection\n",
    "count = collection.count()\n",
    "\n",
    "print(f\"\\n=== OSB Vector Store Statistics ===\")\n",
    "print(f\"Collection Name: {vectorstore.collection_name}\")\n",
    "print(f\"Total Chunks: {count}\")\n",
    "print(f\"Embedding Dimensions: {vectorstore.dimensionality}\")\n",
    "print(f\"Embedding Model: {vectorstore.embedding_model}\")\n",
    "\n",
    "# Get a sample of metadata to understand the structure\n",
    "sample_results = collection.get(limit=3, include=[\"metadatas\", \"documents\"])\n",
    "\n",
    "print(f\"\\n=== Sample Metadata Structure ===\")\n",
    "if sample_results[\"metadatas\"]:\n",
    "    sample_metadata = sample_results[\"metadatas\"][0]\n",
    "    print(\"Available metadata fields:\")\n",
    "    for key, value in sample_metadata.items():\n",
    "        print(f\"  - {key}: {type(value).__name__} = {str(value)[:50]}...\")\n",
    "\n",
    "print(f\"\\n=== Storage Locations ===\")\n",
    "print(f\"ChromaDB Directory: {vectorstore.persist_directory}\")\n",
    "print(f\"Embeddings Directory: {vectorstore.arrow_save_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Advanced Search Examples\n",
    "\n",
    "Let's explore some advanced search patterns and filtering capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Direct ChromaDB queries with metadata filtering\n",
    "async def advanced_search_examples():\n",
    "    \"\"\"Demonstrate advanced search capabilities.\"\"\"\n",
    "    print(\"\\n=== Advanced Search Examples ===\")\n",
    "\n",
    "    # 1. Search with metadata filtering\n",
    "    print(\"\\n1. Search within specific case:\")\n",
    "    results = collection.query(\n",
    "        query_texts=[\"content moderation challenges\"], n_results=5, where={\"case_number\": \"OSB-2024-001\"}, include=[\"documents\", \"metadatas\"]\n",
    "    )\n",
    "    print(f\"   Found {len(results['ids'][0]) if results['ids'] else 0} results in OSB-2024-001\")\n",
    "\n",
    "    # 2. Similarity search across all documents\n",
    "    print(\"\\n2. General similarity search:\")\n",
    "    results = collection.query(query_texts=[\"artificial intelligence and safety\"], n_results=5, include=[\"documents\", \"metadatas\", \"distances\"])\n",
    "\n",
    "    if results[\"ids\"] and results[\"ids\"][0]:\n",
    "        print(f\"   Found {len(results['ids'][0])} results\")\n",
    "        for i, (doc, metadata, distance) in enumerate(zip(results[\"documents\"][0][:3], results[\"metadatas\"][0][:3], results[\"distances\"][0][:3])):\n",
    "            print(f\"   Result {i+1} (similarity: {1-distance:.3f}): {metadata.get('title', 'N/A')}\")\n",
    "            print(f\"     {doc[:100]}...\")\n",
    "\n",
    "    # 3. Multi-query search\n",
    "    print(\"\\n3. Multi-query search:\")\n",
    "    multi_queries = [\"platform safety measures\", \"user protection mechanisms\", \"digital safety standards\"]\n",
    "\n",
    "    for query in multi_queries:\n",
    "        results = collection.query(query_texts=[query], n_results=2, include=[\"metadatas\"])\n",
    "        count = len(results[\"ids\"][0]) if results[\"ids\"] else 0\n",
    "        print(f\"   '{query}': {count} results\")\n",
    "\n",
    "\n",
    "await advanced_search_examples()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Production Considerations\n",
    "\n",
    "Here are key considerations for using this in production:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    \"\"\"\n",
    "=== Production Deployment Checklist ===\n",
    "\n",
    "🔧 Configuration:\n",
    "   ✓ Use GCS for persist_directory: gs://your-bucket/chromadb\n",
    "   ✓ Configure appropriate chunk_size for your content\n",
    "   ✓ Set concurrency based on your compute resources\n",
    "   ✓ Use production embedding models (text-embedding-004/005)\n",
    "\n",
    "📊 Performance:\n",
    "   ✓ Monitor embedding generation costs\n",
    "   ✓ Implement caching for frequently accessed data\n",
    "   ✓ Use batch processing for large datasets\n",
    "   ✓ Configure appropriate timeout values\n",
    "\n",
    "🔒 Security:\n",
    "   ✓ Secure GCS bucket access with proper IAM\n",
    "   ✓ Implement data access controls\n",
    "   ✓ Audit vector store queries\n",
    "   ✓ Protect sensitive metadata\n",
    "\n",
    "🚀 Scalability:\n",
    "   ✓ Plan for vector store size growth\n",
    "   ✓ Implement horizontal scaling for embeddings\n",
    "   ✓ Monitor query performance\n",
    "   ✓ Set up proper logging and monitoring\n",
    "\n",
    "🔄 Maintenance:\n",
    "   ✓ Plan for data updates and reindexing\n",
    "   ✓ Implement backup strategies\n",
    "   ✓ Version control for embeddings and metadata\n",
    "   ✓ Regular quality assessments\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# Show next steps\n",
    "print(\n",
    "    \"\"\"\n",
    "=== Next Steps ===\n",
    "\n",
    "1. Scale to Full Dataset:\n",
    "   - Use the osb_vectorize.yaml configuration\n",
    "   - Run: uv run python -m buttermilk.data.vector +run=osb_vectorize\n",
    "\n",
    "2. Deploy RAG Flow:\n",
    "   - Use the osb_rag.yaml flow configuration\n",
    "   - Run: uv run python -m buttermilk.runner.cli +flow=osb_rag +run=api\n",
    "\n",
    "3. Integrate with Frontend:\n",
    "   - Use the Buttermilk web interface\n",
    "   - Connect to WebSocket endpoints for real-time chat\n",
    "\n",
    "4. Monitor and Optimize:\n",
    "   - Track query performance\n",
    "   - Monitor embedding costs\n",
    "   - Tune chunk sizes and retrieval parameters\n",
    "\"\"\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔒 Smart Cache Management\n",
    "\n",
    "The vector database now includes smart cache management to prevent overwriting local changes:\n",
    "\n",
    "### **Problem Solved**\n",
    "Previously, re-running embedding cells would download the remote ChromaDB cache and overwrite any local changes, losing newly added embeddings.\n",
    "\n",
    "### **Solution: Smart Cache Management**\n",
    "The system now includes intelligent cache handling:\n",
    "\n",
    "```python\n",
    "async def _smart_cache_management(self, remote_path: str) -> Path:\n",
    "    \"\"\"Smart cache management that prevents overwriting newer local changes.\"\"\"\n",
    "    \n",
    "    # Check if local cache was recently modified (within 1 hour)\n",
    "    if time_since_modified < 3600:  # 1 hour\n",
    "        logger.info(\"🔒 Skipping download to preserve local changes\")\n",
    "        return cache_path\n",
    "    \n",
    "    # Only download if cache is stale\n",
    "    logger.info(\"🔄 Syncing remote ChromaDB\")\n",
    "    return await ensure_chromadb_cache(remote_path)\n",
    "```\n",
    "\n",
    "### **Automatic Sync-Back**\n",
    "After successful embedding operations, local changes are automatically synced to remote storage:\n",
    "\n",
    "```python\n",
    "async def _sync_local_changes_to_remote(self) -> None:\n",
    "    \"\"\"Sync local ChromaDB changes back to remote storage.\"\"\"\n",
    "    \n",
    "    # Only sync if recently modified (indicates recent work)\n",
    "    if time_since_modified < 21600:  # 6 hours\n",
    "        await upload_chromadb_cache(local_path, remote_path)\n",
    "        logger.info(\"✅ Successfully synced local changes to remote storage\")\n",
    "```\n",
    "\n",
    "### **Benefits**\n",
    "- ✅ **Prevents Data Loss**: Local embedding work is preserved\n",
    "- ✅ **Automatic Sync**: Changes are pushed back to remote storage  \n",
    "- ✅ **Time-Based Logic**: Only acts on recently modified caches\n",
    "- ✅ **Transparent Operation**: Clear logging of all cache decisions\n",
    "- ✅ **Production Ready**: Handles concurrent access and failures gracefully\n",
    "\n",
    "### **Usage**\n",
    "This happens automatically - no code changes needed! The smart cache management activates whenever you:\n",
    "1. Run embedding operations in this notebook\n",
    "2. Use the vectorstore in production flows\n",
    "3. Process new documents with the vector pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🚀 Production Deployment Guide\n",
    "\n",
    "This vector store is now ready for production use with the unified storage system. Here's how to deploy and use it:\n",
    "\n",
    "### 📋 **For Full Dataset Processing**\n",
    "```python\n",
    "# In cell 7, change this line:\n",
    "doc_limit = 5  # Set to None for full dataset\n",
    "\n",
    "# To:\n",
    "doc_limit = None  # Processes all OSB documents\n",
    "```\n",
    "\n",
    "### 🏭 **Production Usage Examples**\n",
    "\n",
    "#### **Option 1: RAG Agent Integration**\n",
    "```python\n",
    "from buttermilk.agents.rag.rag_agent import RagAgent\n",
    "from buttermilk._core.config import AgentConfig\n",
    "from buttermilk._core.storage_config import StorageConfig\n",
    "\n",
    "# Same config as creation - no changes needed with unified storage!\n",
    "storage_config = StorageConfig(**cfg.storage.osb_vector)\n",
    "\n",
    "agent_config = AgentConfig(\n",
    "    role=\"RESEARCHER\",\n",
    "    agent_obj=\"RagAgent\", \n",
    "    description=\"OSB Knowledge Assistant\",\n",
    "    data={\"osb_vector\": storage_config},\n",
    "    parameters={\"n_results\": 10, \"max_queries\": 3}\n",
    ")\n",
    "\n",
    "rag_agent = RagAgent(**agent_config.model_dump())\n",
    "```\n",
    "\n",
    "#### **Option 2: Direct Storage Access**\n",
    "```python\n",
    "# Create vector store instance (reads existing embeddings) using unified storage\n",
    "production_vectorstore = bm.get_storage(cfg.storage.osb_vector)\n",
    "await production_vectorstore.ensure_cache_initialized()\n",
    "\n",
    "# Perform semantic search\n",
    "results = production_vectorstore.collection.query(\n",
    "    query_texts=[\"platform safety policies\"],\n",
    "    n_results=5\n",
    ")\n",
    "```\n",
    "\n",
    "#### **Option 3: Flow Integration**\n",
    "```yaml\n",
    "# conf/flows/osb_rag.yaml\n",
    "defaults:\n",
    "  - base_flow\n",
    "\n",
    "orchestrator: buttermilk.orchestrators.groupchat.AutogenOrchestrator\n",
    "storage: osb_vector  # References the same storage config\n",
    "agents: [rag_agent, host/sequencer]\n",
    "```\n",
    "\n",
    "### 🏗️ **Enhanced Record Benefits**\n",
    "- ✅ **Direct Processing**: Records processed without conversion steps\n",
    "- ✅ **Vector Fields**: Built-in support for chunks, embeddings, file_path\n",
    "- ✅ **Unified API**: Same Record class used throughout the system\n",
    "- ✅ **Type Safety**: Full Pydantic validation for vector operations\n",
    "\n",
    "### 🔒 **Production Considerations**\n",
    "- ✅ **Persistent Storage**: Vector store saved to `gs://prosocial-public/osb/chromadb`  \n",
    "- ✅ **Config Reuse**: Same `osb.yaml` works for both creation and reading\n",
    "- ✅ **Scalability**: ChromaDB handles thousands of documents efficiently\n",
    "- ✅ **Monitoring**: Check collection count and performance metrics\n",
    "- ✅ **Updates**: Re-run this notebook to add new OSB documents\n",
    "\n",
    "### 💡 **Next Steps**\n",
    "1. **Scale Up**: Remove `doc_limit` to process full OSB dataset\n",
    "2. **Deploy**: Use in RAG agents, search APIs, or analytical workflows  \n",
    "3. **Monitor**: Track embedding quality and search relevance\n",
    "4. **Iterate**: Add new documents by re-running the pipeline\n",
    "\n",
    "### 🔧 **Migration Benefits**\n",
    "This notebook now uses:\n",
    "- ✅ **StorageConfig**: Unified configuration for all storage types\n",
    "- ✅ **Enhanced Record**: Built-in vector processing capabilities  \n",
    "- ✅ **bm.get_storage()**: Unified storage access API\n",
    "- ✅ **process_record()**: Direct Record processing without conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kq161px7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's test the text splitter behavior with a sample text\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Create a test text that should definitely be split\n",
    "test_text = \"This is a test document. \" * 100  # 2500 characters\n",
    "print(f\"Test text length: {len(test_text)} characters\")\n",
    "\n",
    "# Test with the same config as OSB\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1200,\n",
    "    chunk_overlap=400,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    "    add_start_index=False,\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_text(test_text)\n",
    "print(f\"Number of chunks created: {len(chunks)}\")\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"Chunk {i+1}: {len(chunk)} characters\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1959yzujxs8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at a sample of the OSB data to understand the actual field structure\n",
    "import json\n",
    "import requests\n",
    "\n",
    "# Let's fetch a small sample of the OSB data to see the actual structure\n",
    "try:\n",
    "    # For security, I'll create a simple test to understand the field structure\n",
    "    # Based on the config, it seems like the JSON has:\n",
    "    # - \"id\" field (maps to record_id)\n",
    "    # - \"full_text\" field (maps to content)\n",
    "    # - \"title\", \"case_number\", \"url\", \"summary\" fields (go to metadata)\n",
    "\n",
    "    print(\"Based on your config, the OSB JSON structure should be:\")\n",
    "    print(\n",
    "        \"\"\"\n",
    "    {\n",
    "        \"id\": \"some-id\",\n",
    "        \"full_text\": \"The main content text that should be chunked\",\n",
    "        \"title\": \"Document title\", \n",
    "        \"case_number\": \"OSB-2024-001\",\n",
    "        \"url\": \"https://...\",\n",
    "        \"summary\": \"Document summary\"\n",
    "    }\n",
    "    \"\"\"\n",
    "    )\n",
    "\n",
    "    print(\"\\nYour current mapping:\")\n",
    "    print(\"- JSON 'id' → Record 'record_id'\")\n",
    "    print(\"- JSON 'full_text' → Record 'content'\")\n",
    "    print(\"- JSON 'title' → Record metadata['title']\")\n",
    "    print(\"- JSON 'summary' → Record metadata['summary']\")\n",
    "    print(\"- JSON 'case_number' → Record metadata['case_number']\")\n",
    "    print(\"- JSON 'url' → Record metadata['url']\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a60qp1mf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's quickly test what happens when we load OSB data with the current config\n",
    "from buttermilk._core.types import Record\n",
    "\n",
    "# Test creating a record like OSB would\n",
    "test_record = Record(\n",
    "    record_id=\"test-123\",\n",
    "    content=\"This is the main content from fulltext field\",\n",
    "    metadata={\"title\": \"Test Document\", \"summary\": \"Test summary\", \"case_number\": \"OSB-2024-001\", \"url\": \"https://example.com\"},\n",
    ")\n",
    "\n",
    "print(\"🔍 Record Fields:\")\n",
    "print(f\"content: {test_record.content[:50]}...\")\n",
    "print(f\"text_content: {test_record.text_content[:50]}...\")\n",
    "print(f\"metadata keys: {list(test_record.metadata.keys())}\")\n",
    "print(f\"metadata: {test_record.metadata}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 🎉 Enhanced Vector Database Summary\n\n### ✅ What You Just Saw\n\n1. **🔄 Smart Deduplication**: The system automatically detected and skipped existing records, preventing duplicate embeddings\n\n2. **📊 Comprehensive Results**: Every operation returns detailed `ProcessingResult` or `BatchProcessingResult` with status, timing, and metadata\n\n3. **🔍 Pre-Validation**: Batch operations validate all records before processing, providing early warning of potential issues\n\n4. **📁 BM Integration**: Complete integration with existing Buttermilk logging and run management infrastructure\n\n5. **⚡ Performance**: Enhanced metadata tracking with provenance for every chunk\n\n### 🚀 Production Benefits\n\n#### **Before (Old API)**\n- ❌ No deduplication → wasted compute on existing records\n- ❌ No validation → failures discovered during processing  \n- ❌ Limited error information → difficult debugging\n- ❌ No resume capability → manual tracking required\n\n#### **After (New API)**  \n- ✅ **Smart Skip**: Existing records skipped automatically\n- ✅ **Safe Updates**: Validation prevents data corruption\n- ✅ **Rich Results**: Detailed status and error information\n- ✅ **BM Integration**: Uses existing logging infrastructure\n- ✅ **Resume Capability**: Add new records safely to existing collections\n\n### 🔧 Key Deduplication Strategies\n\n| Strategy | Behavior | Use Case |\n|----------|----------|----------|\n| `\"record_id\"` | Skip if record ID exists | Fast deduplication based on ID only |\n| `\"content_hash\"` | Skip if content unchanged | Detect actual content changes |\n| `\"both\"` | Conservative: skip only if ID exists AND content same | Maximum safety (default) |\n\n### 🏭 Production Usage\n\n```python\n# New production-ready workflow\nbatch_result = await vectorstore.process_batch(\n    new_records,\n    mode=\"safe\",           # Safe incremental updates\n    max_failures=5,        # Allow some failures\n    require_all_new=False  # Mixed new/existing OK\n)\n\n# Check results\nif batch_result.successful_count > 0:\n    print(f\"✅ Added {batch_result.successful_count} new records\")\n    \nif batch_result.skipped_count > 0:\n    print(f\"⏭️  Skipped {batch_result.skipped_count} existing records\")\n\n# Finalize with BM logging integration\nawait vectorstore.finalize_processing()\n```\n\nThe enhanced vector database is now production-ready with comprehensive safety guarantees and efficient resume capability!",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# 5. Demonstrate BM Integration and Finalization\nprint(f\"\\n5️⃣ BM INTEGRATION & FINALIZATION\")\nprint(\"-\" * 40)\n\n# Show BM integration\nprint(f\"📊 Records Processed This Session: {vectorstore._processed_records_count}\")\nprint(f\"🔍 Deduplication Cache Size: {len(vectorstore._processed_combinations_cache)} combinations\")\n\n# Show BM run information if available\ntry:\n    from buttermilk._core.dmrc import get_bm\n    bm = get_bm()\n    if bm and bm.run_info:\n        print(f\"📁 BM Run ID: {bm.run_info.run_id}\")\n        print(f\"💾 BM Save Directory: {bm.run_info.save_dir}\")\n        print(f\"📍 BM Platform: {bm.run_info.platform}\")\n    else:\n        print(f\"⚠️  BM run info not available\")\nexcept Exception as e:\n    print(f\"⚠️  Could not access BM: {e}\")\n\n# Finalize processing (uses existing BM logging)\nprint(f\"\\n🔄 Finalizing processing session...\")\nfinalize_success = await vectorstore.finalize_processing()\n\nif finalize_success:\n    print(f\"✅ Finalization successful!\")\n    print(f\"📊 Final Statistics:\")\n    print(f\"   🔢 Total embeddings in collection: {vectorstore.collection.count()}\")\n    print(f\"   📦 Processed records this session: {vectorstore._processed_records_count}\")\n    print(f\"   🔍 Deduplication strategy: {vectorstore.deduplication_strategy}\")\n    print(f\"   📈 Cache efficiency: {len(vectorstore._processed_combinations_cache)} combinations cached\")\nelse:\n    print(f\"❌ Finalization failed!\")\n\nprint(f\"\\n✅ All processing logged via existing BM infrastructure\")\nprint(f\"💡 Run metadata is automatically saved by Buttermilk to standard locations\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Test batch processing with validation\n",
    "print(f\"\\n3️⃣ BATCH PROCESSING WITH VALIDATION\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Create a mix of new and existing records for testing\n",
    "test_batch = test_records[:3]  # Use first 3 records\n",
    "print(f\"Processing batch of {len(test_batch)} records...\")\n",
    "\n",
    "# NEW API: Batch processing with comprehensive validation\n",
    "batch_result = await vectorstore.process_batch(\n",
    "    test_batch,\n",
    "    mode=\"safe\",  # \"safe\", \"force\", or \"validate_only\"\n",
    "    max_failures=0,  # Fail fast (stop on first failure)\n",
    "    require_all_new=False,  # Don't require all records to be new\n",
    ")\n",
    "\n",
    "print(f\"✅ Batch Results:\")\n",
    "print(f\"   📊 Total Records: {batch_result.total_records}\")\n",
    "print(f\"   ✅ Processed: {batch_result.successful_count}\")\n",
    "print(f\"   ⏭️  Skipped (existing): {batch_result.skipped_count}\")\n",
    "print(f\"   ❌ Failed: {batch_result.failed_count}\")\n",
    "print(f\"   ⏱️  Total Time: {batch_result.processing_time_ms:.1f}ms\")\n",
    "\n",
    "if batch_result.failed_records:\n",
    "    print(f\"   🚫 Failed Records:\")\n",
    "    for record_id, error in batch_result.failed_records:\n",
    "        print(f\"      - {record_id}: {error}\")\n",
    "\n",
    "# Show validation results\n",
    "if batch_result.validation_result:\n",
    "    validation = batch_result.validation_result\n",
    "    print(f\"\\n📋 Validation Summary:\")\n",
    "    print(f\"   🔍 Would Process: {validation['stats']['would_process']}\")\n",
    "    print(f\"   ⏭️  Would Skip: {validation['stats']['would_skip']}\")\n",
    "    print(f\"   ⚠️  Warnings: {len(validation['warnings'])}\")\n",
    "    print(f\"   🚫 Conflicts: {len(validation['conflicts'])}\")\n",
    "\n",
    "# 4. Test validation-only mode\n",
    "print(f\"\\n4️⃣ VALIDATION-ONLY MODE\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Test with a larger batch including some new records\n",
    "larger_batch = test_records[:8]  # Expand to test more records\n",
    "\n",
    "validation_result = await vectorstore.process_batch(larger_batch, mode=\"validate_only\")  # Only validate, don't process\n",
    "\n",
    "print(f\"📋 Validation-Only Results:\")\n",
    "print(f\"   📊 Total Records: {validation_result.total_records}\")\n",
    "print(f\"   🆕 Would Process: {validation_result.validation_result['stats']['would_process']}\")\n",
    "print(f\"   ⏭️  Would Skip: {validation_result.validation_result['stats']['would_skip']}\")\n",
    "print(f\"   ✅ Safe to Add: {validation_result.validation_result['safe_to_add']}\")\n",
    "\n",
    "# Show some validation warnings\n",
    "if validation_result.validation_result[\"warnings\"]:\n",
    "    print(f\"\\n⚠️  Sample Warnings:\")\n",
    "    for warning in validation_result.validation_result[\"warnings\"][:3]:\n",
    "        print(f\"   - {warning}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# 🔄 Demonstrate Enhanced Vector Database with Resume Functionality\n\nprint(\"🚀 TESTING ENHANCED VECTOR DATABASE API\")\nprint(\"=\" * 60)\n\n# Get a small subset of records for testing\ntest_records = records[:5]  # First 5 records\n\nprint(f\"📋 Testing with {len(test_records)} records\")\nprint(f\"🔍 Using deduplication strategy: {vectorstore.deduplication_strategy}\")\n\n# 1. Test single record processing with new API\nprint(f\"\\n1️⃣ SINGLE RECORD PROCESSING (New API)\")\nprint(\"-\" * 40)\n\nsingle_record = test_records[0]\nprint(f\"Processing record: {single_record.record_id}\")\n\n# NEW API: Enhanced process_record with comprehensive results\nresult = await vectorstore.process_record(\n    single_record,\n    skip_existing=True,  # Skip if already exists (default: True)\n    validate_before_process=True,  # Validate before processing (default: True)\n    force_reprocess=False,  # Don't force reprocessing (default: False)\n)\n\nprint(f\"✅ Result Status: {result.status}\")\nprint(f\"📊 Reason: {result.reason}\")\nprint(f\"📦 Chunks Created: {result.chunks_created}\")\nprint(f\"⏱️  Processing Time: {result.processing_time_ms:.1f}ms\")\nprint(f\"🔧 Metadata: {result.metadata}\")\n\n# 2. Test the same record again (should be skipped due to deduplication)\nprint(f\"\\n2️⃣ DUPLICATE DETECTION TEST\")\nprint(\"-\" * 40)\n\nprint(f\"Processing same record again (should be skipped)...\")\nresult2 = await vectorstore.process_record(single_record, skip_existing=True, validate_before_process=True)\n\nprint(f\"✅ Result Status: {result2.status}\")\nprint(f\"📊 Reason: {result2.reason}\")\nprint(f\"📦 Chunks Created: {result2.chunks_created}\")\nprint(f\"⏱️  Processing Time: {result2.processing_time_ms:.1f}ms\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# 🚀 NEW: Enhanced Vector Database with Resume Functionality\n\n## Breaking Changes - Enhanced API for Production Use\n\nThe vector database has been completely redesigned with breaking changes to provide:\n\n- ✅ **Smart Deduplication**: Prevent re-creating existing embeddings\n- ✅ **Resume Capability**: Safely add new records to existing collections  \n- ✅ **Comprehensive Validation**: Pre-validate batches before processing\n- ✅ **BM Integration**: Uses existing Buttermilk logging infrastructure\n- ✅ **Enhanced Metadata**: Complete provenance tracking\n\n### ⚠️ Breaking Changes\n\n**OLD API (will break):**\n```python\nresult = await vectorstore.process_record(record)\nif result:\n    print(\"Success\")\n```\n\n**NEW API (required):**\n```python\nresult = await vectorstore.process_record(record, skip_existing=True)\nif result.status == \"processed\":\n    print(f\"Success: {result.chunks_created} chunks\")\n```",
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "buttermilk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}